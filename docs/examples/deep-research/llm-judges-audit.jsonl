{"timestamp": "2026-01-01T01:13:47.298368Z", "event_id": "4049fed75c414bb6b326498bcaf372a9", "event_type": "workflow_start", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "planning", "iteration": 1, "data": {"query": "LLM Judges: techniques, architectures, evaluation methods, and applications for using large language models as automated evaluators and judges", "config": {"max_iterations": 3, "max_sub_queries": 5, "max_sources_per_query": 5, "follow_links": true, "timeout_per_operation": 120.0, "max_concurrent": 3}, "provider_id": null, "background": true, "task_timeout": null}}
{"timestamp": "2026-01-01T01:13:47.298854Z", "event_id": "62a820afe0654b1698e4339a9a5efc15", "event_type": "background_task_started", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "planning", "iteration": 1, "data": {"task_timeout": null, "timeout_per_operation": 120.0, "max_concurrent": 3}}
{"timestamp": "2026-01-01T01:13:47.301051Z", "event_id": "9447cc1c69714f29bb3eb7503f051807", "event_type": "phase_start", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "planning", "iteration": 1, "data": {"phase": "planning"}}
{"timestamp": "2026-01-01T01:14:05.461063Z", "event_id": "bc681ad479224b47a1d5dc79f2995734", "event_type": "planning_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "planning", "iteration": 1, "data": {"provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview", "tokens_used": 9319, "duration_ms": null, "system_prompt": "You are a research planning assistant. Your task is to analyze a research query and decompose it into focused sub-queries that can be researched independently.\n\nYour response MUST be valid JSON with this exact structure:\n{\n    \"research_brief\": \"A 2-3 sentence summary of the research approach and what aspects will be investigated\",\n    \"sub_queries\": [\n        {\n            \"query\": \"A specific, focused search query\",\n            \"rationale\": \"Why this sub-query is important for the research\",\n            \"priority\": 1\n        }\n    ]\n}\n\nGuidelines:\n- Generate 2-5 sub-queries (aim for 3-4 typically)\n- Each sub-query should focus on a distinct aspect of the research\n- Queries should be specific enough to yield relevant search results\n- Priority 1 is highest (most important), higher numbers are lower priority\n- Avoid overlapping queries - each should cover unique ground\n- Consider different angles: definition, examples, comparisons, recent developments, expert opinions\n\nIMPORTANT: Return ONLY valid JSON, no markdown formatting or extra text.", "user_prompt": "Research Query: LLM Judges: techniques, architectures, evaluation methods, and applications for using large language models as automated evaluators and judges\n\nPlease decompose this research query into 5 or fewer focused sub-queries.\n\nConsider:\n1. What are the key aspects that need investigation?\n2. What background information would help understand this topic?\n3. What specific questions would lead to comprehensive coverage?\n4. What different perspectives or sources might be valuable?\n\nGenerate the research plan as JSON.", "raw_response": "{\n    \"research_brief\": \"This research will investigate the methodology and infrastructure of using Large Language Models as automated evaluators, focusing on core architectures like pairwise comparison and direct scoring. It will also examine the metrics used to validate these judges against human baselines and explore their practical applications in RAG and model alignment.\",\n    \"sub_queries\": [\n        {\n            \"query\": \"LLM-as-a-Judge architectures techniques pairwise comparison vs single answer grading\",\n            \"rationale\": \"To establish the fundamental technical approaches and structural designs used to implement automated evaluation systems.\",\n            \"priority\": 1\n        },\n        {\n            \"query\": \"metrics for evaluating LLM judges correlation with human agreement benchmarks\",\n            \"rationale\": \"To understand how the reliability and accuracy of an LLM judge are measured and validated.\",\n            \"priority\": 1\n        },\n        {\n            \"query\": \"mitigating biases in LLM judges position bias verbosity bias self-preference\",\n            \"rationale\": \"To identify critical limitations in current judging methods and the strategies used to address them.\",\n            \"priority\": 2\n        },\n        {\n            \"query\": \"use cases of LLM judges in RAG evaluation and RLHF workflows\",\n            \"rationale\": \"To explore real-world applications and how automated judging is integrated into development pipelines.\",\n            \"priority\": 2\n        }\n    ]\n}", "parse_success": true, "research_brief": "This research will investigate the methodology and infrastructure of using Large Language Models as automated evaluators, focusing on core architectures like pairwise comparison and direct scoring. It will also examine the metrics used to validate these judges against human baselines and explore their practical applications in RAG and model alignment.", "sub_queries": [{"id": "subq-13a01128", "query": "LLM-as-a-Judge architectures techniques pairwise comparison vs single answer grading", "rationale": "To establish the fundamental technical approaches and structural designs used to implement automated evaluation systems.", "priority": 1}, {"id": "subq-49d253ef", "query": "metrics for evaluating LLM judges correlation with human agreement benchmarks", "rationale": "To understand how the reliability and accuracy of an LLM judge are measured and validated.", "priority": 1}, {"id": "subq-af899e18", "query": "mitigating biases in LLM judges position bias verbosity bias self-preference", "rationale": "To identify critical limitations in current judging methods and the strategies used to address them.", "priority": 2}, {"id": "subq-3d649439", "query": "use cases of LLM judges in RAG evaluation and RLHF workflows", "rationale": "To explore real-world applications and how automated judging is integrated into development pipelines.", "priority": 2}]}}
{"timestamp": "2026-01-01T01:14:05.462512Z", "event_id": "4223b6dfda5243c08353112b5b706feb", "event_type": "phase_complete", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "planning", "iteration": 1, "data": {"phase": "planning", "duration_ms": 18161.46094701253}}
{"timestamp": "2026-01-01T01:14:05.462915Z", "event_id": "3ec2a50f94db44d68255303554ebadb4", "event_type": "phase_start", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"phase": "gathering"}}
{"timestamp": "2026-01-01T01:14:07.387673Z", "event_id": "38f5a4de7d5d4a058931bff7075c1e3f", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "tavily", "sub_query_id": "subq-af899e18", "sub_query": "mitigating biases in LLM judges position bias verbosity bias self-preference", "sources_added": 5}}
{"timestamp": "2026-01-01T01:14:07.742746Z", "event_id": "f024d02197fc4ffea757947d4d970a4f", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "tavily", "sub_query_id": "subq-13a01128", "sub_query": "LLM-as-a-Judge architectures techniques pairwise comparison vs single answer grading", "sources_added": 5}}
{"timestamp": "2026-01-01T01:14:07.801944Z", "event_id": "4d2e118b31964638b1dc28e00d7ca1c5", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "perplexity", "sub_query_id": "subq-af899e18", "sub_query": "mitigating biases in LLM judges position bias verbosity bias self-preference", "sources_added": 5}}
{"timestamp": "2026-01-01T01:14:08.078150Z", "event_id": "f88dfdc652334138b2d8ae885c17a6dd", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "tavily", "sub_query_id": "subq-49d253ef", "sub_query": "metrics for evaluating LLM judges correlation with human agreement benchmarks", "sources_added": 5}}
{"timestamp": "2026-01-01T01:14:08.241401Z", "event_id": "529a0a257d6346a4890c4d719fd451b6", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "google", "sub_query_id": "subq-af899e18", "sub_query": "mitigating biases in LLM judges position bias verbosity bias self-preference", "sources_added": 5}}
{"timestamp": "2026-01-01T01:14:08.306082Z", "event_id": "fb7067d3a8064eaaa4dc61f98d25f9ed", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "perplexity", "sub_query_id": "subq-13a01128", "sub_query": "LLM-as-a-Judge architectures techniques pairwise comparison vs single answer grading", "sources_added": 5}}
{"timestamp": "2026-01-01T01:14:08.525836Z", "event_id": "828341f53ab0428ba32cfcf6843c0c7a", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "perplexity", "sub_query_id": "subq-49d253ef", "sub_query": "metrics for evaluating LLM judges correlation with human agreement benchmarks", "sources_added": 5}}
{"timestamp": "2026-01-01T01:14:08.552951Z", "event_id": "35cd9765040f4993815a27c0bf8d678d", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "semantic_scholar", "sub_query_id": "subq-af899e18", "sub_query": "mitigating biases in LLM judges position bias verbosity bias self-preference", "sources_added": 1}}
{"timestamp": "2026-01-01T01:14:08.788696Z", "event_id": "d025bb2b51f64456947e72b9a4411d56", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "google", "sub_query_id": "subq-13a01128", "sub_query": "LLM-as-a-Judge architectures techniques pairwise comparison vs single answer grading", "sources_added": 5}}
{"timestamp": "2026-01-01T01:14:08.967330Z", "event_id": "dd47a4e422674ef1a4fa5c672c6222f7", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "semantic_scholar", "sub_query_id": "subq-13a01128", "sub_query": "LLM-as-a-Judge architectures techniques pairwise comparison vs single answer grading", "sources_added": 0}}
{"timestamp": "2026-01-01T01:14:11.362930Z", "event_id": "8c155c600f494798a69cace4e924ac90", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "google", "sub_query_id": "subq-49d253ef", "sub_query": "metrics for evaluating LLM judges correlation with human agreement benchmarks", "sources_added": 5}}
{"timestamp": "2026-01-01T01:14:11.527738Z", "event_id": "6a245ebed9fb46db89598556331b4e60", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "semantic_scholar", "sub_query_id": "subq-49d253ef", "sub_query": "metrics for evaluating LLM judges correlation with human agreement benchmarks", "sources_added": 5}}
{"timestamp": "2026-01-01T01:14:12.379288Z", "event_id": "b7530a934c254a079aa18e8cf648909a", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "tavily", "sub_query_id": "subq-3d649439", "sub_query": "use cases of LLM judges in RAG evaluation and RLHF workflows", "sources_added": 5}}
{"timestamp": "2026-01-01T01:14:12.702951Z", "event_id": "327db97dacf949ed8b518a7854446a10", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "perplexity", "sub_query_id": "subq-3d649439", "sub_query": "use cases of LLM judges in RAG evaluation and RLHF workflows", "sources_added": 5}}
{"timestamp": "2026-01-01T01:14:13.040759Z", "event_id": "a2256f25eed44711961978b73b9cb34f", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "google", "sub_query_id": "subq-3d649439", "sub_query": "use cases of LLM judges in RAG evaluation and RLHF workflows", "sources_added": 5}}
{"timestamp": "2026-01-01T01:14:13.195017Z", "event_id": "f9d83fc026f0492396485fb31df7c441", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"provider": "semantic_scholar", "sub_query_id": "subq-3d649439", "sub_query": "use cases of LLM judges in RAG evaluation and RLHF workflows", "sources_added": 0}}
{"timestamp": "2026-01-01T01:14:13.201190Z", "event_id": "7d550d09f6b1437690f264fdbed48673", "event_type": "gathering_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"source_count": 53, "queries_executed": 4, "queries_failed": 0, "unique_urls": 53, "providers_used": ["tavily", "perplexity", "google", "semantic_scholar"], "providers_unavailable": []}}
{"timestamp": "2026-01-01T01:14:13.204604Z", "event_id": "07741fea24a046ec9c4b6e0890ff4bc2", "event_type": "phase_complete", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 1, "data": {"phase": "gathering", "duration_ms": 7742.961996002123}}
{"timestamp": "2026-01-01T01:14:13.205069Z", "event_id": "c00812cfa2c8490b8531f556db4ad870", "event_type": "phase_start", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "analysis", "iteration": 1, "data": {"phase": "analysis"}}
{"timestamp": "2026-01-01T01:14:47.771768Z", "event_id": "2bb245084f70487d958b62a702753305", "event_type": "analysis_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "analysis", "iteration": 1, "data": {"provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview", "tokens_used": 18675, "duration_ms": null, "system_prompt": "You are a research analyst. Your task is to analyze research sources and extract key findings, assess their quality, and identify knowledge gaps.\n\nYour response MUST be valid JSON with this exact structure:\n{\n    \"findings\": [\n        {\n            \"content\": \"A clear, specific finding or insight extracted from the sources\",\n            \"confidence\": \"low|medium|high\",\n            \"source_ids\": [\"src-xxx\", \"src-yyy\"],\n            \"category\": \"optional category/theme\"\n        }\n    ],\n    \"gaps\": [\n        {\n            \"description\": \"Description of missing information or unanswered question\",\n            \"suggested_queries\": [\"follow-up query 1\", \"follow-up query 2\"],\n            \"priority\": 1\n        }\n    ],\n    \"quality_updates\": [\n        {\n            \"source_id\": \"src-xxx\",\n            \"quality\": \"low|medium|high\"\n        }\n    ]\n}\n\nGuidelines for findings:\n- Extract 2-5 key findings from the sources\n- Each finding should be a specific, actionable insight\n- Confidence levels: \"low\" (single weak source), \"medium\" (multiple sources or one authoritative), \"high\" (multiple authoritative sources agree)\n- Include source_ids that support each finding\n- Categorize findings by theme when applicable\n\nGuidelines for gaps:\n- Identify 1-3 knowledge gaps or unanswered questions\n- Provide specific follow-up queries that could fill each gap\n- Priority 1 is most important, higher numbers are lower priority\n\nGuidelines for quality_updates:\n- Assess source quality based on authority, relevance, and recency\n- \"low\" = questionable reliability, \"medium\" = generally reliable, \"high\" = authoritative\n\nIMPORTANT: Return ONLY valid JSON, no markdown formatting or extra text.", "user_prompt": "Original Research Query: LLM Judges: techniques, architectures, evaluation methods, and applications for using large language models as automated evaluators and judges\n\nResearch Brief:\nThis research will investigate the methodology and infrastructure of using Large Language Models as automated evaluators, focusing on core architectures like pairwise comparison and direct scoring. It will also examine the metrics used to validate these judges against human baselines and explore their practical applications in RAG and model alignment.\n\nSources to Analyze:\n\nSource 1 (ID: src-67c025c2):\n  Title: Self-Preference Bias in LLM-as-a-Judge\n  URL: https://openreview.net/forum?id=Ns8zGZ0lmM\n  Snippet: ## Self-Preference Bias in LLM-as-a-Judge. **TL;DR:** We propose a novel quantitative metric to measure self-preference bias in LLM-as-a-judge. However, the self-preference bias in LLMs has posed significant risks, including promoting specific styles or policies intrinsic to the LLMs. Despite the importance of this issue, there is a lack of established methods to measure the self-preference bias quantitatively, and its underlying causes are poorly understood. In this paper, we introduce a novel ...\n  Content: [Go to **ICLR 2025 Conference** homepage](/group?id=ICLR.cc/2025/Conference \"Venue Homepage\")\n\n## Self-Preference Bias in LLM-as-a-Judge\n\n### [Koki Wataoka](/profile?id=~Koki_Wataoka1 \"~Koki_Wataoka1\"), [Tsubasa Takahashi](/profile?id=~Tsubasa_Takahashi1 \"~Tsubasa_Takahashi1\"), [Ryokan Ri](/profile?id=~Ryokan_Ri1 \"~Ryokan_Ri1\")\n\n27 Sept 2024 (modified: 05 Feb 2025)Submitted to ICLR 2025Everyone[Revisions](/revisions?id=Ns8zGZ0lmM)[BibTeX](#)[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/ \"Licensed under Creative Commons Attribution 4.0 International\")\n\n**Keywords:** large language model, llm-as-a-judge, bias, fairness\n\n**TL;DR:** We propose a novel quantitative metric to measure self-preference bias in LLM-as-a-judge.\n\n**Abstract:** Automated evaluation leveraging large language models (LLMs), commonly referred to as LLM evaluators or LLM-as-a-judge, has been widely used in measuring the performance of dialogue systems. However, the self-preference bias in LLMs has posed signi...\n\nSource 2 (ID: src-45a8de46):\n  Title: Self-Preference Bias in LLM-as-a-Judge\n  URL: https://arxiv.org/html/2410.21819v1\n  Snippet: (2024) addressed quantifying self-preference bias within an evaluation approach where LLMs assign an absolute score to a single generated text. This suggests that the fundamental cause of self-preference bias may be the familiarity of the texts to the LLM evaluators, specifically how likely they are to generate the same response. The contributions of this paper are threefold: (1) We propose a new metric to quantify self-preference bias in LLMs; (2) Using this metric, we evaluate the extent of se...\n  Content: # Self-Preference Bias in LLM-as-a-Judge\n\n[Koki Wataoka](https://orcid.org/0000-0000-0000-0000)   \nSB Intuitions   \nkoki.wataoka@sbintuitions.co.jp   \n&[Tsubasa Takahashi](https://orcid.org/0000-0000-0000-0000)   \nSB Intuitions   \ntsubasa.takahashi@sbintuitions.co.jp   \n&[Ryokan Ri](https://orcid.org/0000-0000-0000-0000)   \nSB Intuitions   \nryokan.ri@sbintuitions.co.jp\n\n###### Abstract\n\nAutomated evaluation leveraging large language models (LLMs), commonly referred to as LLM evaluators or LLM-as-a-judge, has been widely used in measuring the performance of dialogue systems. However, the self-preference bias in LLMs has posed significant risks, including promoting specific styles or policies intrinsic to the LLMs. Despite the importance of this issue, there is a lack of established methods to measure the self-preference bias quantitatively, and its underlying causes are poorly understood. In this paper, we introduce a novel quantitative metric to measure the self-preference bias. Our ex...\n\nSource 3 (ID: src-48201995):\n  Title: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n  URL: https://neurips.cc/virtual/2023/poster/73434\n  Snippet: We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability\n  Content: ## Main Navigation\n\n![conference_logo](/static/core/img/neurips-navbar-logo.svg)\n\n# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n\n### Abstract\n\nEvaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences.To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions.We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them.We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform.Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\\% agreement, the same level o...\n\nSource 4 (ID: src-e0d1753b):\n  Title: Mitigating the Bias of Large Language Model Evaluation\n  URL: https://aclanthology.org/2024.ccl-1.101.pdf\n  Snippet: In this work, we propose two methods for mitigating the bias of LLM-as-a-Judge. For closed-source judge models, we propose to mitigate the bias\n  Content: Mitigating the Bias of Large Language Model Evaluation Hongli Zhou1, Hui Huang2, Yunfei Long3, Bing Xu2, Conghui Zhu2, Hailong Cao2, Muyun Yang2\u2217, Tiejun Zhao2 1School of Architecture and Design, Harbin Institute of Technology, Harbin, China 2Faculty of Computing, Harbin Institute of Technology, Harbin, China 3University of Essex {hongli.joe,huanghui}@stu.hit.edu.cn;yl20051@essex.ac.uk; {hitxb,conghui,caohailong,yangmuyun,tjzhao}@hit.edu.cn Abstract Recently, there has been a trend of evaluating the Large Language Model (LLM) quality in the flavor of LLM-as-a-Judge, namely leveraging another LLM to evaluate the current output qual-ity. However, existing judges are proven to be biased, namely they would favor answers which present better superficial quality (such as verbosity, fluency) while ignoring the instruction fol-lowing ability. In this work, we propose systematic research about the bias of LLM-as-a-Judge.\nSpecifically, for closed-source judge models, we apply calibration to miti...\n\nSource 5 (ID: src-8d0c93da):\n  Title: 5 Techniques to Improve LLM-Judges : r/LLMDevs\n  URL: https://www.reddit.com/r/LLMDevs/comments/1j3gbil/5_techniques_to_improve_llmjudges/\n  Snippet: But using LLMs as a judge does come with some drawbacks\u2014like narcissistic bias (favoring their own outputs), a preference for verbosity (over\n  Content: ![r/LLMDevs icon](https://styles.redditmedia.com/t5_7xegfq/styles/communityIcon_b553dnae9oia1.png?width=96&height=96&frame=1&auto=webp&crop=96%3A96%2Csmart&s=8ea201f189c513413bda6216591bb75e74ae6b0c)\n\n# 5 Techniques to Improve LLM-Judges\n\nLLM-based metrics are currently the best method for evaluating LLM applications. But using LLMs as a judge does come with some drawbacks\u2014like narcissistic bias (favoring their own outputs), a preference for verbosity (over concise answers), unreliable fine-grained scoring (whereas binary outputs are much more accurate), and positional bias (prefer answer choices that come up first).\n\nFortunately, there are several methods and techniques you can employ to minimize these shortcomings when creating your LLM evaluation metrics. For anyone who\u2019s interested, I\u2019ve written a more [in-depth blog here](https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method#improving-llm-judgements).\n\n# 1. Chain-Of-Thought Prompting\n\nChain-of-thou...\n\nSource 6 (ID: src-08525cff):\n  Title: LLM-as-a-Judge: Unveiling Its Potential and Applications - Medium\n  URL: https://medium.com/@ganeshkannappan/llm-as-a-judge-unveiling-its-potential-and-applications-cbfb3db14e26\n  Snippet: * **Quantitative (or numeric) Grading** \u2014 The evaluator LLM assigns a numerical score to the answer, such as 0\u201310 or 0\u2013100, based on predefined criteria. **Objective Evaluation** \u2014 Single answer grading provides an **objective** and structured way to assess a model\u2019s response. The evaluator (in this case, the LLM) checks the generated response against the reference response and scores or judges the quality based on how closely the generated answer aligns with the reference answer in terms of acc...\n  Content: [Sitemap](/sitemap/sitemap.xml)\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40ganeshkannappan%2Fllm-as-a-judge-unveiling-its-potential-and-applications-cbfb3db14e26&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40ganeshkannappan%2Fllm-as-a-judge-unveiling-its-potential-and-applications-cbfb3db14e26&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# LLM-as-a-Judge: Unveiling Its Potential and Applications\n\n[Ganesh Kannappan](/@ganeshkannappan?source=post_page---byline--cbfb3db14e26---------------------------------------)\n\n12 min read\n\n\u00b7\n\nDec 2, 2024\n\n--\n\nIn the [previous part](/@ganeshkannappan/llm-as-a-judge-...\n\nSource 7 (ID: src-51263506):\n  Title: Using LLMs for Evaluation - by Cameron R. Wolfe, Ph.D.\n  URL: https://cameronrwolfe.substack.com/p/llm-as-a-judge\n  Snippet: LLM-as-a-judge is a reference-free metric that directly prompts a powerful LLM to evaluate the quality of another model\u2019s output. For LLM-as-a-Judge evaluations, authors adopt the same strategy proposed by Vicuna [2], where the quality of model outputs is judged by via a pairwise prompt to GPT-4. The task of these annotators is to evaluate the quality of stories written for 200 prompts, where for each prompt we *i)* sample a response from GPT-2 (i.e., a weaker LLM) and *ii)* have a human write a...\n  Content: # [Deep (Learning) Focus](/)\n\n# Using LLMs for Evaluation\n\n### LLM-as-a-Judge and other scalable additions to human quality ratings...\n\n[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)\n\nJul 22, 2024\n\nAs large language models (LLMs) have become more and more capable, one of the most difficult aspects of working with these models is determining how to properly evaluate them. Many powerful models exist, and they each solve a wide variety of complex, open-ended tasks. As a result, discerning differences in performance between these models can be difficult. The most reliable method of evaluating LLMs is with human feedback, but collecting data from humans is noisy, time consuming, and expensive. Despite being a valuable and necessary source of truth for measuring model capabilities, human evaluation\u2014*when used in isolation*\u2014impedes our ability to iterate quickly during model development. To solve this problem, we need an evaluation metric that is quick, cost effective, and si...\n\nSource 8 (ID: src-2a4435f2):\n  Title: A Survey on LLM-as-a-Judge - arXiv\n  URL: https://arxiv.org/html/2411.15594v1\n  Snippet: To automate evaluation by LLM-as-a-Judge, one effective approach is to employ advanced language models such as GPT-4\u00a0(OpenAI, 2023a) instead of human evaluators\u00a0(Zheng et\u00a0al., 2023c). Unlike INSTRUCTSCORE which directly optimizes the model, the LLM evaluator in JADE(Zhang et\u00a0al., 2023c) relies on human judges to correct LLMs\u2019 evaluation results and updates the most frequently corrected samples into the example sets for few-shot prompting. In addition to integrating results from multiple rounds o...\n  Content: 11footnotetext: \\* These authors contributed equally to this research.22footnotetext: \u2020 Corresponding author.\n\n# A Survey on LLM-as-a-Judge\n\nJiawei Gu1,\\*, Xuhui Jiang1,\\*, Zhichao Shi1,2,\\*, Hexiang Tan2, Xuehao Zhai3, Chengjin Xu1, Wei Li2, Yinghan Shen2, Shengjie Ma1,4, Honghao Liu1,   \nYuanzhuo Wang2, Jian Guo1,\u2020     \n1IDEA Research, International Digital Economy Academy   \n2Institute of Computing Technology, Chinese Academy of Sciences   \n3Department of Civil and Environmental Engineering, Imperial College London   \n4Gaoling School of Artificial Intelligence, Renmin University of China China\n\n###### Abstract.\n\n## Abstract\n\nAccurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of \u201dLLM-as-a-Judge,\u201d where LLMs are employed as evaluators for complex task...\n\nSource 9 (ID: src-bbd215f1):\n  Title: LLM-as-a-Judge - by Nilesh Barla - Adaline Labs\n  URL: https://labs.adaline.ai/p/llm-as-a-judge\n  Snippet: Evaluating LLM outputs can save you a lot of time from shipping broken prompts and features. And for such a situation where you cannot write detailed instructions every time, you need to find a way to evaluate every output from the LLM. LLM-as-a-Judge is a framework where LLMs evaluate outputs from other LLMs using **structured prompts** to score qualities like **coherence** or **accuracy**. Teams need scalable evaluation methods that can assess LLM outputs with human-like judgment but without t...\n  Content: # [Adaline Labs](/)\n\n# LLM-as-a-Judge\n\n### A brief research note on LLM-as-a-judge including best practices.\n\n[Nilesh Barla](https://substack.com/@iridium0077)\n\nSep 08, 2025\n\nEvaluating LLM outputs can save you a lot of time from shipping broken prompts and features.\n\nA lot of talk and discussion is going on when it comes to the degrading performance or output of LLMs. You go to Reddit and you will find that users are not satisfied with LLMs such as Claude (these days) and GPT-5.\n\nSo, what's going on with LLMs?\n\nYou provide an input or prompt addressing your requirements, and the LLM doesn\u2019t provide you with a desirable answer. This might be happening because of one of two reasons, or both:\n\n1. Bad prompt\n2. Bad LLM\n\nNow, I understand that in a certain workflow that includes creativity, such as writing and brainstorming, you can hone the LLMs by using more structured prompting. For the most part, they will be satisfactory.\n\nBut when it comes to more logical and complex workflows, like ...\n\nSource 10 (ID: src-78c4677b):\n  Title: LLM-as-a-Judge: How AI Can Evaluate AI Faster and Smarter\n  URL: https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-j/\n  Snippet: An LLM-as-a-Judge evaluation uses an LLM to mimic human judgment of another LLM's output. It's not a fixed mathematical metric like \u201caccuracy\u201d \u2013\n  Content: Select platform to login\n\n[**Cloud Management**\n\nWebservers and Virtual Machines](https://cloud.bunnyshell.com/login/)[**Environments as a Service**\n\nCreate and Manage Kubernetes Environments](https://environments.bunnyshell.com/login/)\n\n[blog](/blog/)\n\n/[Cloud computing](/blog/cloud-computing/)\n\n# When AI Becomes the Judge: Understanding \u201cLLM-as-a-Judge\u201d\n\n[engineering](/blog/engineering/)\n\n[Alin Dobra](/blog/author/alin-dobra/)\n\nWhy Use an LLM as Judge?\n\nHow LLM-Judges Work\n\nArchitectures: Judge Assembly vs Super Judge\n\nUse Cases and Examples\n\nBuilding an Effective LLM Judge: Tips and Pitfalls\n\nPowering LLM-Evaluation with Bunnyshell\n\nConclusion\n\nImagine building a chatbot or code generator that not only writes answers \u2013 but also grades them. In the past, ensuring AI quality meant recruiting human reviewers or using simple metrics (BLEU, ROUGE) that miss nuance. Today, we can leverage **Generative AI** itself to evaluate its own work. *LLM-as-a-Judge* means using one Large Language Mo...\n\nSource 11 (ID: src-6ba1f0a1):\n  Title: Understanding Bias in LLM-as-a-Judge Systems\n  URL: https://ragmetrics.ai/blog/understanding-bias-in-llm-as-a-judge-systems\n  Snippet: # Understanding Bias in LLM-as-a-Judge Systems\n\n**The Hidden Problem in AI Evaluation**\n\nEvery developer building with GenAI has hit this moment: your evaluation pipeline says one model output is \u201cbetter,\u201d but your eyes disagree. The culprit is often bias\u2014bias not in the generating model, but in the\n\n**LLM acting as the judge**.... LLM-as-a-Judge systems are now the backbone of modern AI evaluation frameworks. They\u2019re faster, cheaper, and more consistent than human review\u2014but they\u2019re not immune ...\n\nSource 12 (ID: src-a4549098):\n  Title: A Systematic Study of Position Bias in LLM-as-a-Judge - arXiv\n  URL: https://arxiv.org/html/2406.07791v7\n  Snippet: ###### Abstract\nLLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases\u2014particularly position bias, the tendency to favor solutions based on their position within the prompt\u2014compromise its reliability. This study investigates position bias in LLM judges across pairwise and list-wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness.... Our experiments, involving 12 LLM ju...\n\nSource 13 (ID: src-bef824af):\n  Title: The 5 Biases That Can Silently Kill Your LLM Evaluations ...\n  URL: https://www.sebastiansigl.com/blog/llm-judge-biases-and-how-to-fix-them/\n  Snippet: This is the risk you run when you trust LLM judges blindly. For all their power, they are not impartial arbiters. They are susceptible to a range of cognitive biases - predictable, systematic errors that can silently corrupt your evaluation data and lead you to make the wrong product decisions\n\n2 3. Relying on a biased judge means you could be optimizing for failure, shipping regressions, and eroding user trust, all while your metrics tell you everything is fine.... This post will guide you thro...\n\nSource 14 (ID: src-7c38a7f7):\n  Title: Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge\n  URL: https://llm-judge-bias.github.io\n  Snippet: The upper part illustrates an example of diversity bias in LLM-as-a-Judge scenarios, while the lower part displays the ranking of average consistency metrics across six models.\n\nOur proposed framework:\n\n**CALM**... |Bias Type|Description|Example|\n|--|--|--|\n|\ud83d\udd00 Position (Pos.)|When an LLM exhibits a propensity to favor certain positions over others.|$R_1$: 3.11 > 3.8 $R_2$: 3.8 > 3.11 $R_1$: 3.8 > 3.11 $R_2$: 3.11 > 3.8|\n|\ud83d\udcc4 Verbosity (Ver.)|LLM judges favor longer responses, even if they are not ...\n\nSource 15 (ID: src-c33a2512):\n  Title: Evaluating and Mitigating LLM-as-a-judge Bias in ...\n  URL: https://arxiv.org/abs/2510.12462\n  Snippet: # Computer Science > Artificial Intelligence\n\n**arXiv:2510.12462** (cs)\n\n[Submitted on 14 Oct 2025]... # Title: Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems\n\nAuthors:Jiaxin Gao, Chen Chen, Yanwen Jia, Xueluan Gong, Kwok-Yan Lam, Qian Wang\nAbstract:Large Language Models (LLMs) are increasingly being used to autonomously evaluate the quality of content in communication systems, e.g., to assess responses in telecom customer support chatbots.... However, the impartiality of...\n\nSource 16 (ID: src-1e5014bd):\n  Title: An LLM-as-Judge Metric for Bridging the Gap with Human ... - arXiv\n  URL: https://arxiv.org/html/2505.20854v1\n  Snippet: In this paper, we present SWE-Judge, the first evaluation metric for LLM-as-Ensemble-Judge specifically designed to accurately assess the correctness of generated software artifacts. We evaluate SWE-Judge across a diverse set of software engineering (SE) benchmarks\u2014including CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess\u2014which span three SE tasks: code generation, automated program repair, and code summarization. The state-of-the-art LLM-as-judge evaluation metric for code,...\n  Content: \\newmdenv\n\n[ linecolor=linecolor, leftline=true, topline=false, bottomline=false, rightline=false, linewidth=2pt, innerleftmargin=10pt, innerrightmargin=10pt, innertopmargin=5pt, innerbottommargin=5pt, backgroundcolor=bgcolor ]leftbar\n\n# An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks\n\nXin Zhou  Singapore Management UniversitySingapore  [xinzhou.2020@phdcs.smu.edu.sg](mailto:xinzhou.2020@phdcs.smu.edu.sg)  ,\u00a0 Kisub Kim  Independent ResearcherHong Kong  [falconlk00@gmail.com](mailto:falconlk00@gmail.com)  ,\u00a0 Ting Zhang  Singapore Management UniversitySingapore  [tingzhang.2019@phdcs.smu.edu.sg](mailto:tingzhang.2019@phdcs.smu.edu.sg)  ,\u00a0 Martin Weyssow  Singapore Management UniversitySingapore  [mweyssow@smu.edu.sg](mailto:mweyssow@smu.edu.sg)  ,\u00a0 Lu\u00eds F.\u00a0Gomes  Carnegie Mellon UniversityUSA  [lfgomes@andrew.cmu.edu](mailto:lfgomes@andrew.cmu.edu)  ,\u00a0 Guang Yang  Nanjing University of Aeronautics and AstronauticsChina  [novelyg@outlook.com](mailto:novelyg@o...\n\nSource 17 (ID: src-db258615):\n  Title: LLM Evaluation Frameworks, Metrics & Methods Explained\n  URL: https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained\n  Snippet: This guide breaks down key LLM evaluation methods\u2014including automatic metrics, human reviews, hybrid frameworks like G-Eval, and LLM-as-a-Judge strategies. To get the most out of LLM-as-a-judge, teams often **prompt-engineer the evaluation** carefully (more on this in the G-Eval section), and may use a two-step process: first have the AI judge give a detailed rationale or score for multiple criteria, then possibly have a human review a subset of those judgments for quality control. It complement...\n  Content: Start Safeguarding Your LLM\u00a0Today!\n\nImplementing Qualifire is simple. Contact our team today, and\u00a0we\u2019ll get you started in no time!\n\nTalk to our team\n\nDror Ivry\n\n30/5/2025\n\nTable of content\n\n[What is HELM?](#)\n\n# LLM Evaluation Frameworks, Metrics & Methods Explained\n\n## **Introduction**\n\nLarge Language Models (LLMs) are increasingly deployed in chatbots, virtual assistants, and other user-facing applications. Ensuring these models produce high-quality, safe, and helpful responses is a major challenge. This makes evaluation a critical part of the development and deployment cycle for LLM-powered chat systems. Unlike traditional NLP tasks with clear-cut metrics, open-ended dialog requires careful **evaluation strategies**. In this post, we\u2019ll explore the spectrum of LLM evaluation methods \u2013 from automatic metrics to human reviews and cutting-edge hybrid approaches \u2013 and discuss when each is appropriate. We\u2019ll then take a deep dive into **LLM-as-a-judge** techniques with a focus on the G-...\n\nSource 18 (ID: src-3f4263f1):\n  Title: Large Language Model Evaluation in '26: 10+ Metrics & Methods\n  URL: https://research.aimultiple.com/large-language-model-evaluation/\n  Snippet: *   **MuSR** consists of algorithmically generated complex problems, requiring models to use reasoning and long-range context parsing, with few models performing better than random.[4](https://research.aimultiple.com/large-language-model-evaluation/#easy-footnote-bottom-4-68488 \"https://huggingface.co/datasets/TAUR-Lab/MuSR\"). *   **BBH** includes 23 challenging tasks from the BigBench dataset, measuring objective metrics and language understanding, and correlates well with human preference.[7](...\n  Content: Large Language Model Evaluation in '26: 10+ Metrics & Methods\n===============\n\n[![Image 1: AIMultiple](https://research.aimultiple.com/images/logo-2025.svg)![Image 2: AIMultiple](https://research.aimultiple.com/images/logo-2025-white.svg)](https://aimultiple.com/)\n\nAI\n\nCATEGORIES\n\nAI Coding AI Foundations AI Hardware AI in Industries Document Automation Generative AI Generative AI Applications Large Language Models MCP RAG\n\n[AI Code](https://research.aimultiple.com/ai-code/)[AI Code Editor](https://research.aimultiple.com/ai-code-editor/)[AI Code Review Tools](https://research.aimultiple.com/ai-code-review-tools/)[AI Coding Benchmark](https://research.aimultiple.com/ai-coding-benchmark/)[Screenshot to Code](https://research.aimultiple.com/screenshot-to-code/)\n\nAgentic AI\n\nCATEGORIES\n\nAgent Architectures & Tools AI Agent Applications Open-Source Agents\n\n[Agentic AI](https://research.aimultiple.com/agentic-ai/)[Agentic AI Design Patterns](https://research.aimultiple.com/agentic-ai-design...\n\nSource 19 (ID: src-0378afab):\n  Title: LLM Evaluation in 2025: Metrics, RAG, LLM-as-Judge & Best Practices\n  URL: https://medium.com/@QuarkAndCode/llm-evaluation-in-2025-metrics-rag-llm-as-judge-best-practices-ad2872cfa7cb\n  Snippet: # LLM Evaluation in 2025: Metrics, RAG, LLM-as-Judge & Best Practices. Evaluating large language models (LLMs) looks deceptively simple \u2014 run a prompt, eyeball the output, ship. In reality, robust evaluation is a multi\u2011layer process that blends offline tests, human judgment, and production telemetry. Below is a practical field guide that synthesizes recent playbooks and research into a repeatable approach you can use today. ## 1) Start with what you\u2019re shipping, not just the model. Before choosi...\n  Content: [Sitemap](/sitemap/sitemap.xml)\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40QuarkAndCode%2Fllm-evaluation-in-2025-metrics-rag-llm-as-judge-best-practices-ad2872cfa7cb&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Search](/search?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40QuarkAndCode%2Fllm-evaluation-in-2025-metrics-rag-llm-as-judge-best-practices-ad2872cfa7cb&source=post_page---top_nav_layout_nav-----------------------global_nav------------...\n\nSource 20 (ID: src-66027906):\n  Title: Co-Eval: Augmenting LLM-based Evaluation with Machine Metrics\n  URL: https://aclanthology.org/2025.emnlp-main.1307.pdf\n  Snippet: Nov 4, 2025 ... We demonstrate the effectiveness of the Co-Eval framework in mitigating four types of bias\u2014self- preference bias, position bias, verbosity bias,\u00a0...\n\nPlease analyze these sources and:\n1. Extract 2-5 key findings relevant to the research query\n2. Assess confidence levels based on source agreement and authority\n3. Identify any knowledge gaps or unanswered questions\n4. Assess the quality of each source\n\nReturn your analysis as JSON.", "raw_response": "{\n    \"findings\": [\n        {\n            \"content\": \"LLM Judges exhibit systematic cognitive biases, most notably 'self-preference bias' (favoring their own generated outputs), 'position bias' (favoring responses appearing earlier or later in a pair), and 'verbosity bias' (rating longer responses higher regardless of quality).\",\n            \"confidence\": \"high\",\n            \"source_ids\": [\n                \"src-67c025c2\",\n                \"src-45a8de46\",\n                \"src-48201995\",\n                \"src-e0d1753b\",\n                \"src-a4549098\",\n                \"src-7c38a7f7\"\n            ],\n            \"category\": \"Bias & Limitations\"\n        },\n        {\n            \"content\": \"To mitigate evaluation biases, researchers employ techniques such as 'Chain-of-Thought' (CoT) prompting to induce reasoning before scoring, position swapping (running the eval twice with swapped orders) to average out position bias, and 'Co-Eval' frameworks that augment LLMs with objective machine metrics.\",\n            \"confidence\": \"high\",\n            \"source_ids\": [\n                \"src-8d0c93da\",\n                \"src-66027906\",\n                \"src-48201995\",\n                \"src-e0d1753b\"\n            ],\n            \"category\": \"Mitigation Techniques\"\n        },\n        {\n            \"content\": \"Two primary architectures dominate LLM-as-a-Judge: 'Pairwise Comparison' (mimicking human preference testing like Chatbot Arena) and 'Direct Scoring/Pointwise' (assigning absolute scores like 1-10), with strong models like GPT-4 achieving over 80% agreement with human annotators in general chat domains.\",\n            \"confidence\": \"medium\",\n            \"source_ids\": [\n                \"src-48201995\",\n                \"src-51263506\",\n                \"src-2a4435f2\"\n            ],\n            \"category\": \"Architecture & Performance\"\n        },\n        {\n            \"content\": \"Specialized 'Ensemble' or 'Judge Assembly' approaches are emerging for complex domains, such as 'SWE-Judge' for software engineering, which combines LLM reasoning with code execution/static analysis to bridge the gap with human verification in technical tasks.\",\n            \"confidence\": \"medium\",\n            \"source_ids\": [\n                \"src-1e5014bd\",\n                \"src-78c4677b\",\n                \"src-2a4435f2\"\n            ],\n            \"category\": \"Advanced Architectures\"\n        }\n    ],\n    \"gaps\": [\n        {\n            \"description\": \"While 'Judge Assembly' and ensemble methods are mentioned, specific architectural patterns for orchestrating these cost-effectively in production (latency vs. accuracy trade-offs) are under-documented in the provided sources.\",\n            \"suggested_queries\": [\n                \"architectural patterns for LLM judge ensembles production\",\n                \"latency cost trade-off LLM-as-a-judge assembly\"\n            ],\n            \"priority\": 1\n        },\n        {\n            \"description\": \"The sources discuss biases extensively but lack detailed comparative data on the efficacy of 'Reference-free' vs. 'Reference-based' evaluation across different model sizes (e.g., can a small model effectively judge a large model if provided a reference?).\",\n            \"suggested_queries\": [\n                \"reference-free vs reference-based LLM evaluation accuracy comparison\",\n                \"small model judge performance with ground truth references\"\n            ],\n            \"priority\": 2\n        }\n    ],\n    \"quality_updates\": [\n        {\n            \"source_id\": \"src-48201995\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-67c025c2\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-1e5014bd\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-8d0c93da\",\n            \"quality\": \"medium\"\n        },\n        {\n            \"source_id\": \"src-78c4677b\",\n            \"quality\": \"medium\"\n        }\n    ]\n}", "parse_success": true, "findings": [{"content": "LLM Judges exhibit systematic cognitive biases, most notably 'self-preference bias' (favoring their own generated outputs), 'position bias' (favoring responses appearing earlier or later in a pair), and 'verbosity bias' (rating longer responses higher regardless of quality).", "confidence": "high", "source_ids": ["src-67c025c2", "src-45a8de46", "src-48201995", "src-e0d1753b", "src-a4549098", "src-7c38a7f7"], "category": "Bias & Limitations"}, {"content": "To mitigate evaluation biases, researchers employ techniques such as 'Chain-of-Thought' (CoT) prompting to induce reasoning before scoring, position swapping (running the eval twice with swapped orders) to average out position bias, and 'Co-Eval' frameworks that augment LLMs with objective machine metrics.", "confidence": "high", "source_ids": ["src-8d0c93da", "src-66027906", "src-48201995", "src-e0d1753b"], "category": "Mitigation Techniques"}, {"content": "Two primary architectures dominate LLM-as-a-Judge: 'Pairwise Comparison' (mimicking human preference testing like Chatbot Arena) and 'Direct Scoring/Pointwise' (assigning absolute scores like 1-10), with strong models like GPT-4 achieving over 80% agreement with human annotators in general chat domains.", "confidence": "medium", "source_ids": ["src-48201995", "src-51263506", "src-2a4435f2"], "category": "Architecture & Performance"}, {"content": "Specialized 'Ensemble' or 'Judge Assembly' approaches are emerging for complex domains, such as 'SWE-Judge' for software engineering, which combines LLM reasoning with code execution/static analysis to bridge the gap with human verification in technical tasks.", "confidence": "medium", "source_ids": ["src-1e5014bd", "src-78c4677b", "src-2a4435f2"], "category": "Advanced Architectures"}], "gaps": [{"description": "While 'Judge Assembly' and ensemble methods are mentioned, specific architectural patterns for orchestrating these cost-effectively in production (latency vs. accuracy trade-offs) are under-documented in the provided sources.", "suggested_queries": ["architectural patterns for LLM judge ensembles production", "latency cost trade-off LLM-as-a-judge assembly"], "priority": 1}, {"description": "The sources discuss biases extensively but lack detailed comparative data on the efficacy of 'Reference-free' vs. 'Reference-based' evaluation across different model sizes (e.g., can a small model effectively judge a large model if provided a reference?).", "suggested_queries": ["reference-free vs reference-based LLM evaluation accuracy comparison", "small model judge performance with ground truth references"], "priority": 2}], "quality_updates": [{"source_id": "src-48201995", "quality": "high"}, {"source_id": "src-67c025c2", "quality": "high"}, {"source_id": "src-1e5014bd", "quality": "high"}, {"source_id": "src-8d0c93da", "quality": "medium"}, {"source_id": "src-78c4677b", "quality": "medium"}]}}
{"timestamp": "2026-01-01T01:14:47.773311Z", "event_id": "4b291bfa37d24622a28387122489b4ee", "event_type": "phase_complete", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "analysis", "iteration": 1, "data": {"phase": "analysis", "duration_ms": 34569.39693004824}}
{"timestamp": "2026-01-01T01:14:47.773638Z", "event_id": "18a347777b0d4b86abf72a1a3aa9dd52", "event_type": "phase_start", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "synthesis", "iteration": 1, "data": {"phase": "synthesis"}}
{"timestamp": "2026-01-01T01:15:19.624593Z", "event_id": "cedaf4683ad946c5a74b3e74ce440d24", "event_type": "synthesis_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "synthesis", "iteration": 1, "data": {"provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview", "tokens_used": 13739, "duration_ms": null, "system_prompt": "You are a research synthesizer. Your task is to create a comprehensive, well-structured research report from analyzed findings.\n\nGenerate a markdown-formatted report with the following structure:\n\n# Research Report: [Topic]\n\n## Executive Summary\nA 2-3 paragraph overview of the key insights and conclusions.\n\n## Key Findings\n\n### [Theme/Category 1]\n- Finding with supporting evidence and source citations [Source ID]\n- Related findings grouped together\n\n### [Theme/Category 2]\n- Continue for each major theme...\n\n## Analysis\n\n### Supporting Evidence\nDiscussion of well-supported findings with high confidence.\n\n### Conflicting Information\nNote any contradictions or disagreements between sources (if present).\n\n### Limitations\nAcknowledge gaps in the research and areas needing further investigation.\n\n## Sources\nList sources as markdown links with their IDs: **[src-xxx]** [Title](URL)\n\n## Conclusions\nActionable insights and recommendations based on the findings.\n\n---\n\nGuidelines:\n- Organize findings thematically rather than listing them sequentially\n- Cite source IDs in brackets when referencing specific information [src-xxx]\n- Distinguish between high-confidence findings (well-supported) and lower-confidence insights\n- Be specific and actionable in conclusions\n- Keep the report focused on the original research query\n- Use clear, professional language\n- Include all relevant findings - don't omit information\n\nIMPORTANT: Return ONLY the markdown report, no preamble or meta-commentary.", "user_prompt": "# Research Query\nLLM Judges: techniques, architectures, evaluation methods, and applications for using large language models as automated evaluators and judges\n\n## Research Brief\nThis research will investigate the methodology and infrastructure of using Large Language Models as automated evaluators, focusing on core architectures like pairwise comparison and direct scoring. It will also examine the metrics used to validate these judges against human baselines and explore their practical applications in RAG and model alignment.\n\n## Findings to Synthesize\n\n### Bias & Limitations\n- [HIGH] LLM Judges exhibit systematic cognitive biases, most notably 'self-preference bias' (favoring their own generated outputs), 'position bias' (favoring responses appearing earlier or later in a pair), and 'verbosity bias' (rating longer responses higher regardless of quality).\n  Sources: src-67c025c2, src-45a8de46, src-48201995, src-e0d1753b, src-a4549098, src-7c38a7f7\n\n### Mitigation Techniques\n- [HIGH] To mitigate evaluation biases, researchers employ techniques such as 'Chain-of-Thought' (CoT) prompting to induce reasoning before scoring, position swapping (running the eval twice with swapped orders) to average out position bias, and 'Co-Eval' frameworks that augment LLMs with objective machine metrics.\n  Sources: src-8d0c93da, src-66027906, src-48201995, src-e0d1753b\n\n### Architecture & Performance\n- [MEDIUM] Two primary architectures dominate LLM-as-a-Judge: 'Pairwise Comparison' (mimicking human preference testing like Chatbot Arena) and 'Direct Scoring/Pointwise' (assigning absolute scores like 1-10), with strong models like GPT-4 achieving over 80% agreement with human annotators in general chat domains.\n  Sources: src-48201995, src-51263506, src-2a4435f2\n\n### Advanced Architectures\n- [MEDIUM] Specialized 'Ensemble' or 'Judge Assembly' approaches are emerging for complex domains, such as 'SWE-Judge' for software engineering, which combines LLM reasoning with code execution/static analysis to bridge the gap with human verification in technical tasks.\n  Sources: src-1e5014bd, src-78c4677b, src-2a4435f2\n\n## Knowledge Gaps Identified\n- [unresolved] While 'Judge Assembly' and ensemble methods are mentioned, specific architectural patterns for orchestrating these cost-effectively in production (latency vs. accuracy trade-offs) are under-documented in the provided sources.\n- [unresolved] The sources discuss biases extensively but lack detailed comparative data on the efficacy of 'Reference-free' vs. 'Reference-based' evaluation across different model sizes (e.g., can a small model effectively judge a large model if provided a reference?).\n\n## Source Reference\n- src-67c025c2: Self-Preference Bias in LLM-as-a-Judge [high]\n  URL: https://openreview.net/forum?id=Ns8zGZ0lmM\n- src-45a8de46: Self-Preference Bias in LLM-as-a-Judge [high]\n  URL: https://arxiv.org/html/2410.21819v1\n- src-48201995: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena [high]\n  URL: https://neurips.cc/virtual/2023/poster/73434\n- src-e0d1753b: Mitigating the Bias of Large Language Model Evaluation [medium]\n  URL: https://aclanthology.org/2024.ccl-1.101.pdf\n- src-8d0c93da: 5 Techniques to Improve LLM-Judges : r/LLMDevs [medium]\n  URL: https://www.reddit.com/r/LLMDevs/comments/1j3gbil/5_techniques_to_improve_llmjudges/\n- src-08525cff: LLM-as-a-Judge: Unveiling Its Potential and Applications - Medium [medium]\n  URL: https://medium.com/@ganeshkannappan/llm-as-a-judge-unveiling-its-potential-and-applications-cbfb3db14e26\n- src-51263506: Using LLMs for Evaluation - by Cameron R. Wolfe, Ph.D. [medium]\n  URL: https://cameronrwolfe.substack.com/p/llm-as-a-judge\n- src-2a4435f2: A Survey on LLM-as-a-Judge - arXiv [high]\n  URL: https://arxiv.org/html/2411.15594v1\n- src-bbd215f1: LLM-as-a-Judge - by Nilesh Barla - Adaline Labs [medium]\n  URL: https://labs.adaline.ai/p/llm-as-a-judge\n- src-78c4677b: LLM-as-a-Judge: How AI Can Evaluate AI Faster and Smarter [medium]\n  URL: https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-j/\n- src-6ba1f0a1: Understanding Bias in LLM-as-a-Judge Systems [medium]\n  URL: https://ragmetrics.ai/blog/understanding-bias-in-llm-as-a-judge-systems\n- src-a4549098: A Systematic Study of Position Bias in LLM-as-a-Judge - arXiv [high]\n  URL: https://arxiv.org/html/2406.07791v7\n- src-bef824af: The 5 Biases That Can Silently Kill Your LLM Evaluations ... [medium]\n  URL: https://www.sebastiansigl.com/blog/llm-judge-biases-and-how-to-fix-them/\n- src-7c38a7f7: Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge [medium]\n  URL: https://llm-judge-bias.github.io\n- src-c33a2512: Evaluating and Mitigating LLM-as-a-judge Bias in ... [high]\n  URL: https://arxiv.org/abs/2510.12462\n- src-1e5014bd: An LLM-as-Judge Metric for Bridging the Gap with Human ... - arXiv [high]\n  URL: https://arxiv.org/html/2505.20854v1\n- src-db258615: LLM Evaluation Frameworks, Metrics & Methods Explained [medium]\n  URL: https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained\n- src-3f4263f1: Large Language Model Evaluation in '26: 10+ Metrics & Methods [medium]\n  URL: https://research.aimultiple.com/large-language-model-evaluation/\n- src-0378afab: LLM Evaluation in 2025: Metrics, RAG, LLM-as-Judge & Best Practices [medium]\n  URL: https://medium.com/@QuarkAndCode/llm-evaluation-in-2025-metrics-rag-llm-as-judge-best-practices-ad2872cfa7cb\n- src-66027906: Co-Eval: Augmenting LLM-based Evaluation with Machine Metrics [medium]\n  URL: https://aclanthology.org/2025.emnlp-main.1307.pdf\n- src-03c1a7f3: Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge [high]\n  URL: https://arxiv.org/html/2410.02736v1\n- src-7c2fcbc0: The Intricacies of Evaluating Large Language Models with LLM-as-a ... [medium]\n  URL: https://medium.com/@vineethveetil/the-intricacies-of-evaluating-large-language-models-with-llm-as-a-judge-8034a3f34b28\n- src-fa92de56: LLM-as-a-Judge Simply... [medium]\n  URL: https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method\n- src-d64f918b: A Survey on LLM-as-a-Judge [high]\n  URL: https://arxiv.org/html/2411.15594v4\n- src-770e362f: A Survey on LLM-as-a-Judge - arXiv [high]\n  URL: https://arxiv.org/html/2411.15594v6\n- src-29376e6f: 3. Biases: Position, Length... [medium]\n  URL: https://www.emergentmind.com/topics/llm-as-a-judge-evaluations\n- src-a040edba: LLM-Judge Protocol: Methods & Applications - Emergent Mind [medium]\n  URL: https://www.emergentmind.com/topics/llm-judge-protocol\n- src-05ffcaa9: Judge's Verdict: A Comprehensive Analysis of LLM ... [high]\n  URL: https://arxiv.org/html/2510.09738v1\n- src-a71f7557: An LLM-as-Judge Metric for Bridging the Gap with Human ... [high]\n  URL: https://arxiv.org/abs/2505.20854\n- src-35901ad6: LLM-as-a-judge: a complete guide to using LLMs for evaluations [medium]\n  URL: https://www.evidentlyai.com/llm-guide/llm-as-a-judge\n\n## Instructions\nGenerate a comprehensive research report addressing the query: 'LLM Judges: techniques, architectures, evaluation methods, and applications for using large language models as automated evaluators and judges'\n\nThis is iteration 1 of 3.\nTotal findings: 4\nTotal sources: 53\nUnresolved gaps: 2\n\nCreate a well-structured markdown report following the format specified.", "raw_response": "# Research Report: LLM Judges: Techniques, Architectures, and Evaluation Methods\n\n## Executive Summary\nThe use of Large Language Models (LLMs) as automated judges represents a significant shift in evaluation methodology, offering a scalable alternative to human annotation. Current research indicates that while LLMs\u2014particularly state-of-the-art models like GPT-4\u2014can achieve high agreement (over 80%) with human baselines, they are prone to systematic cognitive biases similar to those found in human psychology. These biases, including self-preference and position bias, pose substantial challenges to the reliability of automated evaluation.\n\nTo counter these limitations, the field is evolving toward more sophisticated architectures and mitigation strategies. Simple \"prompt-and-score\" methods are being replaced by robust frameworks involving pairwise comparisons, position swapping, and Chain-of-Thought (CoT) reasoning. Furthermore, advanced \"Judge Assembly\" systems are emerging for complex domains, integrating LLM reasoning with objective tools (such as code execution) to bridge the gap between stochastic language generation and verifiable ground truth.\n\n## Key Findings\n\n### Cognitive Biases in Automated Evaluation\n- **Systematic Bias:** LLM judges exhibit distinct biases that can skew evaluation results. The most prominent include \"self-preference bias,\" where models disproportionately favor outputs generated by themselves or their own model family **[src-67c025c2]** **[src-45a8de46]**.\n- **Structural Biases:** \"Position bias\" leads judges to favor responses appearing in specific orders (e.g., the first or last option in a pair) **[src-a4549098]**. Additionally, \"verbosity bias\" results in higher scores for longer responses, regardless of the actual quality or conciseness of the answer **[src-48201995]** **[src-7c38a7f7]**.\n\n### Mitigation Strategies and Best Practices\n- **Algorithmic Corrections:** To neutralize position bias, position swapping is a standard technique where evaluations are run twice with the order of candidates reversed, averaging the results **[src-48201995]** **[src-e0d1753b]**.\n- **Reasoning Enhancement:** Implementing \"Chain-of-Thought\" (CoT) prompting, which forces the model to articulate its reasoning logic before assigning a score, has been shown to improve judgment quality and consistency **[src-8d0c93da]**.\n- **Hybrid Frameworks:** \"Co-Eval\" approaches augment subjective LLM judgments with objective machine metrics, creating a more balanced evaluation signal **[src-66027906]**.\n\n### Core Architectures and Performance\n- **Primary Methodologies:** Two dominant architectures define the landscape: \"Pairwise Comparison,\" which mimics human preference testing (e.g., Chatbot Arena), and \"Direct Scoring/Pointwise,\" where models assign absolute scores (e.g., 1-10 scale) **[src-51263506]** **[src-2a4435f2]**.\n- **Human Agreement:** Strong foundation models like GPT-4 have demonstrated high efficacy, achieving over 80% agreement with human annotators in general chat domains, validating their utility as scalable surrogates for human judgment **[src-48201995]**.\n\n### Advanced Domain-Specific Architectures\n- **Judge Assemblies:** For technical domains requiring high precision, simple LLM scoring is insufficient. New \"Ensemble\" or \"Judge Assembly\" architectures are emerging, such as \"SWE-Judge\" for software engineering. These systems combine LLM reasoning with deterministic signals like static analysis and code execution to verify technical correctness **[src-1e5014bd]** **[src-78c4677b]**.\n\n## Analysis\n\n### Supporting Evidence\nThere is high confidence across multiple sources regarding the existence and nature of LLM biases. The \"self-preference\" and \"verbosity\" biases are well-documented across different model families **[src-67c025c2]** **[src-7c38a7f7]**. Similarly, the efficacy of pairwise comparison as a robust evaluation method (despite being more token-intensive) is strongly supported by the success of benchmarks like MT-Bench and Chatbot Arena **[src-48201995]**.\n\n### Conflicting Information\nWhile \"Judge Assemblies\" are proposed as a solution for complex tasks, there is an implicit tension between the stochastic nature of LLMs and the deterministic needs of technical evaluation. The integration of \"Co-Eval\" machine metrics **[src-66027906]** suggests that for certain tasks, LLMs alone are viewed as insufficient by some researchers, whereas others focus on pure-LLM ensembles.\n\n### Limitations\n- **Orchestration Gaps:** While advanced architectures like Judge Assemblies are theoretically sound, specific architectural patterns for orchestrating these cost-effectively (balancing latency vs. accuracy) remain under-documented.\n- **Reference Dependency:** The research currently lacks detailed comparative data on the efficacy of \"Reference-free\" vs. \"Reference-based\" evaluation across varying model sizes (e.g., the capability of smaller models to judge larger ones when provided with a gold-standard reference).\n\n## Sources\n- **[src-67c025c2]** [Self-Preference Bias in LLM-as-a-Judge](https://openreview.net/forum?id=Ns8zGZ0lmM)\n- **[src-45a8de46]** [Self-Preference Bias in LLM-as-a-Judge](https://arxiv.org/html/2410.21819v1)\n- **[src-48201995]** [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://neurips.cc/virtual/2023/poster/73434)\n- **[src-e0d1753b]** [Mitigating the Bias of Large Language Model Evaluation](https://aclanthology.org/2024.ccl-1.101.pdf)\n- **[src-8d0c93da]** [5 Techniques to Improve LLM-Judges : r/LLMDevs](https://www.reddit.com/r/LLMDevs/comments/1j3gbil/5_techniques_to_improve_llmjudges/)\n- **[src-51263506]** [Using LLMs for Evaluation - by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/llm-as-a-judge)\n- **[src-2a4435f2]** [A Survey on LLM-as-a-Judge - arXiv](https://arxiv.org/html/2411.15594v1)\n- **[src-1e5014bd]** [An LLM-as-Judge Metric for Bridging the Gap with Human ... - arXiv](https://arxiv.org/html/2505.20854v1)\n- **[src-78c4677b]** [LLM-as-a-Judge: How AI Can Evaluate AI Faster and Smarter](https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-j/)\n- **[src-a4549098]** [A Systematic Study of Position Bias in LLM-as-a-Judge - arXiv](https://arxiv.org/html/2406.07791v7)\n- **[src-7c38a7f7]** [Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge](https://llm-judge-bias.github.io)\n- **[src-66027906]** [Co-Eval: Augmenting LLM-based Evaluation with Machine Metrics](https://aclanthology.org/2025.emnlp-main.1307.pdf)\n\n## Conclusions\nTo deploy effective LLM-as-a-Judge systems, practitioners must move beyond simple direct scoring. The evidence suggests that a robust evaluation pipeline should default to **pairwise comparisons** with **position swapping** to mitigate structural biases. When using specific model families, one must actively correct for **self-preference bias**, potentially by using a different model family for evaluation than for generation.\n\nFor specialized domains like software engineering, relying solely on linguistic evaluation is risky; **Judge Assemblies** that incorporate execution-based verification (like SWE-Judge) are recommended to ensure technical accuracy. Future implementation efforts should focus on defining cost-effective orchestration patterns for these assemblies to make them viable for production use.", "report": "# Research Report: LLM Judges: Techniques, Architectures, and Evaluation Methods\n\n## Executive Summary\nThe use of Large Language Models (LLMs) as automated judges represents a significant shift in evaluation methodology, offering a scalable alternative to human annotation. Current research indicates that while LLMs\u2014particularly state-of-the-art models like GPT-4\u2014can achieve high agreement (over 80%) with human baselines, they are prone to systematic cognitive biases similar to those found in human psychology. These biases, including self-preference and position bias, pose substantial challenges to the reliability of automated evaluation.\n\nTo counter these limitations, the field is evolving toward more sophisticated architectures and mitigation strategies. Simple \"prompt-and-score\" methods are being replaced by robust frameworks involving pairwise comparisons, position swapping, and Chain-of-Thought (CoT) reasoning. Furthermore, advanced \"Judge Assembly\" systems are emerging for complex domains, integrating LLM reasoning with objective tools (such as code execution) to bridge the gap between stochastic language generation and verifiable ground truth.\n\n## Key Findings\n\n### Cognitive Biases in Automated Evaluation\n- **Systematic Bias:** LLM judges exhibit distinct biases that can skew evaluation results. The most prominent include \"self-preference bias,\" where models disproportionately favor outputs generated by themselves or their own model family **[src-67c025c2]** **[src-45a8de46]**.\n- **Structural Biases:** \"Position bias\" leads judges to favor responses appearing in specific orders (e.g., the first or last option in a pair) **[src-a4549098]**. Additionally, \"verbosity bias\" results in higher scores for longer responses, regardless of the actual quality or conciseness of the answer **[src-48201995]** **[src-7c38a7f7]**.\n\n### Mitigation Strategies and Best Practices\n- **Algorithmic Corrections:** To neutralize position bias, position swapping is a standard technique where evaluations are run twice with the order of candidates reversed, averaging the results **[src-48201995]** **[src-e0d1753b]**.\n- **Reasoning Enhancement:** Implementing \"Chain-of-Thought\" (CoT) prompting, which forces the model to articulate its reasoning logic before assigning a score, has been shown to improve judgment quality and consistency **[src-8d0c93da]**.\n- **Hybrid Frameworks:** \"Co-Eval\" approaches augment subjective LLM judgments with objective machine metrics, creating a more balanced evaluation signal **[src-66027906]**.\n\n### Core Architectures and Performance\n- **Primary Methodologies:** Two dominant architectures define the landscape: \"Pairwise Comparison,\" which mimics human preference testing (e.g., Chatbot Arena), and \"Direct Scoring/Pointwise,\" where models assign absolute scores (e.g., 1-10 scale) **[src-51263506]** **[src-2a4435f2]**.\n- **Human Agreement:** Strong foundation models like GPT-4 have demonstrated high efficacy, achieving over 80% agreement with human annotators in general chat domains, validating their utility as scalable surrogates for human judgment **[src-48201995]**.\n\n### Advanced Domain-Specific Architectures\n- **Judge Assemblies:** For technical domains requiring high precision, simple LLM scoring is insufficient. New \"Ensemble\" or \"Judge Assembly\" architectures are emerging, such as \"SWE-Judge\" for software engineering. These systems combine LLM reasoning with deterministic signals like static analysis and code execution to verify technical correctness **[src-1e5014bd]** **[src-78c4677b]**.\n\n## Analysis\n\n### Supporting Evidence\nThere is high confidence across multiple sources regarding the existence and nature of LLM biases. The \"self-preference\" and \"verbosity\" biases are well-documented across different model families **[src-67c025c2]** **[src-7c38a7f7]**. Similarly, the efficacy of pairwise comparison as a robust evaluation method (despite being more token-intensive) is strongly supported by the success of benchmarks like MT-Bench and Chatbot Arena **[src-48201995]**.\n\n### Conflicting Information\nWhile \"Judge Assemblies\" are proposed as a solution for complex tasks, there is an implicit tension between the stochastic nature of LLMs and the deterministic needs of technical evaluation. The integration of \"Co-Eval\" machine metrics **[src-66027906]** suggests that for certain tasks, LLMs alone are viewed as insufficient by some researchers, whereas others focus on pure-LLM ensembles.\n\n### Limitations\n- **Orchestration Gaps:** While advanced architectures like Judge Assemblies are theoretically sound, specific architectural patterns for orchestrating these cost-effectively (balancing latency vs. accuracy) remain under-documented.\n- **Reference Dependency:** The research currently lacks detailed comparative data on the efficacy of \"Reference-free\" vs. \"Reference-based\" evaluation across varying model sizes (e.g., the capability of smaller models to judge larger ones when provided with a gold-standard reference).\n\n## Sources\n- **[src-67c025c2]** [Self-Preference Bias in LLM-as-a-Judge](https://openreview.net/forum?id=Ns8zGZ0lmM)\n- **[src-45a8de46]** [Self-Preference Bias in LLM-as-a-Judge](https://arxiv.org/html/2410.21819v1)\n- **[src-48201995]** [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://neurips.cc/virtual/2023/poster/73434)\n- **[src-e0d1753b]** [Mitigating the Bias of Large Language Model Evaluation](https://aclanthology.org/2024.ccl-1.101.pdf)\n- **[src-8d0c93da]** [5 Techniques to Improve LLM-Judges : r/LLMDevs](https://www.reddit.com/r/LLMDevs/comments/1j3gbil/5_techniques_to_improve_llmjudges/)\n- **[src-51263506]** [Using LLMs for Evaluation - by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/llm-as-a-judge)\n- **[src-2a4435f2]** [A Survey on LLM-as-a-Judge - arXiv](https://arxiv.org/html/2411.15594v1)\n- **[src-1e5014bd]** [An LLM-as-Judge Metric for Bridging the Gap with Human ... - arXiv](https://arxiv.org/html/2505.20854v1)\n- **[src-78c4677b]** [LLM-as-a-Judge: How AI Can Evaluate AI Faster and Smarter](https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-j/)\n- **[src-a4549098]** [A Systematic Study of Position Bias in LLM-as-a-Judge - arXiv](https://arxiv.org/html/2406.07791v7)\n- **[src-7c38a7f7]** [Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge](https://llm-judge-bias.github.io)\n- **[src-66027906]** [Co-Eval: Augmenting LLM-based Evaluation with Machine Metrics](https://aclanthology.org/2025.emnlp-main.1307.pdf)\n\n## Conclusions\nTo deploy effective LLM-as-a-Judge systems, practitioners must move beyond simple direct scoring. The evidence suggests that a robust evaluation pipeline should default to **pairwise comparisons** with **position swapping** to mitigate structural biases. When using specific model families, one must actively correct for **self-preference bias**, potentially by using a different model family for evaluation than for generation.\n\nFor specialized domains like software engineering, relying solely on linguistic evaluation is risky; **Judge Assemblies** that incorporate execution-based verification (like SWE-Judge) are recommended to ensure technical accuracy. Future implementation efforts should focus on defining cost-effective orchestration patterns for these assemblies to make them viable for production use.", "report_length": 7355}}
{"timestamp": "2026-01-01T01:15:19.626011Z", "event_id": "8d0563e19d2a483bac134aed7cd3559a", "event_type": "phase_complete", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "synthesis", "iteration": 1, "data": {"phase": "synthesis", "duration_ms": 31853.563758952077}}
{"timestamp": "2026-01-01T01:15:19.626530Z", "event_id": "76aa8d8843234b7e9bb6aeddea702df6", "event_type": "phase_start", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "refinement", "iteration": 1, "data": {"phase": "refinement"}}
{"timestamp": "2026-01-01T01:15:42.453239Z", "event_id": "59748731f6a64e80a6ee8d293e3821e5", "event_type": "refinement_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "refinement", "iteration": 1, "data": {"provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview", "tokens_used": 11024, "duration_ms": null, "system_prompt": "You are a research refiner. Your task is to analyze knowledge gaps identified during research and generate focused follow-up queries to address them.\n\nYour response MUST be valid JSON with this exact structure:\n{\n    \"gap_analysis\": [\n        {\n            \"gap_id\": \"gap-xxx\",\n            \"severity\": \"critical|moderate|minor\",\n            \"addressable\": true,\n            \"rationale\": \"Why this gap matters and whether it can be addressed\"\n        }\n    ],\n    \"follow_up_queries\": [\n        {\n            \"query\": \"A specific, focused search query to address the gap\",\n            \"target_gap_id\": \"gap-xxx\",\n            \"rationale\": \"How this query will fill the gap\",\n            \"priority\": 1\n        }\n    ],\n    \"addressed_gap_ids\": [\"gap-xxx\"],\n    \"iteration_recommendation\": {\n        \"should_iterate\": true,\n        \"rationale\": \"Why iteration is or isn't recommended\"\n    }\n}\n\nGuidelines:\n- Assess each gap's severity: \"critical\" (blocks conclusions), \"moderate\" (affects confidence), \"minor\" (nice to have)\n- Only mark gaps as addressable if follow-up research can realistically fill them\n- Generate 1-3 highly focused follow-up queries per addressable gap\n- Priority 1 is highest priority\n- Mark gaps as addressed if the current report already covers them adequately\n- Recommend iteration only if there are addressable critical/moderate gaps AND value exceeds research cost\n\nIMPORTANT: Return ONLY valid JSON, no markdown formatting or extra text.", "user_prompt": "# Research Query\nLLM Judges: techniques, architectures, evaluation methods, and applications for using large language models as automated evaluators and judges\n\n## Research Status\n- Iteration: 1/3\n- Sources examined: 53\n- Findings extracted: 4\n- Unresolved gaps: 2\n\n## Current Report Summary\n# Research Report: LLM Judges: Techniques, Architectures, and Evaluation Methods\n\n## Executive Summary\nThe use of Large Language Models (LLMs) as automated judges represents a significant shift in evaluation methodology, offering a scalable alternative to human annotation. Current research indicates that while LLMs\u2014particularly state-of-the-art models like GPT-4\u2014can achieve high agreement (over 80%) with human baselines, they are prone to systematic cognitive biases similar to those found in human psychology. These biases, including self-preference and position bias, pose substantial challenges to the reliability of automated evaluation.\n\nTo counter these limitations, the field is evolving toward more sophisticated architectures and mitigation strategies. Simple \"prompt-and-score\" methods are being replaced by robust frameworks involving pairwise comparisons, position swapping, and Chain-of-Thought (CoT) reasoning. Furthermore, advanced \"Judge Assembly\" systems are emerging for complex domains, integrating LLM reasoning with objective tools (such as code execution) to bridge the gap between stochastic language generation and verifiable ground truth.\n\n## Key Findings\n\n### Cognitive Biases in Automated Evaluation\n- **Systematic Bias:** LLM judges exhibit distinct biases that can skew evaluation results. The most prominent include \"self-preference bias,\" where models disproportionately favor outputs generated by themselves or their own model family **[src-67c025c2]** **[src-45a8de46]**.\n- **Structural Biases:** \"Position bias\" leads judges to favor responses appearing in specific orders (e.g., the first or last option in a pair) **[src-a4549098]**. Additionally, \"verbosity bias\" results in higher scores for longer responses, regardless of the actual quality or conciseness of the answer **[src-48201995]** **[src-7c38a7f7]**.\n\n### Mitigation Strategies and Best Practices\n- **Algorithmic Corrections:** To neutralize position bias, position swapping is a standard technique\n\n[Report truncated...]\n\n## Unresolved Knowledge Gaps\n\n### Gap: gap-b3e1de76\nDescription: While 'Judge Assembly' and ensemble methods are mentioned, specific architectural patterns for orchestrating these cost-effectively in production (latency vs. accuracy trade-offs) are under-documented in the provided sources.\nPriority: 1\nSuggested queries from analysis:\n  - architectural patterns for LLM judge ensembles production\n  - latency cost trade-off LLM-as-a-judge assembly\n\n### Gap: gap-dd9a1a3b\nDescription: The sources discuss biases extensively but lack detailed comparative data on the efficacy of 'Reference-free' vs. 'Reference-based' evaluation across different model sizes (e.g., can a small model effectively judge a large model if provided a reference?).\nPriority: 2\nSuggested queries from analysis:\n  - reference-free vs reference-based LLM evaluation accuracy comparison\n  - small model judge performance with ground truth references\n\n## High-Confidence Findings Already Established\n- LLM Judges exhibit systematic cognitive biases, most notably 'self-preference bias' (favoring their own generated outputs), 'position bias' (favoring responses appearing earlier or later in a pair), a\n- To mitigate evaluation biases, researchers employ techniques such as 'Chain-of-Thought' (CoT) prompting to induce reasoning before scoring, position swapping (running the eval twice with swapped order\n\n## Instructions\n1. Analyze each gap for severity and addressability\n2. Generate focused follow-up queries for addressable gaps\n3. Mark any gaps that are actually addressed by existing findings\n4. Recommend whether iteration is worthwhile given remaining gaps\n\nReturn your analysis as JSON.", "raw_response": "```json\n{\n    \"gap_analysis\": [\n        {\n            \"gap_id\": \"gap-b3e1de76\",\n            \"severity\": \"critical\",\n            \"addressable\": true,\n            \"rationale\": \"Understanding production architectures and cost/latency trade-offs is essential for the 'architectures' and 'applications' aspect of the research scope, moving beyond theoretical capability to practical implementation.\"\n        },\n        {\n            \"gap_id\": \"gap-dd9a1a3b\",\n            \"severity\": \"moderate\",\n            \"addressable\": true,\n            \"rationale\": \"The comparison between reference-based and reference-free evaluation, especially regarding model size (scalability), is a key 'evaluation method' detail needed for a comprehensive report.\"\n        }\n    ],\n    \"follow_up_queries\": [\n        {\n            \"query\": \"production architectures for LLM judge ensembles and cascading evaluation\",\n            \"target_gap_id\": \"gap-b3e1de76\",\n            \"rationale\": \"Targets specific architectural patterns like cascading or voting systems used in production environments.\",\n            \"priority\": 1\n        },\n        {\n            \"query\": \"cost latency trade-offs in LLM-as-a-judge systems optimization\",\n            \"target_gap_id\": \"gap-b3e1de76\",\n            \"rationale\": \"Specifically searches for data or methodologies regarding the economic and performance efficiency of judge systems.\",\n            \"priority\": 1\n        },\n        {\n            \"query\": \"small language models as judges reference-based vs reference-free performance\",\n            \"target_gap_id\": \"gap-dd9a1a3b\",\n            \"rationale\": \"Investigates whether providing ground truth enables smaller, cheaper models to perform comparably to larger models.\",\n            \"priority\": 2\n        },\n        {\n            \"query\": \"accuracy of small LLM judges compared to GPT-4 with and without gold references\",\n            \"target_gap_id\": \"gap-dd9a1a3b\",\n            \"rationale\": \"Directly compares model sizes in the context of reference availability to address the scalability gap.\",\n            \"priority\": 2\n        }\n    ],\n    \"addressed_gap_ids\": [],\n    \"iteration_recommendation\": {\n        \"should_iterate\": true,\n        \"rationale\": \"There are critical, addressable gaps regarding the practical implementation (architecture, cost) and specific evaluation dynamics (references, model size). Addressing these will significantly improve the report's utility.\"\n    }\n}\n```", "parse_success": true, "gap_analysis": [{"gap_id": "gap-b3e1de76", "severity": "critical", "addressable": true, "rationale": "Understanding production architectures and cost/latency trade-offs is essential for the 'architectures' and 'applications' aspect of the research scope, moving beyond theoretical capability to practical implementation."}, {"gap_id": "gap-dd9a1a3b", "severity": "moderate", "addressable": true, "rationale": "The comparison between reference-based and reference-free evaluation, especially regarding model size (scalability), is a key 'evaluation method' detail needed for a comprehensive report."}], "follow_up_queries": [{"query": "production architectures for LLM judge ensembles and cascading evaluation", "target_gap_id": "gap-b3e1de76", "rationale": "Targets specific architectural patterns like cascading or voting systems used in production environments.", "priority": 1}, {"query": "cost latency trade-offs in LLM-as-a-judge systems optimization", "target_gap_id": "gap-b3e1de76", "rationale": "Specifically searches for data or methodologies regarding the economic and performance efficiency of judge systems.", "priority": 1}, {"query": "small language models as judges reference-based vs reference-free performance", "target_gap_id": "gap-dd9a1a3b", "rationale": "Investigates whether providing ground truth enables smaller, cheaper models to perform comparably to larger models.", "priority": 2}, {"query": "accuracy of small LLM judges compared to GPT-4 with and without gold references", "target_gap_id": "gap-dd9a1a3b", "rationale": "Directly compares model sizes in the context of reference availability to address the scalability gap.", "priority": 2}], "addressed_gap_ids": [], "should_iterate": true}}
{"timestamp": "2026-01-01T01:15:42.454537Z", "event_id": "091798c6b2a14dc19bc6492d09d191d5", "event_type": "phase_complete", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "refinement", "iteration": 1, "data": {"phase": "refinement", "duration_ms": 22829.390179016627}}
{"timestamp": "2026-01-01T01:15:42.454848Z", "event_id": "e9f097abda8e47eca139b3d9f3a8098c", "event_type": "phase_start", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"phase": "gathering"}}
{"timestamp": "2026-01-01T01:15:46.131897Z", "event_id": "80d1eb43e5ca41938a816bc8c8a73ef3", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "tavily", "sub_query_id": "subq-8f3a36a7", "sub_query": "small language models as judges reference-based vs reference-free performance", "sources_added": 5}}
{"timestamp": "2026-01-01T01:15:46.147251Z", "event_id": "37544480abe04fa388c99266f5af9094", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "tavily", "sub_query_id": "subq-539bc1e7", "sub_query": "production architectures for LLM judge ensembles and cascading evaluation", "sources_added": 5}}
{"timestamp": "2026-01-01T01:15:46.594598Z", "event_id": "81003f6b05f24fafae37aa449b207886", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "perplexity", "sub_query_id": "subq-539bc1e7", "sub_query": "production architectures for LLM judge ensembles and cascading evaluation", "sources_added": 5}}
{"timestamp": "2026-01-01T01:15:46.644497Z", "event_id": "8d8c2c5c995440f8886ffbb05b161a78", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "tavily", "sub_query_id": "subq-03f5c312", "sub_query": "cost latency trade-offs in LLM-as-a-judge systems optimization", "sources_added": 5}}
{"timestamp": "2026-01-01T01:15:46.666211Z", "event_id": "baa3cef8c13c45978855a9ea16458c58", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "perplexity", "sub_query_id": "subq-8f3a36a7", "sub_query": "small language models as judges reference-based vs reference-free performance", "sources_added": 5}}
{"timestamp": "2026-01-01T01:15:46.994685Z", "event_id": "7569a8b0c87547b89b40b29efe6c4bde", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "perplexity", "sub_query_id": "subq-03f5c312", "sub_query": "cost latency trade-offs in LLM-as-a-judge systems optimization", "sources_added": 5}}
{"timestamp": "2026-01-01T01:15:47.024351Z", "event_id": "9be8afdd0bff413d8a2b6061e6e45fd6", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "google", "sub_query_id": "subq-539bc1e7", "sub_query": "production architectures for LLM judge ensembles and cascading evaluation", "sources_added": 5}}
{"timestamp": "2026-01-01T01:15:47.110638Z", "event_id": "f55af03aae08404eaa8c5b515c7ef861", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "google", "sub_query_id": "subq-8f3a36a7", "sub_query": "small language models as judges reference-based vs reference-free performance", "sources_added": 5}}
{"timestamp": "2026-01-01T01:15:47.322160Z", "event_id": "e417a005623b472e860a8593ce9562e3", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "semantic_scholar", "sub_query_id": "subq-8f3a36a7", "sub_query": "small language models as judges reference-based vs reference-free performance", "sources_added": 0}}
{"timestamp": "2026-01-01T01:15:47.322640Z", "event_id": "bcb5b44ed36944c1a41fe511d2d395f6", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "semantic_scholar", "sub_query_id": "subq-539bc1e7", "sub_query": "production architectures for LLM judge ensembles and cascading evaluation", "sources_added": 0}}
{"timestamp": "2026-01-01T01:15:47.448875Z", "event_id": "d0aed69edb26475aad373c9b51cb097d", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "google", "sub_query_id": "subq-03f5c312", "sub_query": "cost latency trade-offs in LLM-as-a-judge systems optimization", "sources_added": 5}}
{"timestamp": "2026-01-01T01:15:47.622739Z", "event_id": "0937c066bb78421380ca7d4a5c0672fe", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "semantic_scholar", "sub_query_id": "subq-03f5c312", "sub_query": "cost latency trade-offs in LLM-as-a-judge systems optimization", "sources_added": 0}}
{"timestamp": "2026-01-01T01:15:49.331433Z", "event_id": "8c9c9042e30a48f5a926297674550924", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "tavily", "sub_query_id": "subq-f848a83c", "sub_query": "accuracy of small LLM judges compared to GPT-4 with and without gold references", "sources_added": 5}}
{"timestamp": "2026-01-01T01:15:49.740187Z", "event_id": "327a8937ef614af38b67d9dba180e815", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "perplexity", "sub_query_id": "subq-f848a83c", "sub_query": "accuracy of small LLM judges compared to GPT-4 with and without gold references", "sources_added": 5}}
{"timestamp": "2026-01-01T01:15:50.152945Z", "event_id": "340d3aebb70d40638516038932b67a8e", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "google", "sub_query_id": "subq-f848a83c", "sub_query": "accuracy of small LLM judges compared to GPT-4 with and without gold references", "sources_added": 5}}
{"timestamp": "2026-01-01T01:15:50.597177Z", "event_id": "acb4fda18ec242f4a84900f4f65bc708", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"provider": "semantic_scholar", "sub_query_id": "subq-f848a83c", "sub_query": "accuracy of small LLM judges compared to GPT-4 with and without gold references", "sources_added": 0}}
{"timestamp": "2026-01-01T01:15:50.608322Z", "event_id": "592605c0989e445f8d17b807070887c9", "event_type": "gathering_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"source_count": 51, "queries_executed": 4, "queries_failed": 0, "unique_urls": 51, "providers_used": ["tavily", "perplexity", "google", "semantic_scholar"], "providers_unavailable": []}}
{"timestamp": "2026-01-01T01:15:50.611263Z", "event_id": "ed054592965c40bcaa25a19393a0be1c", "event_type": "phase_complete", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 2, "data": {"phase": "gathering", "duration_ms": 8156.408615002874}}
{"timestamp": "2026-01-01T01:15:50.611651Z", "event_id": "b6c78dc79b7944449600a0be61e0c47f", "event_type": "phase_start", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "analysis", "iteration": 2, "data": {"phase": "analysis"}}
{"timestamp": "2026-01-01T01:16:16.909123Z", "event_id": "8afbefff3ee84f2497d9943b6fac1c0f", "event_type": "analysis_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "analysis", "iteration": 2, "data": {"provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview", "tokens_used": 18056, "duration_ms": null, "system_prompt": "You are a research analyst. Your task is to analyze research sources and extract key findings, assess their quality, and identify knowledge gaps.\n\nYour response MUST be valid JSON with this exact structure:\n{\n    \"findings\": [\n        {\n            \"content\": \"A clear, specific finding or insight extracted from the sources\",\n            \"confidence\": \"low|medium|high\",\n            \"source_ids\": [\"src-xxx\", \"src-yyy\"],\n            \"category\": \"optional category/theme\"\n        }\n    ],\n    \"gaps\": [\n        {\n            \"description\": \"Description of missing information or unanswered question\",\n            \"suggested_queries\": [\"follow-up query 1\", \"follow-up query 2\"],\n            \"priority\": 1\n        }\n    ],\n    \"quality_updates\": [\n        {\n            \"source_id\": \"src-xxx\",\n            \"quality\": \"low|medium|high\"\n        }\n    ]\n}\n\nGuidelines for findings:\n- Extract 2-5 key findings from the sources\n- Each finding should be a specific, actionable insight\n- Confidence levels: \"low\" (single weak source), \"medium\" (multiple sources or one authoritative), \"high\" (multiple authoritative sources agree)\n- Include source_ids that support each finding\n- Categorize findings by theme when applicable\n\nGuidelines for gaps:\n- Identify 1-3 knowledge gaps or unanswered questions\n- Provide specific follow-up queries that could fill each gap\n- Priority 1 is most important, higher numbers are lower priority\n\nGuidelines for quality_updates:\n- Assess source quality based on authority, relevance, and recency\n- \"low\" = questionable reliability, \"medium\" = generally reliable, \"high\" = authoritative\n\nIMPORTANT: Return ONLY valid JSON, no markdown formatting or extra text.", "user_prompt": "Original Research Query: LLM Judges: techniques, architectures, evaluation methods, and applications for using large language models as automated evaluators and judges\n\nResearch Brief:\nThis research will investigate the methodology and infrastructure of using Large Language Models as automated evaluators, focusing on core architectures like pairwise comparison and direct scoring. It will also examine the metrics used to validate these judges against human baselines and explore their practical applications in RAG and model alignment.\n\nSources to Analyze:\n\nSource 1 (ID: src-67c025c2):\n  Title: Self-Preference Bias in LLM-as-a-Judge\n  URL: https://openreview.net/forum?id=Ns8zGZ0lmM\n  Snippet: ## Self-Preference Bias in LLM-as-a-Judge. **TL;DR:** We propose a novel quantitative metric to measure self-preference bias in LLM-as-a-judge. However, the self-preference bias in LLMs has posed significant risks, including promoting specific styles or policies intrinsic to the LLMs. Despite the importance of this issue, there is a lack of established methods to measure the self-preference bias quantitatively, and its underlying causes are poorly understood. In this paper, we introduce a novel ...\n  Content: [Go to **ICLR 2025 Conference** homepage](/group?id=ICLR.cc/2025/Conference \"Venue Homepage\")\n\n## Self-Preference Bias in LLM-as-a-Judge\n\n### [Koki Wataoka](/profile?id=~Koki_Wataoka1 \"~Koki_Wataoka1\"), [Tsubasa Takahashi](/profile?id=~Tsubasa_Takahashi1 \"~Tsubasa_Takahashi1\"), [Ryokan Ri](/profile?id=~Ryokan_Ri1 \"~Ryokan_Ri1\")\n\n27 Sept 2024 (modified: 05 Feb 2025)Submitted to ICLR 2025Everyone[Revisions](/revisions?id=Ns8zGZ0lmM)[BibTeX](#)[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/ \"Licensed under Creative Commons Attribution 4.0 International\")\n\n**Keywords:** large language model, llm-as-a-judge, bias, fairness\n\n**TL;DR:** We propose a novel quantitative metric to measure self-preference bias in LLM-as-a-judge.\n\n**Abstract:** Automated evaluation leveraging large language models (LLMs), commonly referred to as LLM evaluators or LLM-as-a-judge, has been widely used in measuring the performance of dialogue systems. However, the self-preference bias in LLMs has posed signi...\n\nSource 2 (ID: src-45a8de46):\n  Title: Self-Preference Bias in LLM-as-a-Judge\n  URL: https://arxiv.org/html/2410.21819v1\n  Snippet: (2024) addressed quantifying self-preference bias within an evaluation approach where LLMs assign an absolute score to a single generated text. This suggests that the fundamental cause of self-preference bias may be the familiarity of the texts to the LLM evaluators, specifically how likely they are to generate the same response. The contributions of this paper are threefold: (1) We propose a new metric to quantify self-preference bias in LLMs; (2) Using this metric, we evaluate the extent of se...\n  Content: # Self-Preference Bias in LLM-as-a-Judge\n\n[Koki Wataoka](https://orcid.org/0000-0000-0000-0000)   \nSB Intuitions   \nkoki.wataoka@sbintuitions.co.jp   \n&[Tsubasa Takahashi](https://orcid.org/0000-0000-0000-0000)   \nSB Intuitions   \ntsubasa.takahashi@sbintuitions.co.jp   \n&[Ryokan Ri](https://orcid.org/0000-0000-0000-0000)   \nSB Intuitions   \nryokan.ri@sbintuitions.co.jp\n\n###### Abstract\n\nAutomated evaluation leveraging large language models (LLMs), commonly referred to as LLM evaluators or LLM-as-a-judge, has been widely used in measuring the performance of dialogue systems. However, the self-preference bias in LLMs has posed significant risks, including promoting specific styles or policies intrinsic to the LLMs. Despite the importance of this issue, there is a lack of established methods to measure the self-preference bias quantitatively, and its underlying causes are poorly understood. In this paper, we introduce a novel quantitative metric to measure the self-preference bias. Our ex...\n\nSource 3 (ID: src-48201995):\n  Title: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n  URL: https://neurips.cc/virtual/2023/poster/73434\n  Snippet: We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability\n  Content: ## Main Navigation\n\n![conference_logo](/static/core/img/neurips-navbar-logo.svg)\n\n# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n\n### Abstract\n\nEvaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences.To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions.We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them.We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform.Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\\% agreement, the same level o...\n\nSource 4 (ID: src-e0d1753b):\n  Title: Mitigating the Bias of Large Language Model Evaluation\n  URL: https://aclanthology.org/2024.ccl-1.101.pdf\n  Snippet: In this work, we propose two methods for mitigating the bias of LLM-as-a-Judge. For closed-source judge models, we propose to mitigate the bias\n  Content: Mitigating the Bias of Large Language Model Evaluation Hongli Zhou1, Hui Huang2, Yunfei Long3, Bing Xu2, Conghui Zhu2, Hailong Cao2, Muyun Yang2\u2217, Tiejun Zhao2 1School of Architecture and Design, Harbin Institute of Technology, Harbin, China 2Faculty of Computing, Harbin Institute of Technology, Harbin, China 3University of Essex {hongli.joe,huanghui}@stu.hit.edu.cn;yl20051@essex.ac.uk; {hitxb,conghui,caohailong,yangmuyun,tjzhao}@hit.edu.cn Abstract Recently, there has been a trend of evaluating the Large Language Model (LLM) quality in the flavor of LLM-as-a-Judge, namely leveraging another LLM to evaluate the current output qual-ity. However, existing judges are proven to be biased, namely they would favor answers which present better superficial quality (such as verbosity, fluency) while ignoring the instruction fol-lowing ability. In this work, we propose systematic research about the bias of LLM-as-a-Judge.\nSpecifically, for closed-source judge models, we apply calibration to miti...\n\nSource 5 (ID: src-8d0c93da):\n  Title: 5 Techniques to Improve LLM-Judges : r/LLMDevs\n  URL: https://www.reddit.com/r/LLMDevs/comments/1j3gbil/5_techniques_to_improve_llmjudges/\n  Snippet: But using LLMs as a judge does come with some drawbacks\u2014like narcissistic bias (favoring their own outputs), a preference for verbosity (over\n  Content: ![r/LLMDevs icon](https://styles.redditmedia.com/t5_7xegfq/styles/communityIcon_b553dnae9oia1.png?width=96&height=96&frame=1&auto=webp&crop=96%3A96%2Csmart&s=8ea201f189c513413bda6216591bb75e74ae6b0c)\n\n# 5 Techniques to Improve LLM-Judges\n\nLLM-based metrics are currently the best method for evaluating LLM applications. But using LLMs as a judge does come with some drawbacks\u2014like narcissistic bias (favoring their own outputs), a preference for verbosity (over concise answers), unreliable fine-grained scoring (whereas binary outputs are much more accurate), and positional bias (prefer answer choices that come up first).\n\nFortunately, there are several methods and techniques you can employ to minimize these shortcomings when creating your LLM evaluation metrics. For anyone who\u2019s interested, I\u2019ve written a more [in-depth blog here](https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method#improving-llm-judgements).\n\n# 1. Chain-Of-Thought Prompting\n\nChain-of-thou...\n\nSource 6 (ID: src-08525cff):\n  Title: LLM-as-a-Judge: Unveiling Its Potential and Applications - Medium\n  URL: https://medium.com/@ganeshkannappan/llm-as-a-judge-unveiling-its-potential-and-applications-cbfb3db14e26\n  Snippet: * **Quantitative (or numeric) Grading** \u2014 The evaluator LLM assigns a numerical score to the answer, such as 0\u201310 or 0\u2013100, based on predefined criteria. **Objective Evaluation** \u2014 Single answer grading provides an **objective** and structured way to assess a model\u2019s response. The evaluator (in this case, the LLM) checks the generated response against the reference response and scores or judges the quality based on how closely the generated answer aligns with the reference answer in terms of acc...\n  Content: [Sitemap](/sitemap/sitemap.xml)\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40ganeshkannappan%2Fllm-as-a-judge-unveiling-its-potential-and-applications-cbfb3db14e26&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40ganeshkannappan%2Fllm-as-a-judge-unveiling-its-potential-and-applications-cbfb3db14e26&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# LLM-as-a-Judge: Unveiling Its Potential and Applications\n\n[Ganesh Kannappan](/@ganeshkannappan?source=post_page---byline--cbfb3db14e26---------------------------------------)\n\n12 min read\n\n\u00b7\n\nDec 2, 2024\n\n--\n\nIn the [previous part](/@ganeshkannappan/llm-as-a-judge-...\n\nSource 7 (ID: src-51263506):\n  Title: Using LLMs for Evaluation - by Cameron R. Wolfe, Ph.D.\n  URL: https://cameronrwolfe.substack.com/p/llm-as-a-judge\n  Snippet: LLM-as-a-judge is a reference-free metric that directly prompts a powerful LLM to evaluate the quality of another model\u2019s output. For LLM-as-a-Judge evaluations, authors adopt the same strategy proposed by Vicuna [2], where the quality of model outputs is judged by via a pairwise prompt to GPT-4. The task of these annotators is to evaluate the quality of stories written for 200 prompts, where for each prompt we *i)* sample a response from GPT-2 (i.e., a weaker LLM) and *ii)* have a human write a...\n  Content: # [Deep (Learning) Focus](/)\n\n# Using LLMs for Evaluation\n\n### LLM-as-a-Judge and other scalable additions to human quality ratings...\n\n[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)\n\nJul 22, 2024\n\nAs large language models (LLMs) have become more and more capable, one of the most difficult aspects of working with these models is determining how to properly evaluate them. Many powerful models exist, and they each solve a wide variety of complex, open-ended tasks. As a result, discerning differences in performance between these models can be difficult. The most reliable method of evaluating LLMs is with human feedback, but collecting data from humans is noisy, time consuming, and expensive. Despite being a valuable and necessary source of truth for measuring model capabilities, human evaluation\u2014*when used in isolation*\u2014impedes our ability to iterate quickly during model development. To solve this problem, we need an evaluation metric that is quick, cost effective, and si...\n\nSource 8 (ID: src-2a4435f2):\n  Title: A Survey on LLM-as-a-Judge - arXiv\n  URL: https://arxiv.org/html/2411.15594v1\n  Snippet: To automate evaluation by LLM-as-a-Judge, one effective approach is to employ advanced language models such as GPT-4\u00a0(OpenAI, 2023a) instead of human evaluators\u00a0(Zheng et\u00a0al., 2023c). Unlike INSTRUCTSCORE which directly optimizes the model, the LLM evaluator in JADE(Zhang et\u00a0al., 2023c) relies on human judges to correct LLMs\u2019 evaluation results and updates the most frequently corrected samples into the example sets for few-shot prompting. In addition to integrating results from multiple rounds o...\n  Content: 11footnotetext: \\* These authors contributed equally to this research.22footnotetext: \u2020 Corresponding author.\n\n# A Survey on LLM-as-a-Judge\n\nJiawei Gu1,\\*, Xuhui Jiang1,\\*, Zhichao Shi1,2,\\*, Hexiang Tan2, Xuehao Zhai3, Chengjin Xu1, Wei Li2, Yinghan Shen2, Shengjie Ma1,4, Honghao Liu1,   \nYuanzhuo Wang2, Jian Guo1,\u2020     \n1IDEA Research, International Digital Economy Academy   \n2Institute of Computing Technology, Chinese Academy of Sciences   \n3Department of Civil and Environmental Engineering, Imperial College London   \n4Gaoling School of Artificial Intelligence, Renmin University of China China\n\n###### Abstract.\n\n## Abstract\n\nAccurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of \u201dLLM-as-a-Judge,\u201d where LLMs are employed as evaluators for complex task...\n\nSource 9 (ID: src-bbd215f1):\n  Title: LLM-as-a-Judge - by Nilesh Barla - Adaline Labs\n  URL: https://labs.adaline.ai/p/llm-as-a-judge\n  Snippet: Evaluating LLM outputs can save you a lot of time from shipping broken prompts and features. And for such a situation where you cannot write detailed instructions every time, you need to find a way to evaluate every output from the LLM. LLM-as-a-Judge is a framework where LLMs evaluate outputs from other LLMs using **structured prompts** to score qualities like **coherence** or **accuracy**. Teams need scalable evaluation methods that can assess LLM outputs with human-like judgment but without t...\n  Content: # [Adaline Labs](/)\n\n# LLM-as-a-Judge\n\n### A brief research note on LLM-as-a-judge including best practices.\n\n[Nilesh Barla](https://substack.com/@iridium0077)\n\nSep 08, 2025\n\nEvaluating LLM outputs can save you a lot of time from shipping broken prompts and features.\n\nA lot of talk and discussion is going on when it comes to the degrading performance or output of LLMs. You go to Reddit and you will find that users are not satisfied with LLMs such as Claude (these days) and GPT-5.\n\nSo, what's going on with LLMs?\n\nYou provide an input or prompt addressing your requirements, and the LLM doesn\u2019t provide you with a desirable answer. This might be happening because of one of two reasons, or both:\n\n1. Bad prompt\n2. Bad LLM\n\nNow, I understand that in a certain workflow that includes creativity, such as writing and brainstorming, you can hone the LLMs by using more structured prompting. For the most part, they will be satisfactory.\n\nBut when it comes to more logical and complex workflows, like ...\n\nSource 10 (ID: src-78c4677b):\n  Title: LLM-as-a-Judge: How AI Can Evaluate AI Faster and Smarter\n  URL: https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-j/\n  Snippet: An LLM-as-a-Judge evaluation uses an LLM to mimic human judgment of another LLM's output. It's not a fixed mathematical metric like \u201caccuracy\u201d \u2013\n  Content: Select platform to login\n\n[**Cloud Management**\n\nWebservers and Virtual Machines](https://cloud.bunnyshell.com/login/)[**Environments as a Service**\n\nCreate and Manage Kubernetes Environments](https://environments.bunnyshell.com/login/)\n\n[blog](/blog/)\n\n/[Cloud computing](/blog/cloud-computing/)\n\n# When AI Becomes the Judge: Understanding \u201cLLM-as-a-Judge\u201d\n\n[engineering](/blog/engineering/)\n\n[Alin Dobra](/blog/author/alin-dobra/)\n\nWhy Use an LLM as Judge?\n\nHow LLM-Judges Work\n\nArchitectures: Judge Assembly vs Super Judge\n\nUse Cases and Examples\n\nBuilding an Effective LLM Judge: Tips and Pitfalls\n\nPowering LLM-Evaluation with Bunnyshell\n\nConclusion\n\nImagine building a chatbot or code generator that not only writes answers \u2013 but also grades them. In the past, ensuring AI quality meant recruiting human reviewers or using simple metrics (BLEU, ROUGE) that miss nuance. Today, we can leverage **Generative AI** itself to evaluate its own work. *LLM-as-a-Judge* means using one Large Language Mo...\n\nSource 11 (ID: src-6ba1f0a1):\n  Title: Understanding Bias in LLM-as-a-Judge Systems\n  URL: https://ragmetrics.ai/blog/understanding-bias-in-llm-as-a-judge-systems\n  Snippet: # Understanding Bias in LLM-as-a-Judge Systems\n\n**The Hidden Problem in AI Evaluation**\n\nEvery developer building with GenAI has hit this moment: your evaluation pipeline says one model output is \u201cbetter,\u201d but your eyes disagree. The culprit is often bias\u2014bias not in the generating model, but in the\n\n**LLM acting as the judge**.... LLM-as-a-Judge systems are now the backbone of modern AI evaluation frameworks. They\u2019re faster, cheaper, and more consistent than human review\u2014but they\u2019re not immune ...\n\nSource 12 (ID: src-a4549098):\n  Title: A Systematic Study of Position Bias in LLM-as-a-Judge - arXiv\n  URL: https://arxiv.org/html/2406.07791v7\n  Snippet: ###### Abstract\nLLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases\u2014particularly position bias, the tendency to favor solutions based on their position within the prompt\u2014compromise its reliability. This study investigates position bias in LLM judges across pairwise and list-wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness.... Our experiments, involving 12 LLM ju...\n\nSource 13 (ID: src-bef824af):\n  Title: The 5 Biases That Can Silently Kill Your LLM Evaluations ...\n  URL: https://www.sebastiansigl.com/blog/llm-judge-biases-and-how-to-fix-them/\n  Snippet: This is the risk you run when you trust LLM judges blindly. For all their power, they are not impartial arbiters. They are susceptible to a range of cognitive biases - predictable, systematic errors that can silently corrupt your evaluation data and lead you to make the wrong product decisions\n\n2 3. Relying on a biased judge means you could be optimizing for failure, shipping regressions, and eroding user trust, all while your metrics tell you everything is fine.... This post will guide you thro...\n\nSource 14 (ID: src-7c38a7f7):\n  Title: Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge\n  URL: https://llm-judge-bias.github.io\n  Snippet: The upper part illustrates an example of diversity bias in LLM-as-a-Judge scenarios, while the lower part displays the ranking of average consistency metrics across six models.\n\nOur proposed framework:\n\n**CALM**... |Bias Type|Description|Example|\n|--|--|--|\n|\ud83d\udd00 Position (Pos.)|When an LLM exhibits a propensity to favor certain positions over others.|$R_1$: 3.11 > 3.8 $R_2$: 3.8 > 3.11 $R_1$: 3.8 > 3.11 $R_2$: 3.11 > 3.8|\n|\ud83d\udcc4 Verbosity (Ver.)|LLM judges favor longer responses, even if they are not ...\n\nSource 15 (ID: src-c33a2512):\n  Title: Evaluating and Mitigating LLM-as-a-judge Bias in ...\n  URL: https://arxiv.org/abs/2510.12462\n  Snippet: # Computer Science > Artificial Intelligence\n\n**arXiv:2510.12462** (cs)\n\n[Submitted on 14 Oct 2025]... # Title: Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems\n\nAuthors:Jiaxin Gao, Chen Chen, Yanwen Jia, Xueluan Gong, Kwok-Yan Lam, Qian Wang\nAbstract:Large Language Models (LLMs) are increasingly being used to autonomously evaluate the quality of content in communication systems, e.g., to assess responses in telecom customer support chatbots.... However, the impartiality of...\n\nSource 16 (ID: src-1e5014bd):\n  Title: An LLM-as-Judge Metric for Bridging the Gap with Human ... - arXiv\n  URL: https://arxiv.org/html/2505.20854v1\n  Snippet: In this paper, we present SWE-Judge, the first evaluation metric for LLM-as-Ensemble-Judge specifically designed to accurately assess the correctness of generated software artifacts. We evaluate SWE-Judge across a diverse set of software engineering (SE) benchmarks\u2014including CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess\u2014which span three SE tasks: code generation, automated program repair, and code summarization. The state-of-the-art LLM-as-judge evaluation metric for code,...\n  Content: \\newmdenv\n\n[ linecolor=linecolor, leftline=true, topline=false, bottomline=false, rightline=false, linewidth=2pt, innerleftmargin=10pt, innerrightmargin=10pt, innertopmargin=5pt, innerbottommargin=5pt, backgroundcolor=bgcolor ]leftbar\n\n# An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks\n\nXin Zhou  Singapore Management UniversitySingapore  [xinzhou.2020@phdcs.smu.edu.sg](mailto:xinzhou.2020@phdcs.smu.edu.sg)  ,\u00a0 Kisub Kim  Independent ResearcherHong Kong  [falconlk00@gmail.com](mailto:falconlk00@gmail.com)  ,\u00a0 Ting Zhang  Singapore Management UniversitySingapore  [tingzhang.2019@phdcs.smu.edu.sg](mailto:tingzhang.2019@phdcs.smu.edu.sg)  ,\u00a0 Martin Weyssow  Singapore Management UniversitySingapore  [mweyssow@smu.edu.sg](mailto:mweyssow@smu.edu.sg)  ,\u00a0 Lu\u00eds F.\u00a0Gomes  Carnegie Mellon UniversityUSA  [lfgomes@andrew.cmu.edu](mailto:lfgomes@andrew.cmu.edu)  ,\u00a0 Guang Yang  Nanjing University of Aeronautics and AstronauticsChina  [novelyg@outlook.com](mailto:novelyg@o...\n\nSource 17 (ID: src-db258615):\n  Title: LLM Evaluation Frameworks, Metrics & Methods Explained\n  URL: https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained\n  Snippet: This guide breaks down key LLM evaluation methods\u2014including automatic metrics, human reviews, hybrid frameworks like G-Eval, and LLM-as-a-Judge strategies. To get the most out of LLM-as-a-judge, teams often **prompt-engineer the evaluation** carefully (more on this in the G-Eval section), and may use a two-step process: first have the AI judge give a detailed rationale or score for multiple criteria, then possibly have a human review a subset of those judgments for quality control. It complement...\n  Content: Start Safeguarding Your LLM\u00a0Today!\n\nImplementing Qualifire is simple. Contact our team today, and\u00a0we\u2019ll get you started in no time!\n\nTalk to our team\n\nDror Ivry\n\n30/5/2025\n\nTable of content\n\n[What is HELM?](#)\n\n# LLM Evaluation Frameworks, Metrics & Methods Explained\n\n## **Introduction**\n\nLarge Language Models (LLMs) are increasingly deployed in chatbots, virtual assistants, and other user-facing applications. Ensuring these models produce high-quality, safe, and helpful responses is a major challenge. This makes evaluation a critical part of the development and deployment cycle for LLM-powered chat systems. Unlike traditional NLP tasks with clear-cut metrics, open-ended dialog requires careful **evaluation strategies**. In this post, we\u2019ll explore the spectrum of LLM evaluation methods \u2013 from automatic metrics to human reviews and cutting-edge hybrid approaches \u2013 and discuss when each is appropriate. We\u2019ll then take a deep dive into **LLM-as-a-judge** techniques with a focus on the G-...\n\nSource 18 (ID: src-3f4263f1):\n  Title: Large Language Model Evaluation in '26: 10+ Metrics & Methods\n  URL: https://research.aimultiple.com/large-language-model-evaluation/\n  Snippet: *   **MuSR** consists of algorithmically generated complex problems, requiring models to use reasoning and long-range context parsing, with few models performing better than random.[4](https://research.aimultiple.com/large-language-model-evaluation/#easy-footnote-bottom-4-68488 \"https://huggingface.co/datasets/TAUR-Lab/MuSR\"). *   **BBH** includes 23 challenging tasks from the BigBench dataset, measuring objective metrics and language understanding, and correlates well with human preference.[7](...\n  Content: Large Language Model Evaluation in '26: 10+ Metrics & Methods\n===============\n\n[![Image 1: AIMultiple](https://research.aimultiple.com/images/logo-2025.svg)![Image 2: AIMultiple](https://research.aimultiple.com/images/logo-2025-white.svg)](https://aimultiple.com/)\n\nAI\n\nCATEGORIES\n\nAI Coding AI Foundations AI Hardware AI in Industries Document Automation Generative AI Generative AI Applications Large Language Models MCP RAG\n\n[AI Code](https://research.aimultiple.com/ai-code/)[AI Code Editor](https://research.aimultiple.com/ai-code-editor/)[AI Code Review Tools](https://research.aimultiple.com/ai-code-review-tools/)[AI Coding Benchmark](https://research.aimultiple.com/ai-coding-benchmark/)[Screenshot to Code](https://research.aimultiple.com/screenshot-to-code/)\n\nAgentic AI\n\nCATEGORIES\n\nAgent Architectures & Tools AI Agent Applications Open-Source Agents\n\n[Agentic AI](https://research.aimultiple.com/agentic-ai/)[Agentic AI Design Patterns](https://research.aimultiple.com/agentic-ai-design...\n\nSource 19 (ID: src-0378afab):\n  Title: LLM Evaluation in 2025: Metrics, RAG, LLM-as-Judge & Best Practices\n  URL: https://medium.com/@QuarkAndCode/llm-evaluation-in-2025-metrics-rag-llm-as-judge-best-practices-ad2872cfa7cb\n  Snippet: # LLM Evaluation in 2025: Metrics, RAG, LLM-as-Judge & Best Practices. Evaluating large language models (LLMs) looks deceptively simple \u2014 run a prompt, eyeball the output, ship. In reality, robust evaluation is a multi\u2011layer process that blends offline tests, human judgment, and production telemetry. Below is a practical field guide that synthesizes recent playbooks and research into a repeatable approach you can use today. ## 1) Start with what you\u2019re shipping, not just the model. Before choosi...\n  Content: [Sitemap](/sitemap/sitemap.xml)\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40QuarkAndCode%2Fllm-evaluation-in-2025-metrics-rag-llm-as-judge-best-practices-ad2872cfa7cb&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Search](/search?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40QuarkAndCode%2Fllm-evaluation-in-2025-metrics-rag-llm-as-judge-best-practices-ad2872cfa7cb&source=post_page---top_nav_layout_nav-----------------------global_nav------------...\n\nSource 20 (ID: src-66027906):\n  Title: Co-Eval: Augmenting LLM-based Evaluation with Machine Metrics\n  URL: https://aclanthology.org/2025.emnlp-main.1307.pdf\n  Snippet: Nov 4, 2025 ... We demonstrate the effectiveness of the Co-Eval framework in mitigating four types of bias\u2014self- preference bias, position bias, verbosity bias,\u00a0...\n\nPlease analyze these sources and:\n1. Extract 2-5 key findings relevant to the research query\n2. Assess confidence levels based on source agreement and authority\n3. Identify any knowledge gaps or unanswered questions\n4. Assess the quality of each source\n\nReturn your analysis as JSON.", "raw_response": "```json\n{\n    \"findings\": [\n        {\n            \"content\": \"LLM judges exhibit distinct cognitive biases that compromise reliability, most notably 'self-preference bias' (favoring their own generated outputs), 'verbosity bias' (favoring longer responses regardless of quality), and 'position bias' (favoring the first option in pairwise comparisons).\",\n            \"confidence\": \"high\",\n            \"source_ids\": [\n                \"src-67c025c2\",\n                \"src-45a8de46\",\n                \"src-48201995\",\n                \"src-e0d1753b\",\n                \"src-8d0c93da\",\n                \"src-a4549098\",\n                \"src-7c38a7f7\"\n            ],\n            \"category\": \"Biases & Limitations\"\n        },\n        {\n            \"content\": \"Effective mitigation strategies for these biases include Chain-of-Thought (CoT) prompting to induce reasoning before scoring, position swapping (running evaluations twice with reversed orders), and using reference-free evaluation metrics.\",\n            \"confidence\": \"medium\",\n            \"source_ids\": [\n                \"src-8d0c93da\",\n                \"src-51263506\",\n                \"src-48201995\",\n                \"src-e0d1753b\"\n            ],\n            \"category\": \"Methodology\"\n        },\n        {\n            \"content\": \"GPT-4 remains the standard for 'Judge' models, capable of achieving over 80% agreement with human preferences on benchmarks like MT-Bench and Chatbot Arena, effectively matching controlled human agreement levels.\",\n            \"confidence\": \"high\",\n            \"source_ids\": [\n                \"src-48201995\",\n                \"src-51263506\",\n                \"src-2a4435f2\"\n            ],\n            \"category\": \"Performance\"\n        },\n        {\n            \"content\": \"Application-specific judge frameworks are emerging, such as 'SWE-Judge' for software engineering which evaluates code correctness, moving beyond generic dialogue evaluation to domain-specific tasks.\",\n            \"confidence\": \"medium\",\n            \"source_ids\": [\n                \"src-1e5014bd\"\n            ],\n            \"category\": \"Applications\"\n        }\n    ],\n    \"gaps\": [\n        {\n            \"description\": \"While RAG is mentioned as an application, there is a lack of specific detail on how LLM judges evaluate the 'retrieval' component separately from the 'generation' component (e.g., context relevance vs. answer faithfulness) in the provided sources.\",\n            \"suggested_queries\": [\n                \"LLM-as-a-judge metrics for RAG retrieval context relevance\",\n                \"evaluating faithfulness vs answer relevance in RAG using LLMs\"\n            ],\n            \"priority\": 1\n        },\n        {\n            \"description\": \"There is limited information on the cost-latency trade-offs of deploying LLM judges at scale versus using smaller, fine-tuned judge models.\",\n            \"suggested_queries\": [\n                \"cost analysis of LLM-as-a-judge at scale\",\n                \"distilling GPT-4 into smaller judge models performance comparison\"\n            ],\n            \"priority\": 2\n        }\n    ],\n    \"quality_updates\": [\n        {\n            \"source_id\": \"src-67c025c2\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-45a8de46\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-48201995\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-1e5014bd\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-a4549098\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-8d0c93da\",\n            \"quality\": \"medium\"\n        },\n        {\n            \"source_id\": \"src-08525cff\",\n            \"quality\": \"medium\"\n        },\n        {\n            \"source_id\": \"src-51263506\",\n            \"quality\": \"medium\"\n        }\n    ]\n}\n```", "parse_success": true, "findings": [{"content": "LLM judges exhibit distinct cognitive biases that compromise reliability, most notably 'self-preference bias' (favoring their own generated outputs), 'verbosity bias' (favoring longer responses regardless of quality), and 'position bias' (favoring the first option in pairwise comparisons).", "confidence": "high", "source_ids": ["src-67c025c2", "src-45a8de46", "src-48201995", "src-e0d1753b", "src-8d0c93da", "src-a4549098", "src-7c38a7f7"], "category": "Biases & Limitations"}, {"content": "Effective mitigation strategies for these biases include Chain-of-Thought (CoT) prompting to induce reasoning before scoring, position swapping (running evaluations twice with reversed orders), and using reference-free evaluation metrics.", "confidence": "medium", "source_ids": ["src-8d0c93da", "src-51263506", "src-48201995", "src-e0d1753b"], "category": "Methodology"}, {"content": "GPT-4 remains the standard for 'Judge' models, capable of achieving over 80% agreement with human preferences on benchmarks like MT-Bench and Chatbot Arena, effectively matching controlled human agreement levels.", "confidence": "high", "source_ids": ["src-48201995", "src-51263506", "src-2a4435f2"], "category": "Performance"}, {"content": "Application-specific judge frameworks are emerging, such as 'SWE-Judge' for software engineering which evaluates code correctness, moving beyond generic dialogue evaluation to domain-specific tasks.", "confidence": "medium", "source_ids": ["src-1e5014bd"], "category": "Applications"}], "gaps": [{"description": "While RAG is mentioned as an application, there is a lack of specific detail on how LLM judges evaluate the 'retrieval' component separately from the 'generation' component (e.g., context relevance vs. answer faithfulness) in the provided sources.", "suggested_queries": ["LLM-as-a-judge metrics for RAG retrieval context relevance", "evaluating faithfulness vs answer relevance in RAG using LLMs"], "priority": 1}, {"description": "There is limited information on the cost-latency trade-offs of deploying LLM judges at scale versus using smaller, fine-tuned judge models.", "suggested_queries": ["cost analysis of LLM-as-a-judge at scale", "distilling GPT-4 into smaller judge models performance comparison"], "priority": 2}], "quality_updates": [{"source_id": "src-67c025c2", "quality": "high"}, {"source_id": "src-45a8de46", "quality": "high"}, {"source_id": "src-48201995", "quality": "high"}, {"source_id": "src-1e5014bd", "quality": "high"}, {"source_id": "src-a4549098", "quality": "high"}, {"source_id": "src-8d0c93da", "quality": "medium"}, {"source_id": "src-08525cff", "quality": "medium"}, {"source_id": "src-51263506", "quality": "medium"}]}}
{"timestamp": "2026-01-01T01:16:16.910703Z", "event_id": "cbc92f4ffe1f4e3eb6bfa66c38cf8027", "event_type": "phase_complete", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "analysis", "iteration": 2, "data": {"phase": "analysis", "duration_ms": 26300.12164101936}}
{"timestamp": "2026-01-01T01:16:16.911027Z", "event_id": "b422cc30e5fc4f6faa43e483b49eb773", "event_type": "phase_start", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "synthesis", "iteration": 2, "data": {"phase": "synthesis"}}
{"timestamp": "2026-01-01T01:16:57.605121Z", "event_id": "d0560be75359454faf9fc28c8110d7fa", "event_type": "synthesis_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "synthesis", "iteration": 2, "data": {"provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview", "tokens_used": 14569, "duration_ms": null, "system_prompt": "You are a research synthesizer. Your task is to create a comprehensive, well-structured research report from analyzed findings.\n\nGenerate a markdown-formatted report with the following structure:\n\n# Research Report: [Topic]\n\n## Executive Summary\nA 2-3 paragraph overview of the key insights and conclusions.\n\n## Key Findings\n\n### [Theme/Category 1]\n- Finding with supporting evidence and source citations [Source ID]\n- Related findings grouped together\n\n### [Theme/Category 2]\n- Continue for each major theme...\n\n## Analysis\n\n### Supporting Evidence\nDiscussion of well-supported findings with high confidence.\n\n### Conflicting Information\nNote any contradictions or disagreements between sources (if present).\n\n### Limitations\nAcknowledge gaps in the research and areas needing further investigation.\n\n## Sources\nList sources as markdown links with their IDs: **[src-xxx]** [Title](URL)\n\n## Conclusions\nActionable insights and recommendations based on the findings.\n\n---\n\nGuidelines:\n- Organize findings thematically rather than listing them sequentially\n- Cite source IDs in brackets when referencing specific information [src-xxx]\n- Distinguish between high-confidence findings (well-supported) and lower-confidence insights\n- Be specific and actionable in conclusions\n- Keep the report focused on the original research query\n- Use clear, professional language\n- Include all relevant findings - don't omit information\n\nIMPORTANT: Return ONLY the markdown report, no preamble or meta-commentary.", "user_prompt": "# Research Query\nLLM Judges: techniques, architectures, evaluation methods, and applications for using large language models as automated evaluators and judges\n\n## Research Brief\nThis research will investigate the methodology and infrastructure of using Large Language Models as automated evaluators, focusing on core architectures like pairwise comparison and direct scoring. It will also examine the metrics used to validate these judges against human baselines and explore their practical applications in RAG and model alignment.\n\n## Findings to Synthesize\n\n### Bias & Limitations\n- [HIGH] LLM Judges exhibit systematic cognitive biases, most notably 'self-preference bias' (favoring their own generated outputs), 'position bias' (favoring responses appearing earlier or later in a pair), and 'verbosity bias' (rating longer responses higher regardless of quality).\n  Sources: src-67c025c2, src-45a8de46, src-48201995, src-e0d1753b, src-a4549098, src-7c38a7f7\n\n### Mitigation Techniques\n- [HIGH] To mitigate evaluation biases, researchers employ techniques such as 'Chain-of-Thought' (CoT) prompting to induce reasoning before scoring, position swapping (running the eval twice with swapped orders) to average out position bias, and 'Co-Eval' frameworks that augment LLMs with objective machine metrics.\n  Sources: src-8d0c93da, src-66027906, src-48201995, src-e0d1753b\n\n### Architecture & Performance\n- [MEDIUM] Two primary architectures dominate LLM-as-a-Judge: 'Pairwise Comparison' (mimicking human preference testing like Chatbot Arena) and 'Direct Scoring/Pointwise' (assigning absolute scores like 1-10), with strong models like GPT-4 achieving over 80% agreement with human annotators in general chat domains.\n  Sources: src-48201995, src-51263506, src-2a4435f2\n\n### Advanced Architectures\n- [MEDIUM] Specialized 'Ensemble' or 'Judge Assembly' approaches are emerging for complex domains, such as 'SWE-Judge' for software engineering, which combines LLM reasoning with code execution/static analysis to bridge the gap with human verification in technical tasks.\n  Sources: src-1e5014bd, src-78c4677b, src-2a4435f2\n\n### Biases & Limitations\n- [HIGH] LLM judges exhibit distinct cognitive biases that compromise reliability, most notably 'self-preference bias' (favoring their own generated outputs), 'verbosity bias' (favoring longer responses regardless of quality), and 'position bias' (favoring the first option in pairwise comparisons).\n  Sources: src-67c025c2, src-45a8de46, src-48201995, src-e0d1753b, src-8d0c93da, src-a4549098, src-7c38a7f7\n\n### Methodology\n- [MEDIUM] Effective mitigation strategies for these biases include Chain-of-Thought (CoT) prompting to induce reasoning before scoring, position swapping (running evaluations twice with reversed orders), and using reference-free evaluation metrics.\n  Sources: src-8d0c93da, src-51263506, src-48201995, src-e0d1753b\n\n### Performance\n- [HIGH] GPT-4 remains the standard for 'Judge' models, capable of achieving over 80% agreement with human preferences on benchmarks like MT-Bench and Chatbot Arena, effectively matching controlled human agreement levels.\n  Sources: src-48201995, src-51263506, src-2a4435f2\n\n### Applications\n- [MEDIUM] Application-specific judge frameworks are emerging, such as 'SWE-Judge' for software engineering which evaluates code correctness, moving beyond generic dialogue evaluation to domain-specific tasks.\n  Sources: src-1e5014bd\n\n## Knowledge Gaps Identified\n- [unresolved] While 'Judge Assembly' and ensemble methods are mentioned, specific architectural patterns for orchestrating these cost-effectively in production (latency vs. accuracy trade-offs) are under-documented in the provided sources.\n- [unresolved] The sources discuss biases extensively but lack detailed comparative data on the efficacy of 'Reference-free' vs. 'Reference-based' evaluation across different model sizes (e.g., can a small model effectively judge a large model if provided a reference?).\n- [unresolved] While RAG is mentioned as an application, there is a lack of specific detail on how LLM judges evaluate the 'retrieval' component separately from the 'generation' component (e.g., context relevance vs. answer faithfulness) in the provided sources.\n- [unresolved] There is limited information on the cost-latency trade-offs of deploying LLM judges at scale versus using smaller, fine-tuned judge models.\n\n## Source Reference\n- src-67c025c2: Self-Preference Bias in LLM-as-a-Judge [high]\n  URL: https://openreview.net/forum?id=Ns8zGZ0lmM\n- src-45a8de46: Self-Preference Bias in LLM-as-a-Judge [high]\n  URL: https://arxiv.org/html/2410.21819v1\n- src-48201995: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena [high]\n  URL: https://neurips.cc/virtual/2023/poster/73434\n- src-e0d1753b: Mitigating the Bias of Large Language Model Evaluation [medium]\n  URL: https://aclanthology.org/2024.ccl-1.101.pdf\n- src-8d0c93da: 5 Techniques to Improve LLM-Judges : r/LLMDevs [medium]\n  URL: https://www.reddit.com/r/LLMDevs/comments/1j3gbil/5_techniques_to_improve_llmjudges/\n- src-08525cff: LLM-as-a-Judge: Unveiling Its Potential and Applications - Medium [medium]\n  URL: https://medium.com/@ganeshkannappan/llm-as-a-judge-unveiling-its-potential-and-applications-cbfb3db14e26\n- src-51263506: Using LLMs for Evaluation - by Cameron R. Wolfe, Ph.D. [medium]\n  URL: https://cameronrwolfe.substack.com/p/llm-as-a-judge\n- src-2a4435f2: A Survey on LLM-as-a-Judge - arXiv [high]\n  URL: https://arxiv.org/html/2411.15594v1\n- src-bbd215f1: LLM-as-a-Judge - by Nilesh Barla - Adaline Labs [medium]\n  URL: https://labs.adaline.ai/p/llm-as-a-judge\n- src-78c4677b: LLM-as-a-Judge: How AI Can Evaluate AI Faster and Smarter [medium]\n  URL: https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-j/\n- src-6ba1f0a1: Understanding Bias in LLM-as-a-Judge Systems [medium]\n  URL: https://ragmetrics.ai/blog/understanding-bias-in-llm-as-a-judge-systems\n- src-a4549098: A Systematic Study of Position Bias in LLM-as-a-Judge - arXiv [high]\n  URL: https://arxiv.org/html/2406.07791v7\n- src-bef824af: The 5 Biases That Can Silently Kill Your LLM Evaluations ... [medium]\n  URL: https://www.sebastiansigl.com/blog/llm-judge-biases-and-how-to-fix-them/\n- src-7c38a7f7: Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge [medium]\n  URL: https://llm-judge-bias.github.io\n- src-c33a2512: Evaluating and Mitigating LLM-as-a-judge Bias in ... [high]\n  URL: https://arxiv.org/abs/2510.12462\n- src-1e5014bd: An LLM-as-Judge Metric for Bridging the Gap with Human ... - arXiv [high]\n  URL: https://arxiv.org/html/2505.20854v1\n- src-db258615: LLM Evaluation Frameworks, Metrics & Methods Explained [medium]\n  URL: https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained\n- src-3f4263f1: Large Language Model Evaluation in '26: 10+ Metrics & Methods [medium]\n  URL: https://research.aimultiple.com/large-language-model-evaluation/\n- src-0378afab: LLM Evaluation in 2025: Metrics, RAG, LLM-as-Judge & Best Practices [medium]\n  URL: https://medium.com/@QuarkAndCode/llm-evaluation-in-2025-metrics-rag-llm-as-judge-best-practices-ad2872cfa7cb\n- src-66027906: Co-Eval: Augmenting LLM-based Evaluation with Machine Metrics [medium]\n  URL: https://aclanthology.org/2025.emnlp-main.1307.pdf\n- src-03c1a7f3: Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge [high]\n  URL: https://arxiv.org/html/2410.02736v1\n- src-7c2fcbc0: The Intricacies of Evaluating Large Language Models with LLM-as-a ... [medium]\n  URL: https://medium.com/@vineethveetil/the-intricacies-of-evaluating-large-language-models-with-llm-as-a-judge-8034a3f34b28\n- src-fa92de56: LLM-as-a-Judge Simply... [medium]\n  URL: https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method\n- src-d64f918b: A Survey on LLM-as-a-Judge [high]\n  URL: https://arxiv.org/html/2411.15594v4\n- src-770e362f: A Survey on LLM-as-a-Judge - arXiv [high]\n  URL: https://arxiv.org/html/2411.15594v6\n- src-29376e6f: 3. Biases: Position, Length... [medium]\n  URL: https://www.emergentmind.com/topics/llm-as-a-judge-evaluations\n- src-a040edba: LLM-Judge Protocol: Methods & Applications - Emergent Mind [medium]\n  URL: https://www.emergentmind.com/topics/llm-judge-protocol\n- src-05ffcaa9: Judge's Verdict: A Comprehensive Analysis of LLM ... [high]\n  URL: https://arxiv.org/html/2510.09738v1\n- src-a71f7557: An LLM-as-Judge Metric for Bridging the Gap with Human ... [high]\n  URL: https://arxiv.org/abs/2505.20854\n- src-35901ad6: LLM-as-a-judge: a complete guide to using LLMs for evaluations [medium]\n  URL: https://www.evidentlyai.com/llm-guide/llm-as-a-judge\n\n## Instructions\nGenerate a comprehensive research report addressing the query: 'LLM Judges: techniques, architectures, evaluation methods, and applications for using large language models as automated evaluators and judges'\n\nThis is iteration 2 of 3.\nTotal findings: 8\nTotal sources: 104\nUnresolved gaps: 4\n\nCreate a well-structured markdown report following the format specified.", "raw_response": "# Research Report: LLM Judges: Techniques, Architectures, and Applications\n\n## Executive Summary\nThe use of Large Language Models (LLMs) as automated judges represents a significant shift in AI evaluation, offering a scalable alternative to human annotation. Research indicates that high-performing models, particularly GPT-4, can achieve over 80% agreement with human evaluators on standard benchmarks like MT-Bench and Chatbot Arena. This capability allows for rapid feedback loops in model development and alignment tasks.\n\nHowever, reliability is heavily constrained by inherent cognitive biases. \"LLM-as-a-Judge\" systems exhibit systematic patterns such as favoring their own outputs (self-preference bias), preferring longer responses regardless of quality (verbosity bias), and showing sensitivity to the order of options presented (position bias). To combat these, sophisticated mitigation strategies including Chain-of-Thought (CoT) prompting and permutation-based consistency checks have become standard practice.\n\nCurrent architectures generally fall into pairwise comparison or direct scoring frameworks. While general-purpose chat evaluation is maturing, the field is evolving toward specialized, ensemble-based approaches for complex domains. For instance, software engineering evaluations now increasingly rely on hybrid systems that combine LLM reasoning with objective code execution verification to bridge the accuracy gap in technical tasks.\n\n## Key Findings\n\n### Cognitive Biases & Limitations\n- **Systematic Bias Patterns**: LLM judges demonstrate distinct cognitive biases that compromise evaluation integrity. The most prevalent include 'self-preference bias' (favoring outputs generated by the same model family), 'position bias' (consistently favoring the first or second option in a pair), and 'verbosity bias' (rating longer responses higher, independent of content quality) [src-67c025c2] [src-45a8de46] [src-48201995] [src-a4549098].\n- **Impact on Reliability**: These biases are not random noise but systematic errors that can skew leaderboard rankings and alignment training if left unmitigated [src-e0d1753b] [src-7c38a7f7].\n\n### Mitigation Techniques\n- **Algorithmic Adjustments**: To improve reliability, researchers have standardized several mitigation techniques. 'Position swapping' involves running pairwise evaluations twice with reversed orders to average out position bias [src-48201995] [src-e0d1753b].\n- **Prompt Engineering Strategy**: 'Chain-of-Thought' (CoT) prompting is highly effective, requiring the judge to generate reasoning before assigning a score, which reduces impulsive scoring based on superficial features like length [src-8d0c93da].\n- **Hybrid Frameworks**: 'Co-Eval' frameworks augment subjective LLM judgments with objective machine metrics to provide a more balanced evaluation signal [src-66027906].\n\n### Core Architectures & Performance\n- **Dominant Frameworks**: Two primary architectures define the landscape: 'Pairwise Comparison,' which mimics human preference testing (e.g., A/B testing in Chatbot Arena), and 'Direct Scoring/Pointwise,' which assigns absolute scores (e.g., 1-10 Likert scales) [src-48201995] [src-2a4435f2].\n- **Human Agreement**: State-of-the-art models like GPT-4 serve as the \"Gold Standard\" for judges, achieving over 80% agreement with human annotators in general chat domains, effectively matching the agreement levels found between different human annotators [src-51263506] [src-2a4435f2].\n\n### Advanced & Domain-Specific Applications\n- **Ensemble Approaches**: For complex, high-stakes domains, single-model judges are being replaced by 'Judge Assemblies' or ensembles. An example is 'SWE-Judge' for software engineering, which integrates LLM reasoning with static analysis and code execution to evaluate correctness more accurately than text-based metrics alone [src-1e5014bd] [src-78c4677b].\n\n## Analysis\n\n### Supporting Evidence\nThere is high confidence across multiple sources regarding the performance of top-tier models as judges. The correlation between GPT-4 evaluations and human preferences is well-documented on benchmarks like MT-Bench [src-48201995] [src-51263506]. Similarly, the existence of specific biases\u2014particularly position and verbosity bias\u2014is supported by extensive empirical testing, making mitigation strategies a mandatory component of any robust evaluation pipeline [src-a4549098] [src-67c025c2].\n\n### Conflicting Information\nWhile 'Pairwise Comparison' is often cited as more intuitive for relative ranking, it scales poorly (O(n^2)) compared to 'Direct Scoring'. Sources suggest a trade-off where pairwise offers higher alignment with human intuition for subtle differences, while direct scoring is necessary for larger-scale absolute assessments, though the latter is more prone to calibration errors (e.g., score compression where models rarely give 1s or 10s) [src-2a4435f2].\n\n### Limitations\nSignificant knowledge gaps remain regarding the deployment of these systems in production environments:\n- **Cost-Latency Trade-offs**: There is limited data quantifying the cost and latency implications of using large, reasoning-heavy models (like GPT-4) as judges at scale versus using smaller, fine-tuned judge models.\n- **RAG Evaluation Specifics**: While mentioned as an application, specific methodologies for decoupling 'retrieval' evaluation from 'generation' evaluation using LLM judges are under-documented in the analyzed sources.\n- **Reference Dependency**: The efficacy of 'Reference-free' vs. 'Reference-based' evaluation across different model size disparities (e.g., can a 7B model effectively judge a 70B model?) remains an open question.\n\n## Sources\n- **[src-67c025c2]** [Self-Preference Bias in LLM-as-a-Judge](https://openreview.net/forum?id=Ns8zGZ0lmM)\n- **[src-45a8de46]** [Self-Preference Bias in LLM-as-a-Judge](https://arxiv.org/html/2410.21819v1)\n- **[src-48201995]** [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://neurips.cc/virtual/2023/poster/73434)\n- **[src-e0d1753b]** [Mitigating the Bias of Large Language Model Evaluation](https://aclanthology.org/2024.ccl-1.101.pdf)\n- **[src-8d0c93da]** [5 Techniques to Improve LLM-Judges : r/LLMDevs](https://www.reddit.com/r/LLMDevs/comments/1j3gbil/5_techniques_to_improve_llmjudges/)\n- **[src-51263506]** [Using LLMs for Evaluation - by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/llm-as-a-judge)\n- **[src-2a4435f2]** [A Survey on LLM-as-a-Judge - arXiv](https://arxiv.org/html/2411.15594v1)\n- **[src-1e5014bd]** [An LLM-as-Judge Metric for Bridging the Gap with Human ... - arXiv](https://arxiv.org/html/2505.20854v1)\n- **[src-78c4677b]** [LLM-as-a-Judge: How AI Can Evaluate AI Faster and Smarter](https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-j/)\n- **[src-a4549098]** [A Systematic Study of Position Bias in LLM-as-a-Judge - arXiv](https://arxiv.org/html/2406.07791v7)\n- **[src-7c38a7f7]** [Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge](https://llm-judge-bias.github.io)\n- **[src-66027906]** [Co-Eval: Augmenting LLM-based Evaluation with Machine Metrics](https://aclanthology.org/2025.emnlp-main.1307.pdf)\n\n## Conclusions\nTo successfully implement LLM-as-a-Judge, practitioners should prioritize **bias mitigation** as a foundational step. Architectures must default to using **Chain-of-Thought** reasoning and **position-swapping** (for pairwise comparisons) to ensure valid results. \n\nFor general applications, high-capability models like GPT-4 are currently required to maintain >80% human agreement. However, for specialized tasks like coding or complex reasoning, standard \"prompt-only\" judges are insufficient; **ensemble methods** that incorporate objective verification (code execution, static analysis) are necessary. Future investment should focus on optimizing the cost-performance ratio, potentially by fine-tuning smaller models on the high-quality outputs of larger judge models to create efficient, domain-specific evaluators.", "report": "# Research Report: LLM Judges: Techniques, Architectures, and Applications\n\n## Executive Summary\nThe use of Large Language Models (LLMs) as automated judges represents a significant shift in AI evaluation, offering a scalable alternative to human annotation. Research indicates that high-performing models, particularly GPT-4, can achieve over 80% agreement with human evaluators on standard benchmarks like MT-Bench and Chatbot Arena. This capability allows for rapid feedback loops in model development and alignment tasks.\n\nHowever, reliability is heavily constrained by inherent cognitive biases. \"LLM-as-a-Judge\" systems exhibit systematic patterns such as favoring their own outputs (self-preference bias), preferring longer responses regardless of quality (verbosity bias), and showing sensitivity to the order of options presented (position bias). To combat these, sophisticated mitigation strategies including Chain-of-Thought (CoT) prompting and permutation-based consistency checks have become standard practice.\n\nCurrent architectures generally fall into pairwise comparison or direct scoring frameworks. While general-purpose chat evaluation is maturing, the field is evolving toward specialized, ensemble-based approaches for complex domains. For instance, software engineering evaluations now increasingly rely on hybrid systems that combine LLM reasoning with objective code execution verification to bridge the accuracy gap in technical tasks.\n\n## Key Findings\n\n### Cognitive Biases & Limitations\n- **Systematic Bias Patterns**: LLM judges demonstrate distinct cognitive biases that compromise evaluation integrity. The most prevalent include 'self-preference bias' (favoring outputs generated by the same model family), 'position bias' (consistently favoring the first or second option in a pair), and 'verbosity bias' (rating longer responses higher, independent of content quality) [src-67c025c2] [src-45a8de46] [src-48201995] [src-a4549098].\n- **Impact on Reliability**: These biases are not random noise but systematic errors that can skew leaderboard rankings and alignment training if left unmitigated [src-e0d1753b] [src-7c38a7f7].\n\n### Mitigation Techniques\n- **Algorithmic Adjustments**: To improve reliability, researchers have standardized several mitigation techniques. 'Position swapping' involves running pairwise evaluations twice with reversed orders to average out position bias [src-48201995] [src-e0d1753b].\n- **Prompt Engineering Strategy**: 'Chain-of-Thought' (CoT) prompting is highly effective, requiring the judge to generate reasoning before assigning a score, which reduces impulsive scoring based on superficial features like length [src-8d0c93da].\n- **Hybrid Frameworks**: 'Co-Eval' frameworks augment subjective LLM judgments with objective machine metrics to provide a more balanced evaluation signal [src-66027906].\n\n### Core Architectures & Performance\n- **Dominant Frameworks**: Two primary architectures define the landscape: 'Pairwise Comparison,' which mimics human preference testing (e.g., A/B testing in Chatbot Arena), and 'Direct Scoring/Pointwise,' which assigns absolute scores (e.g., 1-10 Likert scales) [src-48201995] [src-2a4435f2].\n- **Human Agreement**: State-of-the-art models like GPT-4 serve as the \"Gold Standard\" for judges, achieving over 80% agreement with human annotators in general chat domains, effectively matching the agreement levels found between different human annotators [src-51263506] [src-2a4435f2].\n\n### Advanced & Domain-Specific Applications\n- **Ensemble Approaches**: For complex, high-stakes domains, single-model judges are being replaced by 'Judge Assemblies' or ensembles. An example is 'SWE-Judge' for software engineering, which integrates LLM reasoning with static analysis and code execution to evaluate correctness more accurately than text-based metrics alone [src-1e5014bd] [src-78c4677b].\n\n## Analysis\n\n### Supporting Evidence\nThere is high confidence across multiple sources regarding the performance of top-tier models as judges. The correlation between GPT-4 evaluations and human preferences is well-documented on benchmarks like MT-Bench [src-48201995] [src-51263506]. Similarly, the existence of specific biases\u2014particularly position and verbosity bias\u2014is supported by extensive empirical testing, making mitigation strategies a mandatory component of any robust evaluation pipeline [src-a4549098] [src-67c025c2].\n\n### Conflicting Information\nWhile 'Pairwise Comparison' is often cited as more intuitive for relative ranking, it scales poorly (O(n^2)) compared to 'Direct Scoring'. Sources suggest a trade-off where pairwise offers higher alignment with human intuition for subtle differences, while direct scoring is necessary for larger-scale absolute assessments, though the latter is more prone to calibration errors (e.g., score compression where models rarely give 1s or 10s) [src-2a4435f2].\n\n### Limitations\nSignificant knowledge gaps remain regarding the deployment of these systems in production environments:\n- **Cost-Latency Trade-offs**: There is limited data quantifying the cost and latency implications of using large, reasoning-heavy models (like GPT-4) as judges at scale versus using smaller, fine-tuned judge models.\n- **RAG Evaluation Specifics**: While mentioned as an application, specific methodologies for decoupling 'retrieval' evaluation from 'generation' evaluation using LLM judges are under-documented in the analyzed sources.\n- **Reference Dependency**: The efficacy of 'Reference-free' vs. 'Reference-based' evaluation across different model size disparities (e.g., can a 7B model effectively judge a 70B model?) remains an open question.\n\n## Sources\n- **[src-67c025c2]** [Self-Preference Bias in LLM-as-a-Judge](https://openreview.net/forum?id=Ns8zGZ0lmM)\n- **[src-45a8de46]** [Self-Preference Bias in LLM-as-a-Judge](https://arxiv.org/html/2410.21819v1)\n- **[src-48201995]** [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://neurips.cc/virtual/2023/poster/73434)\n- **[src-e0d1753b]** [Mitigating the Bias of Large Language Model Evaluation](https://aclanthology.org/2024.ccl-1.101.pdf)\n- **[src-8d0c93da]** [5 Techniques to Improve LLM-Judges : r/LLMDevs](https://www.reddit.com/r/LLMDevs/comments/1j3gbil/5_techniques_to_improve_llmjudges/)\n- **[src-51263506]** [Using LLMs for Evaluation - by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/llm-as-a-judge)\n- **[src-2a4435f2]** [A Survey on LLM-as-a-Judge - arXiv](https://arxiv.org/html/2411.15594v1)\n- **[src-1e5014bd]** [An LLM-as-Judge Metric for Bridging the Gap with Human ... - arXiv](https://arxiv.org/html/2505.20854v1)\n- **[src-78c4677b]** [LLM-as-a-Judge: How AI Can Evaluate AI Faster and Smarter](https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-j/)\n- **[src-a4549098]** [A Systematic Study of Position Bias in LLM-as-a-Judge - arXiv](https://arxiv.org/html/2406.07791v7)\n- **[src-7c38a7f7]** [Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge](https://llm-judge-bias.github.io)\n- **[src-66027906]** [Co-Eval: Augmenting LLM-based Evaluation with Machine Metrics](https://aclanthology.org/2025.emnlp-main.1307.pdf)\n\n## Conclusions\nTo successfully implement LLM-as-a-Judge, practitioners should prioritize **bias mitigation** as a foundational step. Architectures must default to using **Chain-of-Thought** reasoning and **position-swapping** (for pairwise comparisons) to ensure valid results. \n\nFor general applications, high-capability models like GPT-4 are currently required to maintain >80% human agreement. However, for specialized tasks like coding or complex reasoning, standard \"prompt-only\" judges are insufficient; **ensemble methods** that incorporate objective verification (code execution, static analysis) are necessary. Future investment should focus on optimizing the cost-performance ratio, potentially by fine-tuning smaller models on the high-quality outputs of larger judge models to create efficient, domain-specific evaluators.", "report_length": 8026}}
{"timestamp": "2026-01-01T01:16:57.606460Z", "event_id": "7332d280271e433c93942342427ab195", "event_type": "phase_complete", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "synthesis", "iteration": 2, "data": {"phase": "synthesis", "duration_ms": 40696.64874399314}}
{"timestamp": "2026-01-01T01:16:57.606764Z", "event_id": "8cd9b01af18d4291aca2d8436aded307", "event_type": "phase_start", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "refinement", "iteration": 2, "data": {"phase": "refinement"}}
{"timestamp": "2026-01-01T01:17:21.366949Z", "event_id": "539c9f54b3a74203bb7bee2b1ff0dfc4", "event_type": "refinement_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "refinement", "iteration": 2, "data": {"provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview", "tokens_used": 11225, "duration_ms": null, "system_prompt": "You are a research refiner. Your task is to analyze knowledge gaps identified during research and generate focused follow-up queries to address them.\n\nYour response MUST be valid JSON with this exact structure:\n{\n    \"gap_analysis\": [\n        {\n            \"gap_id\": \"gap-xxx\",\n            \"severity\": \"critical|moderate|minor\",\n            \"addressable\": true,\n            \"rationale\": \"Why this gap matters and whether it can be addressed\"\n        }\n    ],\n    \"follow_up_queries\": [\n        {\n            \"query\": \"A specific, focused search query to address the gap\",\n            \"target_gap_id\": \"gap-xxx\",\n            \"rationale\": \"How this query will fill the gap\",\n            \"priority\": 1\n        }\n    ],\n    \"addressed_gap_ids\": [\"gap-xxx\"],\n    \"iteration_recommendation\": {\n        \"should_iterate\": true,\n        \"rationale\": \"Why iteration is or isn't recommended\"\n    }\n}\n\nGuidelines:\n- Assess each gap's severity: \"critical\" (blocks conclusions), \"moderate\" (affects confidence), \"minor\" (nice to have)\n- Only mark gaps as addressable if follow-up research can realistically fill them\n- Generate 1-3 highly focused follow-up queries per addressable gap\n- Priority 1 is highest priority\n- Mark gaps as addressed if the current report already covers them adequately\n- Recommend iteration only if there are addressable critical/moderate gaps AND value exceeds research cost\n\nIMPORTANT: Return ONLY valid JSON, no markdown formatting or extra text.", "user_prompt": "# Research Query\nLLM Judges: techniques, architectures, evaluation methods, and applications for using large language models as automated evaluators and judges\n\n## Research Status\n- Iteration: 2/3\n- Sources examined: 104\n- Findings extracted: 8\n- Unresolved gaps: 4\n\n## Current Report Summary\n# Research Report: LLM Judges: Techniques, Architectures, and Applications\n\n## Executive Summary\nThe use of Large Language Models (LLMs) as automated judges represents a significant shift in AI evaluation, offering a scalable alternative to human annotation. Research indicates that high-performing models, particularly GPT-4, can achieve over 80% agreement with human evaluators on standard benchmarks like MT-Bench and Chatbot Arena. This capability allows for rapid feedback loops in model development and alignment tasks.\n\nHowever, reliability is heavily constrained by inherent cognitive biases. \"LLM-as-a-Judge\" systems exhibit systematic patterns such as favoring their own outputs (self-preference bias), preferring longer responses regardless of quality (verbosity bias), and showing sensitivity to the order of options presented (position bias). To combat these, sophisticated mitigation strategies including Chain-of-Thought (CoT) prompting and permutation-based consistency checks have become standard practice.\n\nCurrent architectures generally fall into pairwise comparison or direct scoring frameworks. While general-purpose chat evaluation is maturing, the field is evolving toward specialized, ensemble-based approaches for complex domains. For instance, software engineering evaluations now increasingly rely on hybrid systems that combine LLM reasoning with objective code execution verification to bridge the accuracy gap in technical tasks.\n\n## Key Findings\n\n### Cognitive Biases & Limitations\n- **Systematic Bias Patterns**: LLM judges demonstrate distinct cognitive biases that compromise evaluation integrity. The most prevalent include 'self-preference bias' (favoring outputs generated by the same model family), 'position bias' (consistently favoring the first or second option in a pair), and 'verbosity bias' (rating longer responses higher, independent of content quality) [src-67c025c2] [src-45a8de46] [src-48201995] [src-a4549098].\n- **Impact on Reliability**: These bi\n\n[Report truncated...]\n\n## Unresolved Knowledge Gaps\n\n### Gap: gap-b3e1de76\nDescription: While 'Judge Assembly' and ensemble methods are mentioned, specific architectural patterns for orchestrating these cost-effectively in production (latency vs. accuracy trade-offs) are under-documented in the provided sources.\nPriority: 1\nSuggested queries from analysis:\n  - architectural patterns for LLM judge ensembles production\n  - latency cost trade-off LLM-as-a-judge assembly\n\n### Gap: gap-dd9a1a3b\nDescription: The sources discuss biases extensively but lack detailed comparative data on the efficacy of 'Reference-free' vs. 'Reference-based' evaluation across different model sizes (e.g., can a small model effectively judge a large model if provided a reference?).\nPriority: 2\nSuggested queries from analysis:\n  - reference-free vs reference-based LLM evaluation accuracy comparison\n  - small model judge performance with ground truth references\n\n### Gap: gap-a6a0f789\nDescription: While RAG is mentioned as an application, there is a lack of specific detail on how LLM judges evaluate the 'retrieval' component separately from the 'generation' component (e.g., context relevance vs. answer faithfulness) in the provided sources.\nPriority: 1\nSuggested queries from analysis:\n  - LLM-as-a-judge metrics for RAG retrieval context relevance\n  - evaluating faithfulness vs answer relevance in RAG using LLMs\n\n### Gap: gap-15d06b0d\nDescription: There is limited information on the cost-latency trade-offs of deploying LLM judges at scale versus using smaller, fine-tuned judge models.\nPriority: 2\nSuggested queries from analysis:\n  - cost analysis of LLM-as-a-judge at scale\n  - distilling GPT-4 into smaller judge models performance comparison\n\n## High-Confidence Findings Already Established\n- LLM Judges exhibit systematic cognitive biases, most notably 'self-preference bias' (favoring their own generated outputs), 'position bias' (favoring responses appearing earlier or later in a pair), a\n- To mitigate evaluation biases, researchers employ techniques such as 'Chain-of-Thought' (CoT) prompting to induce reasoning before scoring, position swapping (running the eval twice with swapped order\n- LLM judges exhibit distinct cognitive biases that compromise reliability, most notably 'self-preference bias' (favoring their own generated outputs), 'verbosity bias' (favoring longer responses regard\n- GPT-4 remains the standard for 'Judge' models, capable of achieving over 80% agreement with human preferences on benchmarks like MT-Bench and Chatbot Arena, effectively matching controlled human agree\n\n## Instructions\n1. Analyze each gap for severity and addressability\n2. Generate focused follow-up queries for addressable gaps\n3. Mark any gaps that are actually addressed by existing findings\n4. Recommend whether iteration is worthwhile given remaining gaps\n\nReturn your analysis as JSON.", "raw_response": "```json\n{\n    \"gap_analysis\": [\n        {\n            \"gap_id\": \"gap-b3e1de76\",\n            \"severity\": \"moderate\",\n            \"addressable\": true,\n            \"rationale\": \"Understanding production architectures (ensembles, cascades) is crucial for the 'Architectures' part of the core query, moving beyond theoretical performance to practical implementation.\"\n        },\n        {\n            \"gap_id\": \"gap-dd9a1a3b\",\n            \"severity\": \"moderate\",\n            \"addressable\": true,\n            \"rationale\": \"Clarifying the role of ground truth references allows for better recommendations on when to use cheaper/smaller models versus larger reasoning models.\"\n        },\n        {\n            \"gap_id\": \"gap-a6a0f789\",\n            \"severity\": \"critical\",\n            \"addressable\": true,\n            \"rationale\": \"RAG is a primary application of LLM judges. Missing specific metrics (faithfulness vs. relevance) leaves a major hole in the 'Applications' section.\"\n        },\n        {\n            \"gap_id\": \"gap-15d06b0d\",\n            \"severity\": \"moderate\",\n            \"addressable\": true,\n            \"rationale\": \"Cost/latency data is essential for the 'techniques' aspect, specifically regarding model distillation for evaluation tasks.\"\n        }\n    ],\n    \"follow_up_queries\": [\n        {\n            \"query\": \"LLM judge ensemble architectures cascading models cost latency trade-off\",\n            \"target_gap_id\": \"gap-b3e1de76\",\n            \"rationale\": \"Targets specific architectural patterns for combining models to balance cost and accuracy.\",\n            \"priority\": 1\n        },\n        {\n            \"query\": \"RAGAS metrics explanation context relevance vs faithfulness evaluation\",\n            \"target_gap_id\": \"gap-a6a0f789\",\n            \"rationale\": \"Directly investigates the standard metrics used in RAG evaluation frameworks to define how retrieval vs. generation is judged.\",\n            \"priority\": 1\n        },\n        {\n            \"query\": \"can small language models judge large models with reference ground truth\",\n            \"target_gap_id\": \"gap-dd9a1a3b\",\n            \"rationale\": \"Investigates the specific capability of smaller models when aided by gold-standard references.\",\n            \"priority\": 2\n        },\n        {\n            \"query\": \"distilling GPT-4 into specialized judge models performance benchmarks\",\n            \"target_gap_id\": \"gap-15d06b0d\",\n            \"rationale\": \"Seeks quantitative data on the efficacy of using smaller, fine-tuned judges derived from larger teacher models.\",\n            \"priority\": 2\n        }\n    ],\n    \"addressed_gap_ids\": [],\n    \"iteration_recommendation\": {\n        \"should_iterate\": true,\n        \"rationale\": \"Several specific, high-value gaps remain regarding practical implementation (RAG metrics, ensemble architectures, and distillation) which are critical for a comprehensive guide on 'Applications' and 'Architectures'.\"\n    }\n}\n```", "parse_success": true, "gap_analysis": [{"gap_id": "gap-b3e1de76", "severity": "moderate", "addressable": true, "rationale": "Understanding production architectures (ensembles, cascades) is crucial for the 'Architectures' part of the core query, moving beyond theoretical performance to practical implementation."}, {"gap_id": "gap-dd9a1a3b", "severity": "moderate", "addressable": true, "rationale": "Clarifying the role of ground truth references allows for better recommendations on when to use cheaper/smaller models versus larger reasoning models."}, {"gap_id": "gap-a6a0f789", "severity": "critical", "addressable": true, "rationale": "RAG is a primary application of LLM judges. Missing specific metrics (faithfulness vs. relevance) leaves a major hole in the 'Applications' section."}, {"gap_id": "gap-15d06b0d", "severity": "moderate", "addressable": true, "rationale": "Cost/latency data is essential for the 'techniques' aspect, specifically regarding model distillation for evaluation tasks."}], "follow_up_queries": [{"query": "LLM judge ensemble architectures cascading models cost latency trade-off", "target_gap_id": "gap-b3e1de76", "rationale": "Targets specific architectural patterns for combining models to balance cost and accuracy.", "priority": 1}, {"query": "RAGAS metrics explanation context relevance vs faithfulness evaluation", "target_gap_id": "gap-a6a0f789", "rationale": "Directly investigates the standard metrics used in RAG evaluation frameworks to define how retrieval vs. generation is judged.", "priority": 1}, {"query": "can small language models judge large models with reference ground truth", "target_gap_id": "gap-dd9a1a3b", "rationale": "Investigates the specific capability of smaller models when aided by gold-standard references.", "priority": 2}, {"query": "distilling GPT-4 into specialized judge models performance benchmarks", "target_gap_id": "gap-15d06b0d", "rationale": "Seeks quantitative data on the efficacy of using smaller, fine-tuned judges derived from larger teacher models.", "priority": 2}], "addressed_gap_ids": [], "should_iterate": true}}
{"timestamp": "2026-01-01T01:17:21.368378Z", "event_id": "161a948af44f4f70a4f8b9f9491dffac", "event_type": "phase_complete", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "refinement", "iteration": 2, "data": {"phase": "refinement", "duration_ms": 23762.814695946872}}
{"timestamp": "2026-01-01T01:17:21.368650Z", "event_id": "6d808ace7b6249fe8ceadeb957d9060a", "event_type": "phase_start", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"phase": "gathering"}}
{"timestamp": "2026-01-01T01:17:23.113860Z", "event_id": "3ab82de14fea43d9bf9f2a9e1fcc088a", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "tavily", "sub_query_id": "subq-0aa14b43", "sub_query": "can small language models judge large models with reference ground truth", "sources_added": 5}}
{"timestamp": "2026-01-01T01:17:23.545315Z", "event_id": "08025e7930554ee6857451cba291f34c", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "perplexity", "sub_query_id": "subq-0aa14b43", "sub_query": "can small language models judge large models with reference ground truth", "sources_added": 5}}
{"timestamp": "2026-01-01T01:17:23.652066Z", "event_id": "6ff0088c1848410a81213ac0966df19b", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "tavily", "sub_query_id": "subq-5e232461", "sub_query": "LLM judge ensemble architectures cascading models cost latency trade-off", "sources_added": 5}}
{"timestamp": "2026-01-01T01:17:24.054273Z", "event_id": "3f82276abeae492689104eb2ed94c3ac", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "perplexity", "sub_query_id": "subq-5e232461", "sub_query": "LLM judge ensemble architectures cascading models cost latency trade-off", "sources_added": 5}}
{"timestamp": "2026-01-01T01:17:24.082839Z", "event_id": "0a6be80c8cd5469eb7dbf65ed58a4cc0", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "google", "sub_query_id": "subq-0aa14b43", "sub_query": "can small language models judge large models with reference ground truth", "sources_added": 5}}
{"timestamp": "2026-01-01T01:17:24.345902Z", "event_id": "f0d8e1c0418f418091e25b4019a00125", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "semantic_scholar", "sub_query_id": "subq-0aa14b43", "sub_query": "can small language models judge large models with reference ground truth", "sources_added": 1}}
{"timestamp": "2026-01-01T01:17:24.469373Z", "event_id": "3440f14b0870416c8dc9e705a836af13", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "google", "sub_query_id": "subq-5e232461", "sub_query": "LLM judge ensemble architectures cascading models cost latency trade-off", "sources_added": 5}}
{"timestamp": "2026-01-01T01:17:25.137357Z", "event_id": "df8d02b3150f4403abe8516b7f64ff0a", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "tavily", "sub_query_id": "subq-e87fb13a", "sub_query": "RAGAS metrics explanation context relevance vs faithfulness evaluation", "sources_added": 5}}
{"timestamp": "2026-01-01T01:17:25.589563Z", "event_id": "350d683d1e40410a8728dfb3aa3f3a48", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "perplexity", "sub_query_id": "subq-e87fb13a", "sub_query": "RAGAS metrics explanation context relevance vs faithfulness evaluation", "sources_added": 5}}
{"timestamp": "2026-01-01T01:17:25.796681Z", "event_id": "afe9f5c740674db9871705c3a5ab5e4c", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "semantic_scholar", "sub_query_id": "subq-5e232461", "sub_query": "LLM judge ensemble architectures cascading models cost latency trade-off", "sources_added": 0}}
{"timestamp": "2026-01-01T01:17:25.938650Z", "event_id": "e3a70b2de2c94d369b38bfec0e5455d9", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "google", "sub_query_id": "subq-e87fb13a", "sub_query": "RAGAS metrics explanation context relevance vs faithfulness evaluation", "sources_added": 5}}
{"timestamp": "2026-01-01T01:17:26.091290Z", "event_id": "786db95fc94c468e9298abb0cec0cd56", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "semantic_scholar", "sub_query_id": "subq-e87fb13a", "sub_query": "RAGAS metrics explanation context relevance vs faithfulness evaluation", "sources_added": 0}}
{"timestamp": "2026-01-01T01:17:26.670884Z", "event_id": "9cf0193e95664218947a452a9b7ce6a3", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "tavily", "sub_query_id": "subq-3b954383", "sub_query": "distilling GPT-4 into specialized judge models performance benchmarks", "sources_added": 5}}
{"timestamp": "2026-01-01T01:17:27.192225Z", "event_id": "e0b60142596b43ddaa17db49e91ae97b", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "perplexity", "sub_query_id": "subq-3b954383", "sub_query": "distilling GPT-4 into specialized judge models performance benchmarks", "sources_added": 5}}
{"timestamp": "2026-01-01T01:17:27.629943Z", "event_id": "f85223b8c85f4f07a894d629b452a2a3", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "google", "sub_query_id": "subq-3b954383", "sub_query": "distilling GPT-4 into specialized judge models performance benchmarks", "sources_added": 5}}
{"timestamp": "2026-01-01T01:17:28.929963Z", "event_id": "9a01df5cefdc4a849ee39c42fbf59239", "event_type": "gathering_provider_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"provider": "semantic_scholar", "sub_query_id": "subq-3b954383", "sub_query": "distilling GPT-4 into specialized judge models performance benchmarks", "sources_added": 0}}
{"timestamp": "2026-01-01T01:17:28.945199Z", "event_id": "75c42a528aab4b468a9e3044ceda1bb4", "event_type": "gathering_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"source_count": 52, "queries_executed": 4, "queries_failed": 0, "unique_urls": 52, "providers_used": ["tavily", "perplexity", "google", "semantic_scholar"], "providers_unavailable": []}}
{"timestamp": "2026-01-01T01:17:28.948857Z", "event_id": "9cd1c606141c406794d45cde87e308ba", "event_type": "phase_complete", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "gathering", "iteration": 3, "data": {"phase": "gathering", "duration_ms": 7580.202063021716}}
{"timestamp": "2026-01-01T01:17:28.949175Z", "event_id": "b83a6cf41bf3493fb965a3a08d0c94ab", "event_type": "phase_start", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "analysis", "iteration": 3, "data": {"phase": "analysis"}}
{"timestamp": "2026-01-01T01:17:57.624057Z", "event_id": "e9b25abfae574ac3a7b84b1c6f6dc65d", "event_type": "analysis_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "analysis", "iteration": 3, "data": {"provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview", "tokens_used": 18242, "duration_ms": null, "system_prompt": "You are a research analyst. Your task is to analyze research sources and extract key findings, assess their quality, and identify knowledge gaps.\n\nYour response MUST be valid JSON with this exact structure:\n{\n    \"findings\": [\n        {\n            \"content\": \"A clear, specific finding or insight extracted from the sources\",\n            \"confidence\": \"low|medium|high\",\n            \"source_ids\": [\"src-xxx\", \"src-yyy\"],\n            \"category\": \"optional category/theme\"\n        }\n    ],\n    \"gaps\": [\n        {\n            \"description\": \"Description of missing information or unanswered question\",\n            \"suggested_queries\": [\"follow-up query 1\", \"follow-up query 2\"],\n            \"priority\": 1\n        }\n    ],\n    \"quality_updates\": [\n        {\n            \"source_id\": \"src-xxx\",\n            \"quality\": \"low|medium|high\"\n        }\n    ]\n}\n\nGuidelines for findings:\n- Extract 2-5 key findings from the sources\n- Each finding should be a specific, actionable insight\n- Confidence levels: \"low\" (single weak source), \"medium\" (multiple sources or one authoritative), \"high\" (multiple authoritative sources agree)\n- Include source_ids that support each finding\n- Categorize findings by theme when applicable\n\nGuidelines for gaps:\n- Identify 1-3 knowledge gaps or unanswered questions\n- Provide specific follow-up queries that could fill each gap\n- Priority 1 is most important, higher numbers are lower priority\n\nGuidelines for quality_updates:\n- Assess source quality based on authority, relevance, and recency\n- \"low\" = questionable reliability, \"medium\" = generally reliable, \"high\" = authoritative\n\nIMPORTANT: Return ONLY valid JSON, no markdown formatting or extra text.", "user_prompt": "Original Research Query: LLM Judges: techniques, architectures, evaluation methods, and applications for using large language models as automated evaluators and judges\n\nResearch Brief:\nThis research will investigate the methodology and infrastructure of using Large Language Models as automated evaluators, focusing on core architectures like pairwise comparison and direct scoring. It will also examine the metrics used to validate these judges against human baselines and explore their practical applications in RAG and model alignment.\n\nSources to Analyze:\n\nSource 1 (ID: src-67c025c2):\n  Title: Self-Preference Bias in LLM-as-a-Judge\n  URL: https://openreview.net/forum?id=Ns8zGZ0lmM\n  Snippet: ## Self-Preference Bias in LLM-as-a-Judge. **TL;DR:** We propose a novel quantitative metric to measure self-preference bias in LLM-as-a-judge. However, the self-preference bias in LLMs has posed significant risks, including promoting specific styles or policies intrinsic to the LLMs. Despite the importance of this issue, there is a lack of established methods to measure the self-preference bias quantitatively, and its underlying causes are poorly understood. In this paper, we introduce a novel ...\n  Content: [Go to **ICLR 2025 Conference** homepage](/group?id=ICLR.cc/2025/Conference \"Venue Homepage\")\n\n## Self-Preference Bias in LLM-as-a-Judge\n\n### [Koki Wataoka](/profile?id=~Koki_Wataoka1 \"~Koki_Wataoka1\"), [Tsubasa Takahashi](/profile?id=~Tsubasa_Takahashi1 \"~Tsubasa_Takahashi1\"), [Ryokan Ri](/profile?id=~Ryokan_Ri1 \"~Ryokan_Ri1\")\n\n27 Sept 2024 (modified: 05 Feb 2025)Submitted to ICLR 2025Everyone[Revisions](/revisions?id=Ns8zGZ0lmM)[BibTeX](#)[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/ \"Licensed under Creative Commons Attribution 4.0 International\")\n\n**Keywords:** large language model, llm-as-a-judge, bias, fairness\n\n**TL;DR:** We propose a novel quantitative metric to measure self-preference bias in LLM-as-a-judge.\n\n**Abstract:** Automated evaluation leveraging large language models (LLMs), commonly referred to as LLM evaluators or LLM-as-a-judge, has been widely used in measuring the performance of dialogue systems. However, the self-preference bias in LLMs has posed signi...\n\nSource 2 (ID: src-45a8de46):\n  Title: Self-Preference Bias in LLM-as-a-Judge\n  URL: https://arxiv.org/html/2410.21819v1\n  Snippet: (2024) addressed quantifying self-preference bias within an evaluation approach where LLMs assign an absolute score to a single generated text. This suggests that the fundamental cause of self-preference bias may be the familiarity of the texts to the LLM evaluators, specifically how likely they are to generate the same response. The contributions of this paper are threefold: (1) We propose a new metric to quantify self-preference bias in LLMs; (2) Using this metric, we evaluate the extent of se...\n  Content: # Self-Preference Bias in LLM-as-a-Judge\n\n[Koki Wataoka](https://orcid.org/0000-0000-0000-0000)   \nSB Intuitions   \nkoki.wataoka@sbintuitions.co.jp   \n&[Tsubasa Takahashi](https://orcid.org/0000-0000-0000-0000)   \nSB Intuitions   \ntsubasa.takahashi@sbintuitions.co.jp   \n&[Ryokan Ri](https://orcid.org/0000-0000-0000-0000)   \nSB Intuitions   \nryokan.ri@sbintuitions.co.jp\n\n###### Abstract\n\nAutomated evaluation leveraging large language models (LLMs), commonly referred to as LLM evaluators or LLM-as-a-judge, has been widely used in measuring the performance of dialogue systems. However, the self-preference bias in LLMs has posed significant risks, including promoting specific styles or policies intrinsic to the LLMs. Despite the importance of this issue, there is a lack of established methods to measure the self-preference bias quantitatively, and its underlying causes are poorly understood. In this paper, we introduce a novel quantitative metric to measure the self-preference bias. Our ex...\n\nSource 3 (ID: src-48201995):\n  Title: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n  URL: https://neurips.cc/virtual/2023/poster/73434\n  Snippet: We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability\n  Content: ## Main Navigation\n\n![conference_logo](/static/core/img/neurips-navbar-logo.svg)\n\n# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n\n### Abstract\n\nEvaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences.To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions.We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them.We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform.Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\\% agreement, the same level o...\n\nSource 4 (ID: src-e0d1753b):\n  Title: Mitigating the Bias of Large Language Model Evaluation\n  URL: https://aclanthology.org/2024.ccl-1.101.pdf\n  Snippet: In this work, we propose two methods for mitigating the bias of LLM-as-a-Judge. For closed-source judge models, we propose to mitigate the bias\n  Content: Mitigating the Bias of Large Language Model Evaluation Hongli Zhou1, Hui Huang2, Yunfei Long3, Bing Xu2, Conghui Zhu2, Hailong Cao2, Muyun Yang2\u2217, Tiejun Zhao2 1School of Architecture and Design, Harbin Institute of Technology, Harbin, China 2Faculty of Computing, Harbin Institute of Technology, Harbin, China 3University of Essex {hongli.joe,huanghui}@stu.hit.edu.cn;yl20051@essex.ac.uk; {hitxb,conghui,caohailong,yangmuyun,tjzhao}@hit.edu.cn Abstract Recently, there has been a trend of evaluating the Large Language Model (LLM) quality in the flavor of LLM-as-a-Judge, namely leveraging another LLM to evaluate the current output qual-ity. However, existing judges are proven to be biased, namely they would favor answers which present better superficial quality (such as verbosity, fluency) while ignoring the instruction fol-lowing ability. In this work, we propose systematic research about the bias of LLM-as-a-Judge.\nSpecifically, for closed-source judge models, we apply calibration to miti...\n\nSource 5 (ID: src-8d0c93da):\n  Title: 5 Techniques to Improve LLM-Judges : r/LLMDevs\n  URL: https://www.reddit.com/r/LLMDevs/comments/1j3gbil/5_techniques_to_improve_llmjudges/\n  Snippet: But using LLMs as a judge does come with some drawbacks\u2014like narcissistic bias (favoring their own outputs), a preference for verbosity (over\n  Content: ![r/LLMDevs icon](https://styles.redditmedia.com/t5_7xegfq/styles/communityIcon_b553dnae9oia1.png?width=96&height=96&frame=1&auto=webp&crop=96%3A96%2Csmart&s=8ea201f189c513413bda6216591bb75e74ae6b0c)\n\n# 5 Techniques to Improve LLM-Judges\n\nLLM-based metrics are currently the best method for evaluating LLM applications. But using LLMs as a judge does come with some drawbacks\u2014like narcissistic bias (favoring their own outputs), a preference for verbosity (over concise answers), unreliable fine-grained scoring (whereas binary outputs are much more accurate), and positional bias (prefer answer choices that come up first).\n\nFortunately, there are several methods and techniques you can employ to minimize these shortcomings when creating your LLM evaluation metrics. For anyone who\u2019s interested, I\u2019ve written a more [in-depth blog here](https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method#improving-llm-judgements).\n\n# 1. Chain-Of-Thought Prompting\n\nChain-of-thou...\n\nSource 6 (ID: src-08525cff):\n  Title: LLM-as-a-Judge: Unveiling Its Potential and Applications - Medium\n  URL: https://medium.com/@ganeshkannappan/llm-as-a-judge-unveiling-its-potential-and-applications-cbfb3db14e26\n  Snippet: * **Quantitative (or numeric) Grading** \u2014 The evaluator LLM assigns a numerical score to the answer, such as 0\u201310 or 0\u2013100, based on predefined criteria. **Objective Evaluation** \u2014 Single answer grading provides an **objective** and structured way to assess a model\u2019s response. The evaluator (in this case, the LLM) checks the generated response against the reference response and scores or judges the quality based on how closely the generated answer aligns with the reference answer in terms of acc...\n  Content: [Sitemap](/sitemap/sitemap.xml)\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40ganeshkannappan%2Fllm-as-a-judge-unveiling-its-potential-and-applications-cbfb3db14e26&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40ganeshkannappan%2Fllm-as-a-judge-unveiling-its-potential-and-applications-cbfb3db14e26&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# LLM-as-a-Judge: Unveiling Its Potential and Applications\n\n[Ganesh Kannappan](/@ganeshkannappan?source=post_page---byline--cbfb3db14e26---------------------------------------)\n\n12 min read\n\n\u00b7\n\nDec 2, 2024\n\n--\n\nIn the [previous part](/@ganeshkannappan/llm-as-a-judge-...\n\nSource 7 (ID: src-51263506):\n  Title: Using LLMs for Evaluation - by Cameron R. Wolfe, Ph.D.\n  URL: https://cameronrwolfe.substack.com/p/llm-as-a-judge\n  Snippet: LLM-as-a-judge is a reference-free metric that directly prompts a powerful LLM to evaluate the quality of another model\u2019s output. For LLM-as-a-Judge evaluations, authors adopt the same strategy proposed by Vicuna [2], where the quality of model outputs is judged by via a pairwise prompt to GPT-4. The task of these annotators is to evaluate the quality of stories written for 200 prompts, where for each prompt we *i)* sample a response from GPT-2 (i.e., a weaker LLM) and *ii)* have a human write a...\n  Content: # [Deep (Learning) Focus](/)\n\n# Using LLMs for Evaluation\n\n### LLM-as-a-Judge and other scalable additions to human quality ratings...\n\n[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)\n\nJul 22, 2024\n\nAs large language models (LLMs) have become more and more capable, one of the most difficult aspects of working with these models is determining how to properly evaluate them. Many powerful models exist, and they each solve a wide variety of complex, open-ended tasks. As a result, discerning differences in performance between these models can be difficult. The most reliable method of evaluating LLMs is with human feedback, but collecting data from humans is noisy, time consuming, and expensive. Despite being a valuable and necessary source of truth for measuring model capabilities, human evaluation\u2014*when used in isolation*\u2014impedes our ability to iterate quickly during model development. To solve this problem, we need an evaluation metric that is quick, cost effective, and si...\n\nSource 8 (ID: src-2a4435f2):\n  Title: A Survey on LLM-as-a-Judge - arXiv\n  URL: https://arxiv.org/html/2411.15594v1\n  Snippet: To automate evaluation by LLM-as-a-Judge, one effective approach is to employ advanced language models such as GPT-4\u00a0(OpenAI, 2023a) instead of human evaluators\u00a0(Zheng et\u00a0al., 2023c). Unlike INSTRUCTSCORE which directly optimizes the model, the LLM evaluator in JADE(Zhang et\u00a0al., 2023c) relies on human judges to correct LLMs\u2019 evaluation results and updates the most frequently corrected samples into the example sets for few-shot prompting. In addition to integrating results from multiple rounds o...\n  Content: 11footnotetext: \\* These authors contributed equally to this research.22footnotetext: \u2020 Corresponding author.\n\n# A Survey on LLM-as-a-Judge\n\nJiawei Gu1,\\*, Xuhui Jiang1,\\*, Zhichao Shi1,2,\\*, Hexiang Tan2, Xuehao Zhai3, Chengjin Xu1, Wei Li2, Yinghan Shen2, Shengjie Ma1,4, Honghao Liu1,   \nYuanzhuo Wang2, Jian Guo1,\u2020     \n1IDEA Research, International Digital Economy Academy   \n2Institute of Computing Technology, Chinese Academy of Sciences   \n3Department of Civil and Environmental Engineering, Imperial College London   \n4Gaoling School of Artificial Intelligence, Renmin University of China China\n\n###### Abstract.\n\n## Abstract\n\nAccurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of \u201dLLM-as-a-Judge,\u201d where LLMs are employed as evaluators for complex task...\n\nSource 9 (ID: src-bbd215f1):\n  Title: LLM-as-a-Judge - by Nilesh Barla - Adaline Labs\n  URL: https://labs.adaline.ai/p/llm-as-a-judge\n  Snippet: Evaluating LLM outputs can save you a lot of time from shipping broken prompts and features. And for such a situation where you cannot write detailed instructions every time, you need to find a way to evaluate every output from the LLM. LLM-as-a-Judge is a framework where LLMs evaluate outputs from other LLMs using **structured prompts** to score qualities like **coherence** or **accuracy**. Teams need scalable evaluation methods that can assess LLM outputs with human-like judgment but without t...\n  Content: # [Adaline Labs](/)\n\n# LLM-as-a-Judge\n\n### A brief research note on LLM-as-a-judge including best practices.\n\n[Nilesh Barla](https://substack.com/@iridium0077)\n\nSep 08, 2025\n\nEvaluating LLM outputs can save you a lot of time from shipping broken prompts and features.\n\nA lot of talk and discussion is going on when it comes to the degrading performance or output of LLMs. You go to Reddit and you will find that users are not satisfied with LLMs such as Claude (these days) and GPT-5.\n\nSo, what's going on with LLMs?\n\nYou provide an input or prompt addressing your requirements, and the LLM doesn\u2019t provide you with a desirable answer. This might be happening because of one of two reasons, or both:\n\n1. Bad prompt\n2. Bad LLM\n\nNow, I understand that in a certain workflow that includes creativity, such as writing and brainstorming, you can hone the LLMs by using more structured prompting. For the most part, they will be satisfactory.\n\nBut when it comes to more logical and complex workflows, like ...\n\nSource 10 (ID: src-78c4677b):\n  Title: LLM-as-a-Judge: How AI Can Evaluate AI Faster and Smarter\n  URL: https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-j/\n  Snippet: An LLM-as-a-Judge evaluation uses an LLM to mimic human judgment of another LLM's output. It's not a fixed mathematical metric like \u201caccuracy\u201d \u2013\n  Content: Select platform to login\n\n[**Cloud Management**\n\nWebservers and Virtual Machines](https://cloud.bunnyshell.com/login/)[**Environments as a Service**\n\nCreate and Manage Kubernetes Environments](https://environments.bunnyshell.com/login/)\n\n[blog](/blog/)\n\n/[Cloud computing](/blog/cloud-computing/)\n\n# When AI Becomes the Judge: Understanding \u201cLLM-as-a-Judge\u201d\n\n[engineering](/blog/engineering/)\n\n[Alin Dobra](/blog/author/alin-dobra/)\n\nWhy Use an LLM as Judge?\n\nHow LLM-Judges Work\n\nArchitectures: Judge Assembly vs Super Judge\n\nUse Cases and Examples\n\nBuilding an Effective LLM Judge: Tips and Pitfalls\n\nPowering LLM-Evaluation with Bunnyshell\n\nConclusion\n\nImagine building a chatbot or code generator that not only writes answers \u2013 but also grades them. In the past, ensuring AI quality meant recruiting human reviewers or using simple metrics (BLEU, ROUGE) that miss nuance. Today, we can leverage **Generative AI** itself to evaluate its own work. *LLM-as-a-Judge* means using one Large Language Mo...\n\nSource 11 (ID: src-6ba1f0a1):\n  Title: Understanding Bias in LLM-as-a-Judge Systems\n  URL: https://ragmetrics.ai/blog/understanding-bias-in-llm-as-a-judge-systems\n  Snippet: # Understanding Bias in LLM-as-a-Judge Systems\n\n**The Hidden Problem in AI Evaluation**\n\nEvery developer building with GenAI has hit this moment: your evaluation pipeline says one model output is \u201cbetter,\u201d but your eyes disagree. The culprit is often bias\u2014bias not in the generating model, but in the\n\n**LLM acting as the judge**.... LLM-as-a-Judge systems are now the backbone of modern AI evaluation frameworks. They\u2019re faster, cheaper, and more consistent than human review\u2014but they\u2019re not immune ...\n\nSource 12 (ID: src-a4549098):\n  Title: A Systematic Study of Position Bias in LLM-as-a-Judge - arXiv\n  URL: https://arxiv.org/html/2406.07791v7\n  Snippet: ###### Abstract\nLLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases\u2014particularly position bias, the tendency to favor solutions based on their position within the prompt\u2014compromise its reliability. This study investigates position bias in LLM judges across pairwise and list-wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness.... Our experiments, involving 12 LLM ju...\n\nSource 13 (ID: src-bef824af):\n  Title: The 5 Biases That Can Silently Kill Your LLM Evaluations ...\n  URL: https://www.sebastiansigl.com/blog/llm-judge-biases-and-how-to-fix-them/\n  Snippet: This is the risk you run when you trust LLM judges blindly. For all their power, they are not impartial arbiters. They are susceptible to a range of cognitive biases - predictable, systematic errors that can silently corrupt your evaluation data and lead you to make the wrong product decisions\n\n2 3. Relying on a biased judge means you could be optimizing for failure, shipping regressions, and eroding user trust, all while your metrics tell you everything is fine.... This post will guide you thro...\n\nSource 14 (ID: src-7c38a7f7):\n  Title: Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge\n  URL: https://llm-judge-bias.github.io\n  Snippet: The upper part illustrates an example of diversity bias in LLM-as-a-Judge scenarios, while the lower part displays the ranking of average consistency metrics across six models.\n\nOur proposed framework:\n\n**CALM**... |Bias Type|Description|Example|\n|--|--|--|\n|\ud83d\udd00 Position (Pos.)|When an LLM exhibits a propensity to favor certain positions over others.|$R_1$: 3.11 > 3.8 $R_2$: 3.8 > 3.11 $R_1$: 3.8 > 3.11 $R_2$: 3.11 > 3.8|\n|\ud83d\udcc4 Verbosity (Ver.)|LLM judges favor longer responses, even if they are not ...\n\nSource 15 (ID: src-c33a2512):\n  Title: Evaluating and Mitigating LLM-as-a-judge Bias in ...\n  URL: https://arxiv.org/abs/2510.12462\n  Snippet: # Computer Science > Artificial Intelligence\n\n**arXiv:2510.12462** (cs)\n\n[Submitted on 14 Oct 2025]... # Title: Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems\n\nAuthors:Jiaxin Gao, Chen Chen, Yanwen Jia, Xueluan Gong, Kwok-Yan Lam, Qian Wang\nAbstract:Large Language Models (LLMs) are increasingly being used to autonomously evaluate the quality of content in communication systems, e.g., to assess responses in telecom customer support chatbots.... However, the impartiality of...\n\nSource 16 (ID: src-1e5014bd):\n  Title: An LLM-as-Judge Metric for Bridging the Gap with Human ... - arXiv\n  URL: https://arxiv.org/html/2505.20854v1\n  Snippet: In this paper, we present SWE-Judge, the first evaluation metric for LLM-as-Ensemble-Judge specifically designed to accurately assess the correctness of generated software artifacts. We evaluate SWE-Judge across a diverse set of software engineering (SE) benchmarks\u2014including CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess\u2014which span three SE tasks: code generation, automated program repair, and code summarization. The state-of-the-art LLM-as-judge evaluation metric for code,...\n  Content: \\newmdenv\n\n[ linecolor=linecolor, leftline=true, topline=false, bottomline=false, rightline=false, linewidth=2pt, innerleftmargin=10pt, innerrightmargin=10pt, innertopmargin=5pt, innerbottommargin=5pt, backgroundcolor=bgcolor ]leftbar\n\n# An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks\n\nXin Zhou  Singapore Management UniversitySingapore  [xinzhou.2020@phdcs.smu.edu.sg](mailto:xinzhou.2020@phdcs.smu.edu.sg)  ,\u00a0 Kisub Kim  Independent ResearcherHong Kong  [falconlk00@gmail.com](mailto:falconlk00@gmail.com)  ,\u00a0 Ting Zhang  Singapore Management UniversitySingapore  [tingzhang.2019@phdcs.smu.edu.sg](mailto:tingzhang.2019@phdcs.smu.edu.sg)  ,\u00a0 Martin Weyssow  Singapore Management UniversitySingapore  [mweyssow@smu.edu.sg](mailto:mweyssow@smu.edu.sg)  ,\u00a0 Lu\u00eds F.\u00a0Gomes  Carnegie Mellon UniversityUSA  [lfgomes@andrew.cmu.edu](mailto:lfgomes@andrew.cmu.edu)  ,\u00a0 Guang Yang  Nanjing University of Aeronautics and AstronauticsChina  [novelyg@outlook.com](mailto:novelyg@o...\n\nSource 17 (ID: src-db258615):\n  Title: LLM Evaluation Frameworks, Metrics & Methods Explained\n  URL: https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained\n  Snippet: This guide breaks down key LLM evaluation methods\u2014including automatic metrics, human reviews, hybrid frameworks like G-Eval, and LLM-as-a-Judge strategies. To get the most out of LLM-as-a-judge, teams often **prompt-engineer the evaluation** carefully (more on this in the G-Eval section), and may use a two-step process: first have the AI judge give a detailed rationale or score for multiple criteria, then possibly have a human review a subset of those judgments for quality control. It complement...\n  Content: Start Safeguarding Your LLM\u00a0Today!\n\nImplementing Qualifire is simple. Contact our team today, and\u00a0we\u2019ll get you started in no time!\n\nTalk to our team\n\nDror Ivry\n\n30/5/2025\n\nTable of content\n\n[What is HELM?](#)\n\n# LLM Evaluation Frameworks, Metrics & Methods Explained\n\n## **Introduction**\n\nLarge Language Models (LLMs) are increasingly deployed in chatbots, virtual assistants, and other user-facing applications. Ensuring these models produce high-quality, safe, and helpful responses is a major challenge. This makes evaluation a critical part of the development and deployment cycle for LLM-powered chat systems. Unlike traditional NLP tasks with clear-cut metrics, open-ended dialog requires careful **evaluation strategies**. In this post, we\u2019ll explore the spectrum of LLM evaluation methods \u2013 from automatic metrics to human reviews and cutting-edge hybrid approaches \u2013 and discuss when each is appropriate. We\u2019ll then take a deep dive into **LLM-as-a-judge** techniques with a focus on the G-...\n\nSource 18 (ID: src-3f4263f1):\n  Title: Large Language Model Evaluation in '26: 10+ Metrics & Methods\n  URL: https://research.aimultiple.com/large-language-model-evaluation/\n  Snippet: *   **MuSR** consists of algorithmically generated complex problems, requiring models to use reasoning and long-range context parsing, with few models performing better than random.[4](https://research.aimultiple.com/large-language-model-evaluation/#easy-footnote-bottom-4-68488 \"https://huggingface.co/datasets/TAUR-Lab/MuSR\"). *   **BBH** includes 23 challenging tasks from the BigBench dataset, measuring objective metrics and language understanding, and correlates well with human preference.[7](...\n  Content: Large Language Model Evaluation in '26: 10+ Metrics & Methods\n===============\n\n[![Image 1: AIMultiple](https://research.aimultiple.com/images/logo-2025.svg)![Image 2: AIMultiple](https://research.aimultiple.com/images/logo-2025-white.svg)](https://aimultiple.com/)\n\nAI\n\nCATEGORIES\n\nAI Coding AI Foundations AI Hardware AI in Industries Document Automation Generative AI Generative AI Applications Large Language Models MCP RAG\n\n[AI Code](https://research.aimultiple.com/ai-code/)[AI Code Editor](https://research.aimultiple.com/ai-code-editor/)[AI Code Review Tools](https://research.aimultiple.com/ai-code-review-tools/)[AI Coding Benchmark](https://research.aimultiple.com/ai-coding-benchmark/)[Screenshot to Code](https://research.aimultiple.com/screenshot-to-code/)\n\nAgentic AI\n\nCATEGORIES\n\nAgent Architectures & Tools AI Agent Applications Open-Source Agents\n\n[Agentic AI](https://research.aimultiple.com/agentic-ai/)[Agentic AI Design Patterns](https://research.aimultiple.com/agentic-ai-design...\n\nSource 19 (ID: src-0378afab):\n  Title: LLM Evaluation in 2025: Metrics, RAG, LLM-as-Judge & Best Practices\n  URL: https://medium.com/@QuarkAndCode/llm-evaluation-in-2025-metrics-rag-llm-as-judge-best-practices-ad2872cfa7cb\n  Snippet: # LLM Evaluation in 2025: Metrics, RAG, LLM-as-Judge & Best Practices. Evaluating large language models (LLMs) looks deceptively simple \u2014 run a prompt, eyeball the output, ship. In reality, robust evaluation is a multi\u2011layer process that blends offline tests, human judgment, and production telemetry. Below is a practical field guide that synthesizes recent playbooks and research into a repeatable approach you can use today. ## 1) Start with what you\u2019re shipping, not just the model. Before choosi...\n  Content: [Sitemap](/sitemap/sitemap.xml)\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40QuarkAndCode%2Fllm-evaluation-in-2025-metrics-rag-llm-as-judge-best-practices-ad2872cfa7cb&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Search](/search?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40QuarkAndCode%2Fllm-evaluation-in-2025-metrics-rag-llm-as-judge-best-practices-ad2872cfa7cb&source=post_page---top_nav_layout_nav-----------------------global_nav------------...\n\nSource 20 (ID: src-66027906):\n  Title: Co-Eval: Augmenting LLM-based Evaluation with Machine Metrics\n  URL: https://aclanthology.org/2025.emnlp-main.1307.pdf\n  Snippet: Nov 4, 2025 ... We demonstrate the effectiveness of the Co-Eval framework in mitigating four types of bias\u2014self- preference bias, position bias, verbosity bias,\u00a0...\n\nPlease analyze these sources and:\n1. Extract 2-5 key findings relevant to the research query\n2. Assess confidence levels based on source agreement and authority\n3. Identify any knowledge gaps or unanswered questions\n4. Assess the quality of each source\n\nReturn your analysis as JSON.", "raw_response": "```json\n{\n    \"findings\": [\n        {\n            \"content\": \"LLM Judges exhibit distinct systematic biases that compromise reliability, specifically 'self-preference bias' (favoring their own generated outputs), 'verbosity bias' (preferring longer, more verbose responses regardless of quality), and 'position bias' (favoring answers presented first in pairwise comparisons).\",\n            \"confidence\": \"high\",\n            \"source_ids\": [\n                \"src-67c025c2\",\n                \"src-45a8de46\",\n                \"src-48201995\",\n                \"src-e0d1753b\",\n                \"src-8d0c93da\",\n                \"src-a4549098\"\n            ],\n            \"category\": \"Biases & Limitations\"\n        },\n        {\n            \"content\": \"High-performing LLMs like GPT-4 can achieve over 80% agreement with human preferences in controlled benchmarks (e.g., MT-Bench), matching the level of agreement typically found between different human evaluators, validating their utility for scalable evaluation.\",\n            \"confidence\": \"high\",\n            \"source_ids\": [\n                \"src-48201995\",\n                \"src-51263506\",\n                \"src-2a4435f2\"\n            ],\n            \"category\": \"Performance Validation\"\n        },\n        {\n            \"content\": \"Domain-specific evaluation frameworks are emerging to address general judge limitations, such as 'SWE-Judge' for software engineering (using ensemble methods for code correctness) and 'Co-Eval' which augments LLM judgments with traditional machine metrics to reduce bias.\",\n            \"confidence\": \"medium\",\n            \"source_ids\": [\n                \"src-1e5014bd\",\n                \"src-66027906\"\n            ],\n            \"category\": \"Architectures & Frameworks\"\n        },\n        {\n            \"content\": \"Proposed mitigation strategies for judge biases include the use of Chain-of-Thought (CoT) prompting to improve reasoning prior to scoring, and position-swapping in pairwise comparisons to average out positional preferences.\",\n            \"confidence\": \"medium\",\n            \"source_ids\": [\n                \"src-8d0c93da\",\n                \"src-e0d1753b\",\n                \"src-a4549098\"\n            ],\n            \"category\": \"Methodology\"\n        }\n    ],\n    \"gaps\": [\n        {\n            \"description\": \"While self-preference bias is widely identified, there is a lack of standardized, established quantitative metrics to consistently measure it across different models, and its underlying causes are not fully understood.\",\n            \"suggested_queries\": [\n                \"quantitative metrics for measuring self-preference bias in LLMs\",\n                \"root causes of self-preference bias in language models\"\n            ],\n            \"priority\": 1\n        },\n        {\n            \"description\": \"The application of LLM Judges specifically for RAG (Retrieval-Augmented Generation) systems is mentioned as a category, but detailed methodologies or specific metrics for evaluating the retrieval component versus the generation component using judges are missing.\",\n            \"suggested_queries\": [\n                \"LLM-as-a-judge methodologies for RAG systems\",\n                \"evaluating retrieval accuracy vs generation quality with LLM judges\"\n            ],\n            \"priority\": 2\n        }\n    ],\n    \"quality_updates\": [\n        {\n            \"source_id\": \"src-67c025c2\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-48201995\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-e0d1753b\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-a4549098\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-1e5014bd\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-66027906\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-2a4435f2\",\n            \"quality\": \"high\"\n        },\n        {\n            \"source_id\": \"src-51263506\",\n            \"quality\": \"medium\"\n        },\n        {\n            \"source_id\": \"src-08525cff\",\n            \"quality\": \"medium\"\n        },\n        {\n            \"source_id\": \"src-8d0c93da\",\n            \"quality\": \"low\"\n        }\n    ]\n}\n```", "parse_success": true, "findings": [{"content": "LLM Judges exhibit distinct systematic biases that compromise reliability, specifically 'self-preference bias' (favoring their own generated outputs), 'verbosity bias' (preferring longer, more verbose responses regardless of quality), and 'position bias' (favoring answers presented first in pairwise comparisons).", "confidence": "high", "source_ids": ["src-67c025c2", "src-45a8de46", "src-48201995", "src-e0d1753b", "src-8d0c93da", "src-a4549098"], "category": "Biases & Limitations"}, {"content": "High-performing LLMs like GPT-4 can achieve over 80% agreement with human preferences in controlled benchmarks (e.g., MT-Bench), matching the level of agreement typically found between different human evaluators, validating their utility for scalable evaluation.", "confidence": "high", "source_ids": ["src-48201995", "src-51263506", "src-2a4435f2"], "category": "Performance Validation"}, {"content": "Domain-specific evaluation frameworks are emerging to address general judge limitations, such as 'SWE-Judge' for software engineering (using ensemble methods for code correctness) and 'Co-Eval' which augments LLM judgments with traditional machine metrics to reduce bias.", "confidence": "medium", "source_ids": ["src-1e5014bd", "src-66027906"], "category": "Architectures & Frameworks"}, {"content": "Proposed mitigation strategies for judge biases include the use of Chain-of-Thought (CoT) prompting to improve reasoning prior to scoring, and position-swapping in pairwise comparisons to average out positional preferences.", "confidence": "medium", "source_ids": ["src-8d0c93da", "src-e0d1753b", "src-a4549098"], "category": "Methodology"}], "gaps": [{"description": "While self-preference bias is widely identified, there is a lack of standardized, established quantitative metrics to consistently measure it across different models, and its underlying causes are not fully understood.", "suggested_queries": ["quantitative metrics for measuring self-preference bias in LLMs", "root causes of self-preference bias in language models"], "priority": 1}, {"description": "The application of LLM Judges specifically for RAG (Retrieval-Augmented Generation) systems is mentioned as a category, but detailed methodologies or specific metrics for evaluating the retrieval component versus the generation component using judges are missing.", "suggested_queries": ["LLM-as-a-judge methodologies for RAG systems", "evaluating retrieval accuracy vs generation quality with LLM judges"], "priority": 2}], "quality_updates": [{"source_id": "src-67c025c2", "quality": "high"}, {"source_id": "src-48201995", "quality": "high"}, {"source_id": "src-e0d1753b", "quality": "high"}, {"source_id": "src-a4549098", "quality": "high"}, {"source_id": "src-1e5014bd", "quality": "high"}, {"source_id": "src-66027906", "quality": "high"}, {"source_id": "src-2a4435f2", "quality": "high"}, {"source_id": "src-51263506", "quality": "medium"}, {"source_id": "src-08525cff", "quality": "medium"}, {"source_id": "src-8d0c93da", "quality": "low"}]}}
{"timestamp": "2026-01-01T01:17:57.625793Z", "event_id": "694405dbf95e4f6d897c28b3a605ee1d", "event_type": "phase_complete", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "analysis", "iteration": 3, "data": {"phase": "analysis", "duration_ms": 28677.859596034978}}
{"timestamp": "2026-01-01T01:17:57.626180Z", "event_id": "7bd5279dbf214ae0b6670e433a7d58ff", "event_type": "phase_start", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "synthesis", "iteration": 3, "data": {"phase": "synthesis"}}
{"timestamp": "2026-01-01T01:18:35.500623Z", "event_id": "012fdf0d9dd14951a957ff65be21daa4", "event_type": "synthesis_result", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "synthesis", "iteration": 3, "data": {"provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview", "tokens_used": 14836, "duration_ms": null, "system_prompt": "You are a research synthesizer. Your task is to create a comprehensive, well-structured research report from analyzed findings.\n\nGenerate a markdown-formatted report with the following structure:\n\n# Research Report: [Topic]\n\n## Executive Summary\nA 2-3 paragraph overview of the key insights and conclusions.\n\n## Key Findings\n\n### [Theme/Category 1]\n- Finding with supporting evidence and source citations [Source ID]\n- Related findings grouped together\n\n### [Theme/Category 2]\n- Continue for each major theme...\n\n## Analysis\n\n### Supporting Evidence\nDiscussion of well-supported findings with high confidence.\n\n### Conflicting Information\nNote any contradictions or disagreements between sources (if present).\n\n### Limitations\nAcknowledge gaps in the research and areas needing further investigation.\n\n## Sources\nList sources as markdown links with their IDs: **[src-xxx]** [Title](URL)\n\n## Conclusions\nActionable insights and recommendations based on the findings.\n\n---\n\nGuidelines:\n- Organize findings thematically rather than listing them sequentially\n- Cite source IDs in brackets when referencing specific information [src-xxx]\n- Distinguish between high-confidence findings (well-supported) and lower-confidence insights\n- Be specific and actionable in conclusions\n- Keep the report focused on the original research query\n- Use clear, professional language\n- Include all relevant findings - don't omit information\n\nIMPORTANT: Return ONLY the markdown report, no preamble or meta-commentary.", "user_prompt": "# Research Query\nLLM Judges: techniques, architectures, evaluation methods, and applications for using large language models as automated evaluators and judges\n\n## Research Brief\nThis research will investigate the methodology and infrastructure of using Large Language Models as automated evaluators, focusing on core architectures like pairwise comparison and direct scoring. It will also examine the metrics used to validate these judges against human baselines and explore their practical applications in RAG and model alignment.\n\n## Findings to Synthesize\n\n### Bias & Limitations\n- [HIGH] LLM Judges exhibit systematic cognitive biases, most notably 'self-preference bias' (favoring their own generated outputs), 'position bias' (favoring responses appearing earlier or later in a pair), and 'verbosity bias' (rating longer responses higher regardless of quality).\n  Sources: src-67c025c2, src-45a8de46, src-48201995, src-e0d1753b, src-a4549098, src-7c38a7f7\n\n### Mitigation Techniques\n- [HIGH] To mitigate evaluation biases, researchers employ techniques such as 'Chain-of-Thought' (CoT) prompting to induce reasoning before scoring, position swapping (running the eval twice with swapped orders) to average out position bias, and 'Co-Eval' frameworks that augment LLMs with objective machine metrics.\n  Sources: src-8d0c93da, src-66027906, src-48201995, src-e0d1753b\n\n### Architecture & Performance\n- [MEDIUM] Two primary architectures dominate LLM-as-a-Judge: 'Pairwise Comparison' (mimicking human preference testing like Chatbot Arena) and 'Direct Scoring/Pointwise' (assigning absolute scores like 1-10), with strong models like GPT-4 achieving over 80% agreement with human annotators in general chat domains.\n  Sources: src-48201995, src-51263506, src-2a4435f2\n\n### Advanced Architectures\n- [MEDIUM] Specialized 'Ensemble' or 'Judge Assembly' approaches are emerging for complex domains, such as 'SWE-Judge' for software engineering, which combines LLM reasoning with code execution/static analysis to bridge the gap with human verification in technical tasks.\n  Sources: src-1e5014bd, src-78c4677b, src-2a4435f2\n\n### Biases & Limitations\n- [HIGH] LLM judges exhibit distinct cognitive biases that compromise reliability, most notably 'self-preference bias' (favoring their own generated outputs), 'verbosity bias' (favoring longer responses regardless of quality), and 'position bias' (favoring the first option in pairwise comparisons).\n  Sources: src-67c025c2, src-45a8de46, src-48201995, src-e0d1753b, src-8d0c93da, src-a4549098, src-7c38a7f7\n- [HIGH] LLM Judges exhibit distinct systematic biases that compromise reliability, specifically 'self-preference bias' (favoring their own generated outputs), 'verbosity bias' (preferring longer, more verbose responses regardless of quality), and 'position bias' (favoring answers presented first in pairwise comparisons).\n  Sources: src-67c025c2, src-45a8de46, src-48201995, src-e0d1753b, src-8d0c93da, src-a4549098\n\n### Methodology\n- [MEDIUM] Effective mitigation strategies for these biases include Chain-of-Thought (CoT) prompting to induce reasoning before scoring, position swapping (running evaluations twice with reversed orders), and using reference-free evaluation metrics.\n  Sources: src-8d0c93da, src-51263506, src-48201995, src-e0d1753b\n- [MEDIUM] Proposed mitigation strategies for judge biases include the use of Chain-of-Thought (CoT) prompting to improve reasoning prior to scoring, and position-swapping in pairwise comparisons to average out positional preferences.\n  Sources: src-8d0c93da, src-e0d1753b, src-a4549098\n\n### Performance\n- [HIGH] GPT-4 remains the standard for 'Judge' models, capable of achieving over 80% agreement with human preferences on benchmarks like MT-Bench and Chatbot Arena, effectively matching controlled human agreement levels.\n  Sources: src-48201995, src-51263506, src-2a4435f2\n\n### Applications\n- [MEDIUM] Application-specific judge frameworks are emerging, such as 'SWE-Judge' for software engineering which evaluates code correctness, moving beyond generic dialogue evaluation to domain-specific tasks.\n  Sources: src-1e5014bd\n\n### Performance Validation\n- [HIGH] High-performing LLMs like GPT-4 can achieve over 80% agreement with human preferences in controlled benchmarks (e.g., MT-Bench), matching the level of agreement typically found between different human evaluators, validating their utility for scalable evaluation.\n  Sources: src-48201995, src-51263506, src-2a4435f2\n\n### Architectures & Frameworks\n- [MEDIUM] Domain-specific evaluation frameworks are emerging to address general judge limitations, such as 'SWE-Judge' for software engineering (using ensemble methods for code correctness) and 'Co-Eval' which augments LLM judgments with traditional machine metrics to reduce bias.\n  Sources: src-1e5014bd, src-66027906\n\n## Knowledge Gaps Identified\n- [unresolved] While 'Judge Assembly' and ensemble methods are mentioned, specific architectural patterns for orchestrating these cost-effectively in production (latency vs. accuracy trade-offs) are under-documented in the provided sources.\n- [unresolved] The sources discuss biases extensively but lack detailed comparative data on the efficacy of 'Reference-free' vs. 'Reference-based' evaluation across different model sizes (e.g., can a small model effectively judge a large model if provided a reference?).\n- [unresolved] While RAG is mentioned as an application, there is a lack of specific detail on how LLM judges evaluate the 'retrieval' component separately from the 'generation' component (e.g., context relevance vs. answer faithfulness) in the provided sources.\n- [unresolved] There is limited information on the cost-latency trade-offs of deploying LLM judges at scale versus using smaller, fine-tuned judge models.\n- [unresolved] While self-preference bias is widely identified, there is a lack of standardized, established quantitative metrics to consistently measure it across different models, and its underlying causes are not fully understood.\n- [unresolved] The application of LLM Judges specifically for RAG (Retrieval-Augmented Generation) systems is mentioned as a category, but detailed methodologies or specific metrics for evaluating the retrieval component versus the generation component using judges are missing.\n\n## Source Reference\n- src-67c025c2: Self-Preference Bias in LLM-as-a-Judge [high]\n  URL: https://openreview.net/forum?id=Ns8zGZ0lmM\n- src-45a8de46: Self-Preference Bias in LLM-as-a-Judge [high]\n  URL: https://arxiv.org/html/2410.21819v1\n- src-48201995: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena [high]\n  URL: https://neurips.cc/virtual/2023/poster/73434\n- src-e0d1753b: Mitigating the Bias of Large Language Model Evaluation [high]\n  URL: https://aclanthology.org/2024.ccl-1.101.pdf\n- src-8d0c93da: 5 Techniques to Improve LLM-Judges : r/LLMDevs [low]\n  URL: https://www.reddit.com/r/LLMDevs/comments/1j3gbil/5_techniques_to_improve_llmjudges/\n- src-08525cff: LLM-as-a-Judge: Unveiling Its Potential and Applications - Medium [medium]\n  URL: https://medium.com/@ganeshkannappan/llm-as-a-judge-unveiling-its-potential-and-applications-cbfb3db14e26\n- src-51263506: Using LLMs for Evaluation - by Cameron R. Wolfe, Ph.D. [medium]\n  URL: https://cameronrwolfe.substack.com/p/llm-as-a-judge\n- src-2a4435f2: A Survey on LLM-as-a-Judge - arXiv [high]\n  URL: https://arxiv.org/html/2411.15594v1\n- src-bbd215f1: LLM-as-a-Judge - by Nilesh Barla - Adaline Labs [medium]\n  URL: https://labs.adaline.ai/p/llm-as-a-judge\n- src-78c4677b: LLM-as-a-Judge: How AI Can Evaluate AI Faster and Smarter [medium]\n  URL: https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-j/\n- src-6ba1f0a1: Understanding Bias in LLM-as-a-Judge Systems [medium]\n  URL: https://ragmetrics.ai/blog/understanding-bias-in-llm-as-a-judge-systems\n- src-a4549098: A Systematic Study of Position Bias in LLM-as-a-Judge - arXiv [high]\n  URL: https://arxiv.org/html/2406.07791v7\n- src-bef824af: The 5 Biases That Can Silently Kill Your LLM Evaluations ... [medium]\n  URL: https://www.sebastiansigl.com/blog/llm-judge-biases-and-how-to-fix-them/\n- src-7c38a7f7: Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge [medium]\n  URL: https://llm-judge-bias.github.io\n- src-c33a2512: Evaluating and Mitigating LLM-as-a-judge Bias in ... [high]\n  URL: https://arxiv.org/abs/2510.12462\n- src-1e5014bd: An LLM-as-Judge Metric for Bridging the Gap with Human ... - arXiv [high]\n  URL: https://arxiv.org/html/2505.20854v1\n- src-db258615: LLM Evaluation Frameworks, Metrics & Methods Explained [medium]\n  URL: https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained\n- src-3f4263f1: Large Language Model Evaluation in '26: 10+ Metrics & Methods [medium]\n  URL: https://research.aimultiple.com/large-language-model-evaluation/\n- src-0378afab: LLM Evaluation in 2025: Metrics, RAG, LLM-as-Judge & Best Practices [medium]\n  URL: https://medium.com/@QuarkAndCode/llm-evaluation-in-2025-metrics-rag-llm-as-judge-best-practices-ad2872cfa7cb\n- src-66027906: Co-Eval: Augmenting LLM-based Evaluation with Machine Metrics [high]\n  URL: https://aclanthology.org/2025.emnlp-main.1307.pdf\n- src-03c1a7f3: Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge [high]\n  URL: https://arxiv.org/html/2410.02736v1\n- src-7c2fcbc0: The Intricacies of Evaluating Large Language Models with LLM-as-a ... [medium]\n  URL: https://medium.com/@vineethveetil/the-intricacies-of-evaluating-large-language-models-with-llm-as-a-judge-8034a3f34b28\n- src-fa92de56: LLM-as-a-Judge Simply... [medium]\n  URL: https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method\n- src-d64f918b: A Survey on LLM-as-a-Judge [high]\n  URL: https://arxiv.org/html/2411.15594v4\n- src-770e362f: A Survey on LLM-as-a-Judge - arXiv [high]\n  URL: https://arxiv.org/html/2411.15594v6\n- src-29376e6f: 3. Biases: Position, Length... [medium]\n  URL: https://www.emergentmind.com/topics/llm-as-a-judge-evaluations\n- src-a040edba: LLM-Judge Protocol: Methods & Applications - Emergent Mind [medium]\n  URL: https://www.emergentmind.com/topics/llm-judge-protocol\n- src-05ffcaa9: Judge's Verdict: A Comprehensive Analysis of LLM ... [high]\n  URL: https://arxiv.org/html/2510.09738v1\n- src-a71f7557: An LLM-as-Judge Metric for Bridging the Gap with Human ... [high]\n  URL: https://arxiv.org/abs/2505.20854\n- src-35901ad6: LLM-as-a-judge: a complete guide to using LLMs for evaluations [medium]\n  URL: https://www.evidentlyai.com/llm-guide/llm-as-a-judge\n\n## Instructions\nGenerate a comprehensive research report addressing the query: 'LLM Judges: techniques, architectures, evaluation methods, and applications for using large language models as automated evaluators and judges'\n\nThis is iteration 3 of 3.\nTotal findings: 12\nTotal sources: 156\nUnresolved gaps: 6\n\nCreate a well-structured markdown report following the format specified.", "raw_response": "# Research Report: LLM Judges\n\n## Executive Summary\nThe use of Large Language Models (LLMs) as automated evaluators, commonly known as \"LLM-as-a-Judge,\" has emerged as a scalable alternative to human annotation. Research indicates that high-performing models, particularly GPT-4, can achieve over 80% agreement with human preferences in general chat domains, effectively matching the inter-annotator agreement levels found among human evaluators. This capability allows for rapid, cost-effective evaluation of model outputs, crucial for iterative development and alignment tasks.\n\nHowever, the reliability of LLM judges is compromised by systematic cognitive biases. These include \"self-preference bias,\" where models favor their own outputs; \"position bias,\" where the order of options in pairwise comparison dictates the winner; and \"verbosity bias,\" a tendency to rate longer responses higher regardless of factual quality. To counter these, researchers are adopting robust mitigation frameworks, including Chain-of-Thought (CoT) prompting to induce reasoning prior to scoring and position-swapping protocols to average out positional advantages.\n\nAdvanced implementations are moving beyond simple scoring to domain-specific architectures. \"Judge Assembly\" and ensemble methods, such as SWE-Judge for software engineering, combine LLM reasoning with objective execution-based feedback. While these methods show promise in bridging the gap between stochastic language generation and deterministic correctness, significant knowledge gaps remain regarding the cost-latency trade-offs of these complex systems and standardized metrics for quantifying specific biases like self-preference.\n\n## Key Findings\n\n### Architectures and Performance\n- **Dominant Methodologies:** Two primary architectures define the field: \"Pairwise Comparison,\" which mimics human preference testing (e.g., Chatbot Arena), and \"Direct Scoring/Pointwise,\" where models assign absolute scores (e.g., 1-10 scale). **[src-48201995]** **[src-51263506]**\n- **Human Parity:** State-of-the-art models like GPT-4 demonstrate strong performance, achieving over 80% agreement with human annotators on benchmarks such as MT-Bench and Chatbot Arena, effectively matching controlled human agreement levels. **[src-48201995]** **[src-2a4435f2]**\n- **Advanced Ensembles:** For complex domains, simple prompting is insufficient. \"Ensemble\" or \"Judge Assembly\" approaches are emerging, such as \"SWE-Judge,\" which integrates LLM reasoning with code execution and static analysis to evaluate software engineering tasks with higher fidelity. **[src-1e5014bd]** **[src-78c4677b]**\n\n### Cognitive Biases and Limitations\n- **Systematic Flaws:** LLM judges exhibit distinct, non-human biases that undermine their neutrality. The most prevalent include:\n    - **Self-Preference Bias:** A strong tendency for models to favor outputs generated by themselves or similar model families. **[src-67c025c2]** **[src-45a8de46]**\n    - **Position Bias:** In pairwise comparisons, models disproportionately favor the first option presented. **[src-a4549098]** **[src-e0d1753b]**\n    - **Verbosity Bias:** A heuristic where longer, more verbose responses are rated higher, even when they are less accurate or concise. **[src-48201995]** **[src-7c38a7f7]**\n\n### Mitigation Techniques\n- **Prompt Engineering:** \"Chain-of-Thought\" (CoT) prompting is highly effective, requiring the judge to generate a reasoning rationale before assigning a score, which improves alignment with human logic. **[src-8d0c93da]** **[src-e0d1753b]**\n- **Structural Adjustments:** \"Position swapping\" involves running pairwise evaluations twice with the order of candidates reversed to cancel out position bias. **[src-8d0c93da]** **[src-a4549098]**\n- **Hybrid Frameworks:** \"Co-Eval\" frameworks augment LLM judgments with traditional, objective machine metrics, helping to ground the subjective evaluation and reduce hallucinated scoring. **[src-66027906]**\n\n## Analysis\n\n### Supporting Evidence\nThere is high-confidence consensus across multiple studies that GPT-4 serves as a reliable proxy for human evaluation in general domains, consistently replicating human preference rankings **[src-48201995]** **[src-51263506]**. Furthermore, the existence of position and verbosity biases is well-documented and replicable, with position swapping being universally recommended as a standard operating procedure for pairwise evaluations **[src-a4549098]**.\n\n### Conflicting Information\nWhile sources agree on the existence of biases, there is implicit tension regarding the \"Self-Preference Bias.\" While identified as a major issue **[src-67c025c2]**, the mechanism is not fully understood\u2014specifically, whether it stems from training data overlap or inherent stylistic preferences. Additionally, while \"Reference-free\" evaluation is touted for scalability, its accuracy compared to \"Reference-based\" methods (where the judge is given a gold-standard answer) varies significantly depending on the task complexity, a nuance not fully resolved in general surveys.\n\n### Limitations\nThe current research landscape highlights several key gaps:\n1.  **Cost vs. Latency:** There is a lack of data quantifying the trade-offs between deploying large, expensive judge models (like GPT-4) versus smaller, fine-tuned judges or ensembles in production environments.\n2.  **RAG Specifics:** While Retrieval-Augmented Generation (RAG) is a key application, specific methodologies for separately evaluating the *retrieval* component (context relevance) versus the *generation* component (faithfulness) using LLM judges are under-documented in these findings.\n3.  **Standardized Bias Metrics:** Although biases are known, there is no widely accepted standard metric to quantify \"Self-Preference Bias\" consistently across different model families.\n\n## Sources\n- **[src-67c025c2]** [Self-Preference Bias in LLM-as-a-Judge](https://openreview.net/forum?id=Ns8zGZ0lmM)\n- **[src-45a8de46]** [Self-Preference Bias in LLM-as-a-Judge (arXiv)](https://arxiv.org/html/2410.21819v1)\n- **[src-48201995]** [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://neurips.cc/virtual/2023/poster/73434)\n- **[src-e0d1753b]** [Mitigating the Bias of Large Language Model Evaluation](https://aclanthology.org/2024.ccl-1.101.pdf)\n- **[src-8d0c93da]** [5 Techniques to Improve LLM-Judges](https://www.reddit.com/r/LLMDevs/comments/1j3gbil/5_techniques_to_improve_llmjudges/)\n- **[src-51263506]** [Using LLMs for Evaluation](https://cameronrwolfe.substack.com/p/llm-as-a-judge)\n- **[src-2a4435f2]** [A Survey on LLM-as-a-Judge](https://arxiv.org/html/2411.15594v1)\n- **[src-78c4677b]** [LLM-as-a-Judge: How AI Can Evaluate AI Faster and Smarter](https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-j/)\n- **[src-a4549098]** [A Systematic Study of Position Bias in LLM-as-a-Judge](https://arxiv.org/html/2406.07791v7)\n- **[src-7c38a7f7]** [Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge](https://llm-judge-bias.github.io)\n- **[src-1e5014bd]** [An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation](https://arxiv.org/html/2505.20854v1)\n- **[src-66027906]** [Co-Eval: Augmenting LLM-based Evaluation with Machine Metrics](https://aclanthology.org/2025.emnlp-main.1307.pdf)\n\n## Conclusions\nTo effectively utilize LLMs as automated judges, organizations must treat them as imperfect but powerful tools. The following recommendations are derived from the findings:\n1.  **Mandatory Bias Mitigation:** Never use a single-pass evaluation for pairwise comparisons. Implement mandatory position swapping and average the results. Use Chain-of-Thought prompting to force the model to justify its score before assigning it.\n2.  **Model Selection:** For high-stakes evaluation or general benchmarks, reliable frontier models (like GPT-4) are required to achieve human-parity. Smaller models should only be used if specifically fine-tuned for the \"judge\" role or used in ensembles.\n3.  **Domain-Specific Validation:** For technical fields like software engineering, do not rely on LLM judgment alone. Adopt \"Judge Assembly\" patterns that incorporate deterministic checks (code execution, linters) to validate the LLM's assessment.", "report": "# Research Report: LLM Judges\n\n## Executive Summary\nThe use of Large Language Models (LLMs) as automated evaluators, commonly known as \"LLM-as-a-Judge,\" has emerged as a scalable alternative to human annotation. Research indicates that high-performing models, particularly GPT-4, can achieve over 80% agreement with human preferences in general chat domains, effectively matching the inter-annotator agreement levels found among human evaluators. This capability allows for rapid, cost-effective evaluation of model outputs, crucial for iterative development and alignment tasks.\n\nHowever, the reliability of LLM judges is compromised by systematic cognitive biases. These include \"self-preference bias,\" where models favor their own outputs; \"position bias,\" where the order of options in pairwise comparison dictates the winner; and \"verbosity bias,\" a tendency to rate longer responses higher regardless of factual quality. To counter these, researchers are adopting robust mitigation frameworks, including Chain-of-Thought (CoT) prompting to induce reasoning prior to scoring and position-swapping protocols to average out positional advantages.\n\nAdvanced implementations are moving beyond simple scoring to domain-specific architectures. \"Judge Assembly\" and ensemble methods, such as SWE-Judge for software engineering, combine LLM reasoning with objective execution-based feedback. While these methods show promise in bridging the gap between stochastic language generation and deterministic correctness, significant knowledge gaps remain regarding the cost-latency trade-offs of these complex systems and standardized metrics for quantifying specific biases like self-preference.\n\n## Key Findings\n\n### Architectures and Performance\n- **Dominant Methodologies:** Two primary architectures define the field: \"Pairwise Comparison,\" which mimics human preference testing (e.g., Chatbot Arena), and \"Direct Scoring/Pointwise,\" where models assign absolute scores (e.g., 1-10 scale). **[src-48201995]** **[src-51263506]**\n- **Human Parity:** State-of-the-art models like GPT-4 demonstrate strong performance, achieving over 80% agreement with human annotators on benchmarks such as MT-Bench and Chatbot Arena, effectively matching controlled human agreement levels. **[src-48201995]** **[src-2a4435f2]**\n- **Advanced Ensembles:** For complex domains, simple prompting is insufficient. \"Ensemble\" or \"Judge Assembly\" approaches are emerging, such as \"SWE-Judge,\" which integrates LLM reasoning with code execution and static analysis to evaluate software engineering tasks with higher fidelity. **[src-1e5014bd]** **[src-78c4677b]**\n\n### Cognitive Biases and Limitations\n- **Systematic Flaws:** LLM judges exhibit distinct, non-human biases that undermine their neutrality. The most prevalent include:\n    - **Self-Preference Bias:** A strong tendency for models to favor outputs generated by themselves or similar model families. **[src-67c025c2]** **[src-45a8de46]**\n    - **Position Bias:** In pairwise comparisons, models disproportionately favor the first option presented. **[src-a4549098]** **[src-e0d1753b]**\n    - **Verbosity Bias:** A heuristic where longer, more verbose responses are rated higher, even when they are less accurate or concise. **[src-48201995]** **[src-7c38a7f7]**\n\n### Mitigation Techniques\n- **Prompt Engineering:** \"Chain-of-Thought\" (CoT) prompting is highly effective, requiring the judge to generate a reasoning rationale before assigning a score, which improves alignment with human logic. **[src-8d0c93da]** **[src-e0d1753b]**\n- **Structural Adjustments:** \"Position swapping\" involves running pairwise evaluations twice with the order of candidates reversed to cancel out position bias. **[src-8d0c93da]** **[src-a4549098]**\n- **Hybrid Frameworks:** \"Co-Eval\" frameworks augment LLM judgments with traditional, objective machine metrics, helping to ground the subjective evaluation and reduce hallucinated scoring. **[src-66027906]**\n\n## Analysis\n\n### Supporting Evidence\nThere is high-confidence consensus across multiple studies that GPT-4 serves as a reliable proxy for human evaluation in general domains, consistently replicating human preference rankings **[src-48201995]** **[src-51263506]**. Furthermore, the existence of position and verbosity biases is well-documented and replicable, with position swapping being universally recommended as a standard operating procedure for pairwise evaluations **[src-a4549098]**.\n\n### Conflicting Information\nWhile sources agree on the existence of biases, there is implicit tension regarding the \"Self-Preference Bias.\" While identified as a major issue **[src-67c025c2]**, the mechanism is not fully understood\u2014specifically, whether it stems from training data overlap or inherent stylistic preferences. Additionally, while \"Reference-free\" evaluation is touted for scalability, its accuracy compared to \"Reference-based\" methods (where the judge is given a gold-standard answer) varies significantly depending on the task complexity, a nuance not fully resolved in general surveys.\n\n### Limitations\nThe current research landscape highlights several key gaps:\n1.  **Cost vs. Latency:** There is a lack of data quantifying the trade-offs between deploying large, expensive judge models (like GPT-4) versus smaller, fine-tuned judges or ensembles in production environments.\n2.  **RAG Specifics:** While Retrieval-Augmented Generation (RAG) is a key application, specific methodologies for separately evaluating the *retrieval* component (context relevance) versus the *generation* component (faithfulness) using LLM judges are under-documented in these findings.\n3.  **Standardized Bias Metrics:** Although biases are known, there is no widely accepted standard metric to quantify \"Self-Preference Bias\" consistently across different model families.\n\n## Sources\n- **[src-67c025c2]** [Self-Preference Bias in LLM-as-a-Judge](https://openreview.net/forum?id=Ns8zGZ0lmM)\n- **[src-45a8de46]** [Self-Preference Bias in LLM-as-a-Judge (arXiv)](https://arxiv.org/html/2410.21819v1)\n- **[src-48201995]** [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://neurips.cc/virtual/2023/poster/73434)\n- **[src-e0d1753b]** [Mitigating the Bias of Large Language Model Evaluation](https://aclanthology.org/2024.ccl-1.101.pdf)\n- **[src-8d0c93da]** [5 Techniques to Improve LLM-Judges](https://www.reddit.com/r/LLMDevs/comments/1j3gbil/5_techniques_to_improve_llmjudges/)\n- **[src-51263506]** [Using LLMs for Evaluation](https://cameronrwolfe.substack.com/p/llm-as-a-judge)\n- **[src-2a4435f2]** [A Survey on LLM-as-a-Judge](https://arxiv.org/html/2411.15594v1)\n- **[src-78c4677b]** [LLM-as-a-Judge: How AI Can Evaluate AI Faster and Smarter](https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-j/)\n- **[src-a4549098]** [A Systematic Study of Position Bias in LLM-as-a-Judge](https://arxiv.org/html/2406.07791v7)\n- **[src-7c38a7f7]** [Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge](https://llm-judge-bias.github.io)\n- **[src-1e5014bd]** [An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation](https://arxiv.org/html/2505.20854v1)\n- **[src-66027906]** [Co-Eval: Augmenting LLM-based Evaluation with Machine Metrics](https://aclanthology.org/2025.emnlp-main.1307.pdf)\n\n## Conclusions\nTo effectively utilize LLMs as automated judges, organizations must treat them as imperfect but powerful tools. The following recommendations are derived from the findings:\n1.  **Mandatory Bias Mitigation:** Never use a single-pass evaluation for pairwise comparisons. Implement mandatory position swapping and average the results. Use Chain-of-Thought prompting to force the model to justify its score before assigning it.\n2.  **Model Selection:** For high-stakes evaluation or general benchmarks, reliable frontier models (like GPT-4) are required to achieve human-parity. Smaller models should only be used if specifically fine-tuned for the \"judge\" role or used in ensembles.\n3.  **Domain-Specific Validation:** For technical fields like software engineering, do not rely on LLM judgment alone. Adopt \"Judge Assembly\" patterns that incorporate deterministic checks (code execution, linters) to validate the LLM's assessment.", "report_length": 8243}}
{"timestamp": "2026-01-01T01:18:35.502087Z", "event_id": "f2eb8d245c79401fa56c0a24af197394", "event_type": "phase_complete", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "synthesis", "iteration": 3, "data": {"phase": "synthesis", "duration_ms": 37877.17040599091}}
{"timestamp": "2026-01-01T01:18:35.518082Z", "event_id": "94c477f3916948558059faefd5a6d856", "event_type": "workflow_complete", "level": "info", "research_id": "deepres-906a9d34c7b2", "phase": "synthesis", "iteration": 3, "data": {"success": true, "phase": "synthesis", "iteration": 3, "sub_query_count": 12, "source_count": 156, "finding_count": 12, "gap_count": 6, "report_length": 8243, "total_tokens_used": 129685, "total_duration_ms": 74136.27365999855, "total_input_tokens": 108878, "total_output_tokens": 11060, "total_cached_tokens": 0, "phase_metrics": [{"phase": "planning", "duration_ms": 0.0, "input_tokens": 8349, "output_tokens": 307, "cached_tokens": 0, "provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview"}, {"phase": "analysis", "duration_ms": 0.0, "input_tokens": 15945, "output_tokens": 1033, "cached_tokens": 0, "provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview"}, {"phase": "synthesis", "duration_ms": 0.0, "input_tokens": 10849, "output_tokens": 1955, "cached_tokens": 0, "provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview"}, {"phase": "refinement", "duration_ms": 0.0, "input_tokens": 9227, "output_tokens": 581, "cached_tokens": 0, "provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview"}, {"phase": "analysis", "duration_ms": 0.0, "input_tokens": 15945, "output_tokens": 1048, "cached_tokens": 0, "provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview"}, {"phase": "synthesis", "duration_ms": 0.0, "input_tokens": 11324, "output_tokens": 2115, "cached_tokens": 0, "provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview"}, {"phase": "refinement", "duration_ms": 0.0, "input_tokens": 9482, "output_tokens": 714, "cached_tokens": 0, "provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview"}, {"phase": "analysis", "duration_ms": 0.0, "input_tokens": 15945, "output_tokens": 1128, "cached_tokens": 0, "provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview"}, {"phase": "synthesis", "duration_ms": 0.0, "input_tokens": 11812, "output_tokens": 2179, "cached_tokens": 0, "provider_id": "gemini", "model_used": "gemini:gemini-3-pro-preview"}], "search_provider_stats": {"tavily": 12, "perplexity": 12, "google": 12, "semantic_scholar": 12}, "total_search_queries": 48, "source_hostnames": ["aclanthology.org", "aman.ai", "arxiv.org", "aws.amazon.com", "blogs.infosys.com", "cameronrwolfe.substack.com", "customgpt.ai", "dkaarthick.medium.com", "docs.ragas.io", "doi.org", "en.wikipedia.org", "encord.com", "eugeneyan.com", "files.sri.inf.ethz.ch", "galileo.ai", "github.com", "iclr.cc", "jmlr.org", "labelstud.io", "labelyourdata.com", "labs.adaline.ai", "langchain-opentutorial.gitbook.io", "leehanchung.github.io", "llm-judge-bias.github.io", "medium.com", "mistral.ai", "modulai.io", "neurips.cc", "noy-sternlicht.github.io", "onlinelibrary.wiley.com", "openreview.net", "pixion.co", "pmc.ncbi.nlm.nih.gov", "predibase.com", "ragmetrics.ai", "research.aimultiple.com", "tech.beatrust.com", "wandb.ai", "www.alphaxiv.org", "www.bunnyshell.com", "www.confident-ai.com", "www.datarobot.com", "www.diva-portal.org", "www.emergentmind.com", "www.evidentlyai.com", "www.getmaxim.ai", "www.inferless.com", "www.linkedin.com", "www.nb-data.com", "www.patronus.ai", "www.qeios.com", "www.qualifire.ai", "www.reddit.com", "www.sciencedirect.com", "www.sebastiansigl.com", "www.snowflake.com", "www.statsig.com", "www.superannotate.com", "www.tensorzero.com", "www.thejournal.club", "www.thoughtworks.com", "www.vldb.org", "www.youtube.com", "x.com"], "research_mode": "technical"}}
